{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code2Doc-usingGRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnO3GnPTpUUiv2uInc/lui",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "843b6c4e8a754ff78c3d7fe5b9e8415e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8098ec46c5bb4e17a2ed494f7f01d31d",
              "IPY_MODEL_50037afb8751477688a4b4597c985351",
              "IPY_MODEL_b5cd00a828144b2e91cb609adb3f6e78"
            ],
            "layout": "IPY_MODEL_cdf4bcb1803a4f48a43c1628ca6716ee"
          }
        },
        "8098ec46c5bb4e17a2ed494f7f01d31d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22eb8b66c2b64d5dafef3e474379ad5b",
            "placeholder": "​",
            "style": "IPY_MODEL_16690e46bb53464eb2edc7b01505e8ce",
            "value": "100%"
          }
        },
        "50037afb8751477688a4b4597c985351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9646255fbf084e6a8f0e5323b53330df",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aace67b0ffd745d4a595321f0558b871",
            "value": 10
          }
        },
        "b5cd00a828144b2e91cb609adb3f6e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3454a6f046da470c8c9d036184a80af5",
            "placeholder": "​",
            "style": "IPY_MODEL_45ee471439b44bbe92adaf6590bdd387",
            "value": " 10/10 [00:18&lt;00:00,  1.82s/it]"
          }
        },
        "cdf4bcb1803a4f48a43c1628ca6716ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22eb8b66c2b64d5dafef3e474379ad5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16690e46bb53464eb2edc7b01505e8ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9646255fbf084e6a8f0e5323b53330df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aace67b0ffd745d4a595321f0558b871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3454a6f046da470c8c9d036184a80af5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ee471439b44bbe92adaf6590bdd387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisashimabucoro/NMA-Project-Code-Summarization/blob/main/Code2Doc_usingGRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preliminaries**"
      ],
      "metadata": {
        "id": "WIjWyvsybAPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers &> /dev/null\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import unicodedata\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "metadata": {
        "id": "X8XWLGRUiNSr"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61avk545hCV8",
        "outputId": "d02bee6f-f605-4119-fdcd-cc072a49e6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount gdrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic EDA**"
      ],
      "metadata": {
        "id": "XareqRpeana6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.read_json('/content/gdrive/Shareddrives/Dolma2/Datasets/python/train.jsonl', lines=True)"
      ],
      "metadata": {
        "id": "T_YZCJLOiPeO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df[:5] # samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "M7lw9Kybi2f7",
        "outputId": "84efd28f-6f2c-4ac6-a363-ccd43e5b7610"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   repo                path          func_name  \\\n",
              "0  smdabdoub/phylotoast  phylotoast/util.py    split_phylogeny   \n",
              "1  smdabdoub/phylotoast  phylotoast/util.py         ensure_dir   \n",
              "2  smdabdoub/phylotoast  phylotoast/util.py        file_handle   \n",
              "3  smdabdoub/phylotoast  phylotoast/util.py  gather_categories   \n",
              "4  smdabdoub/phylotoast  phylotoast/util.py      parse_unifrac   \n",
              "\n",
              "                                     original_string language  \\\n",
              "0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...   python   \n",
              "1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...   python   \n",
              "2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...   python   \n",
              "3  def gather_categories(imap, header, categories...   python   \n",
              "4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...   python   \n",
              "\n",
              "                                                code  \\\n",
              "0  def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...   \n",
              "1  def ensure_dir(d):\\n    \"\"\"\\n    Check to make...   \n",
              "2  def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...   \n",
              "3  def gather_categories(imap, header, categories...   \n",
              "4  def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...   \n",
              "\n",
              "                                         code_tokens  \\\n",
              "0  [def, split_phylogeny, (, p, ,, level, =, \"s\",...   \n",
              "1  [def, ensure_dir, (, d, ), :, if, not, os, ., ...   \n",
              "2  [def, file_handle, (, fnh, ,, mode, =, \"rU\", )...   \n",
              "3  [def, gather_categories, (, imap, ,, header, ,...   \n",
              "4  [def, parse_unifrac, (, unifracFN, ), :, with,...   \n",
              "\n",
              "                                           docstring  \\\n",
              "0  Return either the full or truncated version of...   \n",
              "1  Check to make sure the supplied directory path...   \n",
              "2  Takes either a file path or an open file handl...   \n",
              "3  Find the user specified categories in the map ...   \n",
              "4  Parses the unifrac results file into a diction...   \n",
              "\n",
              "                                    docstring_tokens  \\\n",
              "0  [Return, either, the, full, or, truncated, ver...   \n",
              "1  [Check, to, make, sure, the, supplied, directo...   \n",
              "2  [Takes, either, a, file, path, or, an, open, f...   \n",
              "3  [Find, the, user, specified, categories, in, t...   \n",
              "4  [Parses, the, unifrac, results, file, into, a,...   \n",
              "\n",
              "                                        sha  \\\n",
              "0  0b74ef171e6a84761710548501dfac71285a58a3   \n",
              "1  0b74ef171e6a84761710548501dfac71285a58a3   \n",
              "2  0b74ef171e6a84761710548501dfac71285a58a3   \n",
              "3  0b74ef171e6a84761710548501dfac71285a58a3   \n",
              "4  0b74ef171e6a84761710548501dfac71285a58a3   \n",
              "\n",
              "                                                 url partition  \n",
              "0  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
              "1  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
              "2  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
              "3  https://github.com/smdabdoub/phylotoast/blob/0...     train  \n",
              "4  https://github.com/smdabdoub/phylotoast/blob/0...     train  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20d76728-38b9-4b1d-bd09-5aee4d81d24d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>repo</th>\n",
              "      <th>path</th>\n",
              "      <th>func_name</th>\n",
              "      <th>original_string</th>\n",
              "      <th>language</th>\n",
              "      <th>code</th>\n",
              "      <th>code_tokens</th>\n",
              "      <th>docstring</th>\n",
              "      <th>docstring_tokens</th>\n",
              "      <th>sha</th>\n",
              "      <th>url</th>\n",
              "      <th>partition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>smdabdoub/phylotoast</td>\n",
              "      <td>phylotoast/util.py</td>\n",
              "      <td>split_phylogeny</td>\n",
              "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
              "      <td>python</td>\n",
              "      <td>def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n ...</td>\n",
              "      <td>[def, split_phylogeny, (, p, ,, level, =, \"s\",...</td>\n",
              "      <td>Return either the full or truncated version of...</td>\n",
              "      <td>[Return, either, the, full, or, truncated, ver...</td>\n",
              "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
              "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>smdabdoub/phylotoast</td>\n",
              "      <td>phylotoast/util.py</td>\n",
              "      <td>ensure_dir</td>\n",
              "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
              "      <td>python</td>\n",
              "      <td>def ensure_dir(d):\\n    \"\"\"\\n    Check to make...</td>\n",
              "      <td>[def, ensure_dir, (, d, ), :, if, not, os, ., ...</td>\n",
              "      <td>Check to make sure the supplied directory path...</td>\n",
              "      <td>[Check, to, make, sure, the, supplied, directo...</td>\n",
              "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
              "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>smdabdoub/phylotoast</td>\n",
              "      <td>phylotoast/util.py</td>\n",
              "      <td>file_handle</td>\n",
              "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
              "      <td>python</td>\n",
              "      <td>def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n   ...</td>\n",
              "      <td>[def, file_handle, (, fnh, ,, mode, =, \"rU\", )...</td>\n",
              "      <td>Takes either a file path or an open file handl...</td>\n",
              "      <td>[Takes, either, a, file, path, or, an, open, f...</td>\n",
              "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
              "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>smdabdoub/phylotoast</td>\n",
              "      <td>phylotoast/util.py</td>\n",
              "      <td>gather_categories</td>\n",
              "      <td>def gather_categories(imap, header, categories...</td>\n",
              "      <td>python</td>\n",
              "      <td>def gather_categories(imap, header, categories...</td>\n",
              "      <td>[def, gather_categories, (, imap, ,, header, ,...</td>\n",
              "      <td>Find the user specified categories in the map ...</td>\n",
              "      <td>[Find, the, user, specified, categories, in, t...</td>\n",
              "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
              "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>smdabdoub/phylotoast</td>\n",
              "      <td>phylotoast/util.py</td>\n",
              "      <td>parse_unifrac</td>\n",
              "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
              "      <td>python</td>\n",
              "      <td>def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Pa...</td>\n",
              "      <td>[def, parse_unifrac, (, unifracFN, ), :, with,...</td>\n",
              "      <td>Parses the unifrac results file into a diction...</td>\n",
              "      <td>[Parses, the, unifrac, results, file, into, a,...</td>\n",
              "      <td>0b74ef171e6a84761710548501dfac71285a58a3</td>\n",
              "      <td>https://github.com/smdabdoub/phylotoast/blob/0...</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20d76728-38b9-4b1d-bd09-5aee4d81d24d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20d76728-38b9-4b1d-bd09-5aee4d81d24d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20d76728-38b9-4b1d-bd09-5aee4d81d24d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = data_df.loc[:,['code_tokens', 'docstring_tokens']].head(300) # removing extra columns\n",
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ry06m5dHjXxe",
        "outputId": "81112cbd-e977-4e1f-941b-a790e1e9998f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           code_tokens  \\\n",
              "0    [def, split_phylogeny, (, p, ,, level, =, \"s\",...   \n",
              "1    [def, ensure_dir, (, d, ), :, if, not, os, ., ...   \n",
              "2    [def, file_handle, (, fnh, ,, mode, =, \"rU\", )...   \n",
              "3    [def, gather_categories, (, imap, ,, header, ,...   \n",
              "4    [def, parse_unifrac, (, unifracFN, ), :, with,...   \n",
              "..                                                 ...   \n",
              "295  [def, _prepare_defaults, (, self, ), :, for, n...   \n",
              "296  [def, from_mongo, (, cls, ,, doc, ), :, if, do...   \n",
              "297  [def, pop, (, self, ,, name, ,, default, =, SE...   \n",
              "298  [def, _op, (, self, ,, operation, ,, other, ,,...   \n",
              "299  [def, _iop, (, self, ,, operation, ,, other, ,...   \n",
              "\n",
              "                                      docstring_tokens  \n",
              "0    [Return, either, the, full, or, truncated, ver...  \n",
              "1    [Check, to, make, sure, the, supplied, directo...  \n",
              "2    [Takes, either, a, file, path, or, an, open, f...  \n",
              "3    [Find, the, user, specified, categories, in, t...  \n",
              "4    [Parses, the, unifrac, results, file, into, a,...  \n",
              "..                                                 ...  \n",
              "295      [Trigger, assignment, of, default, values, .]  \n",
              "296  [Convert, data, coming, in, from, the, MongoDB...  \n",
              "297  [Retrieve, and, remove, a, value, from, the, b...  \n",
              "298  [A, basic, operation, operating, on, a, single...  \n",
              "299  [An, iterative, operation, operating, on, mult...  \n",
              "\n",
              "[300 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88b79ab8-0d9d-42f8-9cd6-5bf6789227d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_tokens</th>\n",
              "      <th>docstring_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[def, split_phylogeny, (, p, ,, level, =, \"s\",...</td>\n",
              "      <td>[Return, either, the, full, or, truncated, ver...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[def, ensure_dir, (, d, ), :, if, not, os, ., ...</td>\n",
              "      <td>[Check, to, make, sure, the, supplied, directo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[def, file_handle, (, fnh, ,, mode, =, \"rU\", )...</td>\n",
              "      <td>[Takes, either, a, file, path, or, an, open, f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[def, gather_categories, (, imap, ,, header, ,...</td>\n",
              "      <td>[Find, the, user, specified, categories, in, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[def, parse_unifrac, (, unifracFN, ), :, with,...</td>\n",
              "      <td>[Parses, the, unifrac, results, file, into, a,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>[def, _prepare_defaults, (, self, ), :, for, n...</td>\n",
              "      <td>[Trigger, assignment, of, default, values, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>[def, from_mongo, (, cls, ,, doc, ), :, if, do...</td>\n",
              "      <td>[Convert, data, coming, in, from, the, MongoDB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>[def, pop, (, self, ,, name, ,, default, =, SE...</td>\n",
              "      <td>[Retrieve, and, remove, a, value, from, the, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>[def, _op, (, self, ,, operation, ,, other, ,,...</td>\n",
              "      <td>[A, basic, operation, operating, on, a, single...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>[def, _iop, (, self, ,, operation, ,, other, ,...</td>\n",
              "      <td>[An, iterative, operation, operating, on, mult...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>300 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88b79ab8-0d9d-42f8-9cd6-5bf6789227d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88b79ab8-0d9d-42f8-9cd6-5bf6789227d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88b79ab8-0d9d-42f8-9cd6-5bf6789227d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list(train_df['docstring_tokens']\" \".join())\n",
        "for i in train_df['docstring_tokens']:\n",
        "  j = \" \".join(i)\n",
        "  print(j)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZlbFXJ3DuZT",
        "outputId": "82e477c4-7219-4749-f2f0-380dc511c00b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return either the full or truncated version of a QIIME - formatted taxonomy string .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docstring = list(train_df['docstring_tokens'].apply(lambda x: \" \".join(x)))\n",
        "code = list(train_df['code_tokens'].apply(lambda x: \" \".join(x)))\n",
        "combined = []\n",
        "for i, x in enumerate(docstring):\n",
        "  pair = []\n",
        "  pair.append(docstring[i])\n",
        "  pair.append(code[i])\n",
        "  combined.append(pair)\n",
        "  print(combined)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyOGLB5iFR1j",
        "outputId": "33f8e040-696d-4134-c6ff-10471d1a1089"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Return either the full or truncated version of a QIIME - formatted taxonomy string .', 'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n",
        "    self.n_words = 3  # Count SOS and EOS and PAD\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  return s\n",
        "\n",
        "\n",
        "def readLangs(lang1, lang2, data, reverse=False):\n",
        "  print(\"Reading lines...\")\n",
        "\n",
        "  docstring = list(train_df['docstring_tokens'].apply(lambda x: \" \".join(x)))\n",
        "  code = list(train_df['code_tokens'].apply(lambda x: \" \".join(x)))\n",
        "  combined = []\n",
        "  for i, x in enumerate(docstring):\n",
        "    pair = []\n",
        "    pair.append(docstring[i])\n",
        "    pair.append(code[i])\n",
        "    combined.append(pair)\n",
        "    #print(combined[:5])\n",
        "  '''\n",
        "  # Read the file and split into lines\n",
        "  lines = io.open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "      read().strip().split('\\n')\n",
        "\n",
        "  # Split every line into pairs and normalize\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "  print(pairs[-5:])\n",
        "\n",
        "  train_df\n",
        "  '''\n",
        "\n",
        "  # Reverse pairs, make Lang instances\n",
        "  if reverse:\n",
        "      pairs = [list(reversed(p)) for p in combined]\n",
        "      input_lang = Lang(lang2)\n",
        "      output_lang = Lang(lang1)\n",
        "  else:\n",
        "      input_lang = Lang(lang1)\n",
        "      output_lang = Lang(lang2)\n",
        "  print(combined)\n",
        "  return input_lang, output_lang, combined"
      ],
      "metadata": {
        "id": "MJXsr-o_DC3z"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 1000\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "      len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n"
      ],
      "metadata": {
        "id": "E6GcR7otI8_q"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, data, reverse=False):\n",
        "  input_lang, output_lang, pairs = readLangs(lang1, lang2, data, reverse)\n",
        "  print(pairs)\n",
        "  print(\"Read %s sentence pairs\" % len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(pairs)\n",
        "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "  print(\"Counted words:\")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('cod', 'des', train_df, True)\n",
        "print(pairs)"
      ],
      "metadata": {
        "id": "j427CqutJCum",
        "outputId": "7b97ab78-84a3-46e5-baac-f747f6601fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "[['Return either the full or truncated version of a QIIME - formatted taxonomy string .', 'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]'], ['Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .', 'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )'], ['Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .', 'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle'], ['Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .', 'def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table'], ['Parses the unifrac results file into a dictionary', 'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )'], ['Function to parse data from older version of unifrac file obtained from Qiime version 1 . 8 and earlier .', 'def parse_unifrac_v1_8 ( unifrac , file_data ) : for line in file_data : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ - 2 ] . split ( \"\\\\t\" ) [ 1 : ] ] unifrac [ \"varexp\" ] = [ float ( entry ) for entry in file_data [ - 1 ] . split ( \"\\\\t\" ) [ 1 : ] ] return unifrac'], ['Function to parse data from newer version of unifrac file obtained from Qiime version 1 . 9 and later .', 'def parse_unifrac_v1_9 ( unifrac , file_data ) : unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ 0 ] . split ( \"\\\\t\" ) ] unifrac [ \"varexp\" ] = [ float ( entry ) * 100 for entry in file_data [ 3 ] . split ( \"\\\\t\" ) ] for line in file_data [ 8 : ] : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] return unifrac'], ['Determine color - category mapping . If color_column was specified then map the category names to color values . Otherwise use the palettable colors to automatically generate a set of colors for the group values .', 'def color_mapping ( sample_map , header , group_column , color_column = None ) : group_colors = OrderedDict ( ) group_gather = gather_categories ( sample_map , header , [ group_column ] ) if color_column is not None : color_gather = gather_categories ( sample_map , header , [ color_column ] ) for group in group_gather : for color in color_gather : if group_gather [ group ] . sids . intersection ( color_gather [ color ] . sids ) : group_colors [ group ] = color else : bcolors = itertools . cycle ( Set3_12 . hex_colors ) for group in group_gather : group_colors [ group ] = bcolors . next ( ) return group_colors'], ['return reverse completment of read', \"def rev_c ( read ) : rc = [ ] rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } for base in read : rc . extend ( rc_nucs [ base . upper ( ) ] ) return rc [ : : - 1 ]\"], ['randomly shuffle genome', \"def shuffle_genome ( genome , cat , fraction = float ( 100 ) , plot = True , alpha = 0.1 , beta = 100000 , min_length = 1000 , max_length = 200000 ) : header = '>randomized_%s' % ( genome . name ) sequence = list ( '' . join ( [ i [ 1 ] for i in parse_fasta ( genome ) ] ) ) length = len ( sequence ) shuffled = [ ] while sequence is not False : s = int ( random . gammavariate ( alpha , beta ) ) if s <= min_length or s >= max_length : continue if len ( sequence ) < s : seq = sequence [ 0 : ] else : seq = sequence [ 0 : s ] sequence = sequence [ s : ] shuffled . append ( '' . join ( seq ) ) if sequence == [ ] : break random . shuffle ( shuffled ) if fraction == float ( 100 ) : subset = shuffled else : max_pieces = int ( length * fraction / 100 ) subset , total = [ ] , 0 for fragment in shuffled : length = len ( fragment ) if total + length <= max_pieces : subset . append ( fragment ) total += length else : diff = max_pieces - total subset . append ( fragment [ 0 : diff ] ) break if cat is True : yield [ header , '' . join ( subset ) ] else : for i , seq in enumerate ( subset ) : yield [ '%s fragment:%s' % ( header , i ) , seq ]\"], ['If the fit contains statistically insignificant parameters remove them . Returns a pruned fit where all parameters have p - values of the t - statistic below p_max', \"def _prune ( self , fit , p_max ) : def remove_from_model_desc ( x , model_desc ) : rhs_termlist = [ ] for t in model_desc . rhs_termlist : if not t . factors : rhs_termlist . append ( t ) elif not x == t . factors [ 0 ] . _varname : rhs_termlist . append ( t ) md = ModelDesc ( model_desc . lhs_termlist , rhs_termlist ) return md corrected_model_desc = ModelDesc ( fit . model . formula . lhs_termlist [ : ] , fit . model . formula . rhs_termlist [ : ] ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass while pars_to_prune : corrected_model_desc = remove_from_model_desc ( pars_to_prune [ 0 ] , corrected_model_desc ) fit = fm . ols ( corrected_model_desc , data = self . df ) . fit ( ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass return fit\"], ['Return the best fit based on rsquared', 'def find_best_rsquared ( list_of_fits ) : res = sorted ( list_of_fits , key = lambda x : x . rsquared ) return res [ - 1 ]'], ['Return a df with predictions and confidence interval', \"def _predict ( self , fit , df ) : df_res = df . copy ( ) if 'Intercept' in fit . model . exog_names : df_res [ 'Intercept' ] = 1.0 df_res [ 'predicted' ] = fit . predict ( df_res ) if not self . allow_negative_predictions : df_res . loc [ df_res [ 'predicted' ] < 0 , 'predicted' ] = 0 prstd , interval_l , interval_u = wls_prediction_std ( fit , df_res [ fit . model . exog_names ] , alpha = 1 - self . confint ) df_res [ 'interval_l' ] = interval_l df_res [ 'interval_u' ] = interval_u if 'Intercept' in df_res : df_res . drop ( labels = [ 'Intercept' ] , axis = 1 , inplace = True ) return df_res\"], ['Calculate the relative abundance of each OTUID in a Sample .', 'def relative_abundance ( biomf , sampleIDs = None ) : if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating relative abundances: The sampleIDs provided do\" \" not match the sampleIDs in biom file. Please double check the sampleIDs\" \" provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) norm_biomf = biomf . norm ( inplace = False ) return { sample : { otuID : norm_biomf . get_value_by_ids ( otuID , sample ) for otuID in otuIDs } for sample in sampleIDs }'], ['Calculate the mean OTU abundance percentage .', 'def mean_otu_pct_abundance ( ra , otuIDs ) : sids = ra . keys ( ) otumeans = defaultdict ( int ) for oid in otuIDs : otumeans [ oid ] = sum ( [ ra [ sid ] [ oid ] for sid in sids if oid in ra [ sid ] ] ) / len ( sids ) * 100 return otumeans'], ['Calculate the mean relative abundance percentage .', 'def MRA ( biomf , sampleIDs = None , transform = None ) : ra = relative_abundance ( biomf , sampleIDs ) if transform is not None : ra = { sample : { otuID : transform ( abd ) for otuID , abd in ra [ sample ] . items ( ) } for sample in ra . keys ( ) } otuIDs = biomf . ids ( axis = \"observation\" ) return mean_otu_pct_abundance ( ra , otuIDs )'], ['Calculate the total number of sequences in each OTU or SampleID .', 'def raw_abundance ( biomf , sampleIDs = None , sample_abd = True ) : results = defaultdict ( int ) if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating raw total abundances: The sampleIDs provided \" \"do not match the sampleIDs in biom file. Please double check the \" \"sampleIDs provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) for sampleID in sampleIDs : for otuID in otuIDs : abd = biomf . get_value_by_ids ( otuID , sampleID ) if sample_abd : results [ sampleID ] += abd else : results [ otuID ] += abd return results'], ['Function to transform the total abundance calculation for each sample ID to another format based on user given transformation function .', 'def transform_raw_abundance ( biomf , fn = math . log10 , sampleIDs = None , sample_abd = True ) : totals = raw_abundance ( biomf , sampleIDs , sample_abd ) return { sid : fn ( abd ) for sid , abd in totals . items ( ) }'], ['Compute the Mann - Whitney U test for unequal group sample sizes .', 'def print_MannWhitneyU ( div_calc ) : try : x = div_calc . values ( ) [ 0 ] . values ( ) y = div_calc . values ( ) [ 1 ] . values ( ) except : return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \" \"significance testing.\" T , p = stats . mannwhitneyu ( x , y ) print \"\\\\nMann-Whitney U test statistic:\" , T print \"Two-tailed p-value: {}\" . format ( 2 * p )'], ['Compute the Kruskal - Wallis H - test for independent samples . A typical rule is that each group must have at least 5 measurements .', 'def print_KruskalWallisH ( div_calc ) : calc = defaultdict ( list ) try : for k1 , v1 in div_calc . iteritems ( ) : for k2 , v2 in v1 . iteritems ( ) : calc [ k1 ] . append ( v2 ) except : return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \" \"significance testing.\" h , p = stats . kruskal ( * calc . values ( ) ) print \"\\\\nKruskal-Wallis H-test statistic for {} groups: {}\" . format ( str ( len ( div_calc ) ) , h ) print \"p-value: {}\" . format ( p )'], ['Parses the given options passed in at the command line .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Calculate the alpha diversity\\\\                                     of a set of samples using one or more \\\\                                     metrics and output a kernal density \\\\                                     estimator-smoothed histogram of the \\\\                                     results.\" ) parser . add_argument ( \"-m\" , \"--map_file\" , help = \"QIIME mapping file.\" ) parser . add_argument ( \"-i\" , \"--biom_fp\" , help = \"Path to the BIOM table\" ) parser . add_argument ( \"-c\" , \"--category\" , help = \"Specific category from the mapping file.\" ) parser . add_argument ( \"-d\" , \"--diversity\" , default = [ \"shannon\" ] , nargs = \"+\" , help = \"The alpha diversity metric. Default \\\\                             value is \\'shannon\\', which will calculate the Shannon\\\\                             entropy. Multiple metrics can be specified (space separated).\\\\                             The full list of metrics is available at:\\\\                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\\                             Beta diversity metrics will be supported in the future.\" ) parser . add_argument ( \"--x_label\" , default = [ None ] , nargs = \"+\" , help = \"The name of the diversity metric to be displayed on the\\\\                        plot as the X-axis label. If multiple metrics are specified,\\\\                        then multiple entries for the X-axis label should be given.\" ) parser . add_argument ( \"--color_by\" , help = \"A column name in the mapping file containing\\\\                              hexadecimal (#FF0000) color values that will\\\\                              be used to color the groups. Each sample ID must\\\\                              have a color entry.\" ) parser . add_argument ( \"--plot_title\" , default = \"\" , help = \"A descriptive title that will appear at the top \\\\                        of the output plot. Surround with quotes if there are\\\\                        spaces in the title.\" ) parser . add_argument ( \"-o\" , \"--output_dir\" , default = \".\" , help = \"The directory plots will be saved to.\" ) parser . add_argument ( \"--image_type\" , default = \"png\" , help = \"The type of image to save: png, svg, pdf, eps, etc...\" ) parser . add_argument ( \"--save_calculations\" , help = \"Path and name of text file to store the calculated \" \"diversity metrics.\" ) parser . add_argument ( \"--suppress_stats\" , action = \"store_true\" , help = \"Do not display \" \"significance testing results which are shown by default.\" ) parser . add_argument ( \"--show_available_metrics\" , action = \"store_true\" , help = \"Supply this parameter to see which alpha diversity metrics \" \" are available for usage. No calculations will be performed\" \" if this parameter is provided.\" ) return parser . parse_args ( )'], ['make blast db', \"def blastdb ( fasta , maxfile = 10000000 ) : db = fasta . rsplit ( '.' , 1 ) [ 0 ] type = check_type ( fasta ) if type == 'nucl' : type = [ 'nhr' , type ] else : type = [ 'phr' , type ] if os . path . exists ( '%s.%s' % ( db , type [ 0 ] ) ) is False and os . path . exists ( '%s.00.%s' % ( db , type [ 0 ] ) ) is False : print ( '# ... making blastdb for: %s' % ( fasta ) , file = sys . stderr ) os . system ( 'makeblastdb \\\\                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' % ( fasta , db , type [ 1 ] , maxfile ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['make usearch db', \"def usearchdb ( fasta , alignment = 'local' , usearch_loc = 'usearch' ) : if '.udb' in fasta : print ( '# ... database found: %s' % ( fasta ) , file = sys . stderr ) return fasta type = check_type ( fasta ) db = '%s.%s.udb' % ( fasta . rsplit ( '.' , 1 ) [ 0 ] , type ) if os . path . exists ( db ) is False : print ( '# ... making usearch db for: %s' % ( fasta ) , file = sys . stderr ) if alignment == 'local' : os . system ( '%s -makeudb_ublast %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) elif alignment == 'global' : os . system ( '%s -makeudb_usearch %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['Pretty print .', \"def _pp ( dict_data ) : for key , val in dict_data . items ( ) : print ( '{0:<11}: {1}' . format ( key , val ) )\"], ['Print licenses .', \"def print_licences ( params , metadata ) : if hasattr ( params , 'licenses' ) : if params . licenses : _pp ( metadata . licenses_desc ( ) ) sys . exit ( 0 )\"], ['Check repository existence .', 'def check_repository_existence ( params ) : repodir = os . path . join ( params . outdir , params . name ) if os . path . isdir ( repodir ) : raise Conflict ( \\'Package repository \"{0}\" has already exists.\\' . format ( repodir ) )'], ['Generate package repository .', 'def generate_package ( params ) : pkg_data = package . PackageData ( params ) pkg_tree = package . PackageTree ( pkg_data ) pkg_tree . generate ( ) pkg_tree . move ( ) VCS ( os . path . join ( pkg_tree . outdir , pkg_tree . name ) , pkg_tree . pkg_data )'], ['print single reads to stderr', \"def print_single ( line , rev ) : if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] fq = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] print ( '\\\\n' . join ( fq ) , file = sys . stderr )\"], ['convert sam to fastq', \"def sam2fastq ( sam , singles = False , force = False ) : L , R = None , None for line in sam : if line . startswith ( '@' ) is True : continue line = line . strip ( ) . split ( ) bit = [ True if i == '1' else False for i in bin ( int ( line [ 1 ] ) ) . split ( 'b' ) [ 1 ] [ : : - 1 ] ] while len ( bit ) < 8 : bit . append ( False ) pair , proper , na , nap , rev , mrev , left , right = bit if pair is False : if singles is True : print_single ( line , rev ) continue if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] if left is True : if L is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if L is not None : L = None continue L = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if R is not None : yield L yield R L , R = None , None if right is True : if R is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if R is not None : R = None continue R = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if L is not None : yield L yield R L , R = None , None\"], ['sort sam file', 'def sort_sam ( sam , sort ) : tempdir = \\'%s/\\' % ( os . path . abspath ( sam ) . rsplit ( \\'/\\' , 1 ) [ 0 ] ) if sort is True : mapping = \\'%s.sorted.sam\\' % ( sam . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if sam != \\'-\\' : if os . path . exists ( mapping ) is False : os . system ( \"\\\\                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\\                    \" % ( sbuffer , tempdir , mapping , sam ) ) else : mapping = \\'stdin-sam.sorted.sam\\' p = Popen ( \"sort -k1 --buffer-size=%sG -T %s -o %s\" % ( sbuffer , tempdir , mapping ) , stdin = sys . stdin , shell = True ) p . communicate ( ) mapping = open ( mapping ) else : if sam == \\'-\\' : mapping = sys . stdin else : mapping = open ( sam ) return mapping'], ['randomly subset sam file', \"def sub_sam ( sam , percent , sort = True , sbuffer = False ) : mapping = sort_sam ( sam , sort ) pool = [ 1 for i in range ( 0 , percent ) ] + [ 0 for i in range ( 0 , 100 - percent ) ] c = cycle ( [ 1 , 2 ] ) for line in mapping : line = line . strip ( ) . split ( ) if line [ 0 ] . startswith ( '@' ) : yield line continue if int ( line [ 1 ] ) <= 20 : if random . choice ( pool ) == 1 : yield line else : n = next ( c ) if n == 1 : prev = line if n == 2 and random . choice ( pool ) == 1 : yield prev yield line\"], ['convert fq to fa', \"def fq2fa ( fq ) : c = cycle ( [ 1 , 2 , 3 , 4 ] ) for line in fq : n = next ( c ) if n == 1 : seq = [ '>%s' % ( line . strip ( ) . split ( '@' , 1 ) [ 1 ] ) ] if n == 2 : seq . append ( line . strip ( ) ) yield seq\"], ['Converts the returned value of wrapped function to the type of the first arg or to the type specified by a kwarg key return_type s value .', \"def change_return_type ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) elif len ( args ) > 0 : return_type = type ( args [ 0 ] ) return return_type ( f ( * args , ** kwargs ) ) else : return f ( * args , ** kwargs ) return wrapper\"], ['Converts all args to set type via self . setify function .', 'def convert_args_to_sets ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : args = ( setify ( x ) for x in args ) return f ( * args , ** kwargs ) return wrapper'], ['Membuat objek - objek entri dari laman yang diambil .', \"def _init_entri ( self , laman ) : sup = BeautifulSoup ( laman . text , 'html.parser' ) estr = '' for label in sup . find ( 'hr' ) . next_siblings : if label . name == 'hr' : self . entri . append ( Entri ( estr ) ) break if label . name == 'h2' : if estr : self . entri . append ( Entri ( estr ) ) estr = '' estr += str ( label ) . strip ( )\"], ['Memproses kata dasar yang ada dalam nama entri .', \"def _init_kata_dasar ( self , dasar ) : for tiap in dasar : kata = tiap . find ( 'a' ) dasar_no = kata . find ( 'sup' ) kata = ambil_teks_dalam_label ( kata ) self . kata_dasar . append ( kata + ' [{}]' . format ( dasar_no . text . strip ( ) ) if dasar_no else kata )\"], ['Mengembalikan hasil serialisasi objek Entri ini .', 'def serialisasi ( self ) : return { \"nama\" : self . nama , \"nomor\" : self . nomor , \"kata_dasar\" : self . kata_dasar , \"pelafalan\" : self . pelafalan , \"bentuk_tidak_baku\" : self . bentuk_tidak_baku , \"varian\" : self . varian , \"makna\" : [ makna . serialisasi ( ) for makna in self . makna ] }'], ['Mengembalikan representasi string untuk semua makna entri ini .', 'def _makna ( self ) : if len ( self . makna ) > 1 : return \\'\\\\n\\' . join ( str ( i ) + \". \" + str ( makna ) for i , makna in enumerate ( self . makna , 1 ) ) return str ( self . makna [ 0 ] )'], ['Mengembalikan representasi string untuk nama entri ini .', 'def _nama ( self ) : hasil = self . nama if self . nomor : hasil += \" [{}]\" . format ( self . nomor ) if self . kata_dasar : hasil = \" » \". j oin( s elf. k ata_dasar)      » \" + h sil return hasil'], ['Mengembalikan representasi string untuk varian entri ini . Dapat digunakan untuk Varian maupun Bentuk tidak baku .', 'def _varian ( self , varian ) : if varian == self . bentuk_tidak_baku : nama = \"Bentuk tidak baku\" elif varian == self . varian : nama = \"Varian\" else : return \\'\\' return nama + \\': \\' + \\', \\' . join ( varian )'], ['Memproses kelas kata yang ada dalam makna .', \"def _init_kelas ( self , makna_label ) : kelas = makna_label . find ( color = 'red' ) lain = makna_label . find ( color = 'darkgreen' ) info = makna_label . find ( color = 'green' ) if kelas : kelas = kelas . find_all ( 'span' ) if lain : self . kelas = { lain . text . strip ( ) : lain [ 'title' ] . strip ( ) } self . submakna = lain . next_sibling . strip ( ) self . submakna += ' ' + makna_label . find ( color = 'grey' ) . text . strip ( ) else : self . kelas = { k . text . strip ( ) : k [ 'title' ] . strip ( ) for k in kelas } if kelas else { } self . info = info . text . strip ( ) if info else ''\"], ['Memproses contoh yang ada dalam makna .', \"def _init_contoh ( self , makna_label ) : indeks = makna_label . text . find ( ': ' ) if indeks != - 1 : contoh = makna_label . text [ indeks + 2 : ] . strip ( ) self . contoh = contoh . split ( '; ' ) else : self . contoh = [ ]\"], ['Mengembalikan hasil serialisasi objek Makna ini .', 'def serialisasi ( self ) : return { \"kelas\" : self . kelas , \"submakna\" : self . submakna , \"info\" : self . info , \"contoh\" : self . contoh }'], ['Build sphinx documentation .', 'def build_sphinx ( pkg_data , projectdir ) : try : version , _minor_version = pkg_data . version . rsplit ( \\'.\\' , 1 ) except ValueError : version = pkg_data . version args = \\' \\' . join ( ( \\'sphinx-quickstart\\' , \\'--sep\\' , \\'-q\\' , \\'-p \"{name}\"\\' , \\'-a \"{author}\"\\' , \\'-v \"{version}\"\\' , \\'-r \"{release}\"\\' , \\'-l en\\' , \\'--suffix=.rst\\' , \\'--master=index\\' , \\'--ext-autodoc\\' , \\'--ext-viewcode\\' , \\'--makefile\\' , \\'{projectdir}\\' ) ) . format ( name = pkg_data . name , author = pkg_data . author , version = version , release = pkg_data . version , projectdir = projectdir ) if subprocess . call ( shlex . split ( args ) ) == 0 : _touch_gitkeep ( projectdir )'], ['make bowtie db', \"def bowtiedb ( fa , keepDB ) : btdir = '%s/bt2' % ( os . getcwd ( ) ) if not os . path . exists ( btdir ) : os . mkdir ( btdir ) btdb = '%s/%s' % ( btdir , fa . rsplit ( '/' , 1 ) [ - 1 ] ) if keepDB is True : if os . path . exists ( '%s.1.bt2' % ( btdb ) ) : return btdb p = subprocess . Popen ( 'bowtie2-build -q %s %s' % ( fa , btdb ) , shell = True ) p . communicate ( ) return btdb\"], ['generate bowtie2 command', \"def bowtie ( sam , btd , f , r , u , opt , no_shrink , threads ) : bt2 = 'bowtie2 -x %s -p %s ' % ( btd , threads ) if f is not False : bt2 += '-1 %s -2 %s ' % ( f , r ) if u is not False : bt2 += '-U %s ' % ( u ) bt2 += opt if no_shrink is False : if f is False : bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' | shrinksam -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' > %s.sam' % ( sam ) return bt2\"], ['map all read sets against all fasta files', \"def crossmap ( fas , reads , options , no_shrink , keepDB , threads , cluster , nodes ) : if cluster is True : threads = '48' btc = [ ] for fa in fas : btd = bowtiedb ( fa , keepDB ) F , R , U = reads if F is not False : if U is False : u = False for i , f in enumerate ( F ) : r = R [ i ] if U is not False : u = U [ i ] sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , f . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) else : f = False r = False for u in U : sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , u . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) if cluster is False : for i in btc : p = subprocess . Popen ( i , shell = True ) p . communicate ( ) else : ID = '' . join ( random . choice ( [ str ( i ) for i in range ( 0 , 9 ) ] ) for _ in range ( 5 ) ) for node , commands in enumerate ( chunks ( btc , nodes ) , 1 ) : bs = open ( '%s/crossmap-qsub.%s.%s.sh' % ( os . getcwd ( ) , ID , node ) , 'w' ) print ( '\\\\n' . join ( commands ) , file = bs ) bs . close ( ) p = subprocess . Popen ( 'qsub -V -N crossmap %s' % ( bs . name ) , shell = True ) p . communicate ( )\"], ['Returns a connection object from the router given args .', \"def get_conn ( self , * args , ** kwargs ) : connections = self . __connections_for ( 'get_conn' , args = args , kwargs = kwargs ) if len ( connections ) is 1 : return connections [ 0 ] else : return connections\"], ['return the non - direct init if the direct algorithm has been selected .', 'def __get_nondirect_init ( self , init ) : crc = init for i in range ( self . Width ) : bit = crc & 0x01 if bit : crc ^= self . Poly crc >>= 1 if bit : crc |= self . MSB_Mask return crc & self . Mask'], ['reflect a data word i . e . reverts the bit order .', 'def reflect ( self , data , width ) : x = data & 0x01 for i in range ( width - 1 ) : data >>= 1 x = ( x << 1 ) | ( data & 0x01 ) return x'], ['Classic simple and slow CRC implementation . This function iterates bit by bit over the augmented input message and returns the calculated CRC value at the end .', 'def bit_by_bit ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] register = self . NonDirectInit for octet in in_data : if self . ReflectIn : octet = self . reflect ( octet , 8 ) for i in range ( 8 ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) | ( ( octet >> ( 7 - i ) ) & 0x01 ) if topbit : register ^= self . Poly for i in range ( self . Width ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) if topbit : register ^= self . Poly if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['This function generates the CRC table used for the table_driven CRC algorithm . The Python version cannot handle tables of an index width other than 8 . See the generated C code for tables with different sizes instead .', 'def gen_table ( self ) : table_length = 1 << self . TableIdxWidth tbl = [ 0 ] * table_length for i in range ( table_length ) : register = i if self . ReflectIn : register = self . reflect ( register , self . TableIdxWidth ) register = register << ( self . Width - self . TableIdxWidth + self . CrcShift ) for j in range ( self . TableIdxWidth ) : if register & ( self . MSB_Mask << self . CrcShift ) != 0 : register = ( register << 1 ) ^ ( self . Poly << self . CrcShift ) else : register = ( register << 1 ) if self . ReflectIn : register = self . reflect ( register >> self . CrcShift , self . Width ) << self . CrcShift tbl [ i ] = register & ( self . Mask << self . CrcShift ) return tbl'], ['The Standard table_driven CRC algorithm .', 'def table_driven ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] tbl = self . gen_table ( ) register = self . DirectInit << self . CrcShift if not self . ReflectIn : for octet in in_data : tblidx = ( ( register >> ( self . Width - self . TableIdxWidth + self . CrcShift ) ) ^ octet ) & 0xff register = ( ( register << ( self . TableIdxWidth - self . CrcShift ) ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = register >> self . CrcShift else : register = self . reflect ( register , self . Width + self . CrcShift ) << self . CrcShift for octet in in_data : tblidx = ( ( register >> self . CrcShift ) ^ octet ) & 0xff register = ( ( register >> self . TableIdxWidth ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = self . reflect ( register , self . Width + self . CrcShift ) & self . Mask if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['parse masked sequence into non - masked and masked regions', 'def parse_masked ( seq , min_len ) : nm , masked = [ ] , [ [ ] ] prev = None for base in seq [ 1 ] : if base . isupper ( ) : nm . append ( base ) if masked != [ [ ] ] and len ( masked [ - 1 ] ) < min_len : nm . extend ( masked [ - 1 ] ) del masked [ - 1 ] prev = False elif base . islower ( ) : if prev is False : masked . append ( [ ] ) masked [ - 1 ] . append ( base ) prev = True return nm , masked'], ['remove masked regions from fasta file as long as they are longer than min_len', \"def strip_masked ( fasta , min_len , print_masked ) : for seq in parse_fasta ( fasta ) : nm , masked = parse_masked ( seq , min_len ) nm = [ '%s removed_masked >=%s' % ( seq [ 0 ] , min_len ) , '' . join ( nm ) ] yield [ 0 , nm ] if print_masked is True : for i , m in enumerate ( [ i for i in masked if i != [ ] ] , 1 ) : m = [ '%s insertion:%s' % ( seq [ 0 ] , i ) , '' . join ( m ) ] yield [ 1 , m ]\"], ['Return arcsine transformed relative abundance from a BIOM format file .', 'def get_relative_abundance ( biomfile ) : biomf = biom . load_table ( biomfile ) norm_biomf = biomf . norm ( inplace = False ) rel_abd = { } for sid in norm_biomf . ids ( ) : rel_abd [ sid ] = { } for otuid in norm_biomf . ids ( \"observation\" ) : otuname = oc . otu_name ( norm_biomf . metadata ( otuid , axis = \"observation\" ) [ \"taxonomy\" ] ) otuname = \" \" . join ( otuname . split ( \"_\" ) ) abd = norm_biomf . get_value_by_ids ( otuid , sid ) rel_abd [ sid ] [ otuname ] = abd ast_rel_abd = bc . arcsine_sqrt_transform ( rel_abd ) return ast_rel_abd'], ['Find an OTU ID in a Newick - format tree . Return the starting position of the ID or None if not found .', 'def find_otu ( otuid , tree ) : for m in re . finditer ( otuid , tree ) : before , after = tree [ m . start ( ) - 1 ] , tree [ m . start ( ) + len ( otuid ) ] if before in [ \"(\" , \",\" , \")\" ] and after in [ \":\" , \";\" ] : return m . start ( ) return None'], ['Replace the OTU ids in the Newick phylogenetic tree format with truncated OTU names', 'def newick_replace_otuids ( tree , biomf ) : for val , id_ , md in biomf . iter ( axis = \"observation\" ) : otu_loc = find_otu ( id_ , tree ) if otu_loc is not None : tree = tree [ : otu_loc ] + oc . otu_name ( md [ \"taxonomy\" ] ) + tree [ otu_loc + len ( id_ ) : ] return tree'], ['return genome info for choosing representative', \"def genome_info ( genome , info ) : try : scg = info [ '#SCGs' ] dups = info [ '#SCG duplicates' ] length = info [ 'genome size (bp)' ] return [ scg - dups , length , genome ] except : return [ False , False , info [ 'genome size (bp)' ] , genome ]\"], ['choose represenative genome and print cluster information', \"def print_clusters ( fastas , info , ANI ) : header = [ '#cluster' , 'num. genomes' , 'rep.' , 'genome' , '#SCGs' , '#SCG duplicates' , 'genome size (bp)' , 'fragments' , 'list' ] yield header in_cluster = [ ] for cluster_num , cluster in enumerate ( connected_components ( ANI ) ) : cluster = sorted ( [ genome_info ( genome , info [ genome ] ) for genome in cluster ] , key = lambda x : x [ 0 : ] , reverse = True ) rep = cluster [ 0 ] [ - 1 ] cluster = [ i [ - 1 ] for i in cluster ] size = len ( cluster ) for genome in cluster : in_cluster . append ( genome ) try : stats = [ size , rep , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] except : stats = [ size , rep , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] if rep == genome : stats = [ '*%s' % ( cluster_num ) ] + stats else : stats = [ cluster_num ] + stats yield stats try : start = cluster_num + 1 except : start = 0 fastas = set ( [ i . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] for i in fastas ] ) for cluster_num , genome in enumerate ( fastas . difference ( set ( in_cluster ) ) , start ) : try : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] except : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] yield stats\"], ['convert ggKbase genome info tables to dictionary', \"def parse_ggKbase_tables ( tables , id_type ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'name' ) : header = line header [ 4 ] = 'genome size (bp)' header [ 12 ] = '#SCGs' header [ 13 ] = '#SCG duplicates' continue name , code , info = line [ 0 ] , line [ 1 ] , line info = [ to_int ( i ) for i in info ] if id_type is False : if 'UNK' in code or 'unknown' in code : code = name if ( name != code ) and ( name and code in g2info ) : print ( '# duplicate name or code in table(s)' , file = sys . stderr ) print ( '# %s and/or %s' % ( name , code ) , file = sys . stderr ) exit ( ) if name not in g2info : g2info [ name ] = { item : stat for item , stat in zip ( header , info ) } if code not in g2info : g2info [ code ] = { item : stat for item , stat in zip ( header , info ) } else : if id_type == 'name' : ID = name elif id_type == 'code' : ID = code else : print ( '# specify name or code column using -id' , file = sys . stderr ) exit ( ) ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['convert checkM genome info tables to dictionary', \"def parse_checkM_tables ( tables ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'Bin Id' ) : header = line header [ 8 ] = 'genome size (bp)' header [ 5 ] = '#SCGs' header [ 6 ] = '#SCG duplicates' continue ID , info = line [ 0 ] , line info = [ to_int ( i ) for i in info ] ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['get genome lengths', \"def genome_lengths ( fastas , info ) : if info is False : info = { } for genome in fastas : name = genome . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] if name in info : continue length = 0 fragments = 0 for seq in parse_fasta ( genome ) : length += len ( seq [ 1 ] ) fragments += 1 info [ name ] = { 'genome size (bp)' : length , '# contigs' : fragments } return info\"], ['Returns a list of db keys to route the given call to .', 'def get_dbs ( self , attr , args , kwargs , ** fkwargs ) : if not self . _ready : if not self . setup_router ( args = args , kwargs = kwargs , ** fkwargs ) : raise self . UnableToSetupRouter ( ) retval = self . _pre_routing ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) if retval is not None : args , kwargs = retval if not ( args or kwargs ) : return self . cluster . hosts . keys ( ) try : db_nums = self . _route ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) except Exception as e : self . _handle_exception ( e ) db_nums = [ ] return self . _post_routing ( attr = attr , db_nums = db_nums , args = args , kwargs = kwargs , ** fkwargs )'], ['Call method to perform any setup', 'def setup_router ( self , args , kwargs , ** fkwargs ) : self . _ready = self . _setup_router ( args = args , kwargs = kwargs , ** fkwargs ) return self . _ready'], ['Perform routing and return db_nums', 'def _route ( self , attr , args , kwargs , ** fkwargs ) : return self . cluster . hosts . keys ( )'], ['Iterates through all connections which were previously listed as unavailable and marks any that have expired their retry_timeout as being up .', 'def check_down_connections ( self ) : now = time . time ( ) for db_num , marked_down_at in self . _down_connections . items ( ) : if marked_down_at + self . retry_timeout <= now : self . mark_connection_up ( db_num )'], ['Marks all connections which were previously listed as unavailable as being up .', 'def flush_down_connections ( self ) : self . _get_db_attempts = 0 for db_num in self . _down_connections . keys ( ) : self . mark_connection_up ( db_num )'], ['Compute standby power', \"def standby ( df , resolution = '24h' , time_window = None ) : if df . empty : raise EmptyDataFrame ( ) df = pd . DataFrame ( df ) def parse_time ( t ) : if isinstance ( t , numbers . Number ) : return pd . Timestamp . utcfromtimestamp ( t ) . time ( ) else : return pd . Timestamp ( t ) . time ( ) if time_window is not None : t_start = parse_time ( time_window [ 0 ] ) t_end = parse_time ( time_window [ 1 ] ) if t_start > t_end : df = df [ ( df . index . time >= t_start ) | ( df . index . time < t_end ) ] else : df = df [ ( df . index . time >= t_start ) & ( df . index . time < t_end ) ] return df . resample ( resolution ) . min ( )\"], ['Compute the share of the standby power in the total consumption .', \"def share_of_standby ( df , resolution = '24h' , time_window = None ) : p_sb = standby ( df , resolution , time_window ) df = df . resample ( resolution ) . mean ( ) p_tot = df . sum ( ) p_standby = p_sb . sum ( ) share_standby = p_standby / p_tot res = share_standby . iloc [ 0 ] return res\"], ['Toggle counter for gas boilers', 'def count_peaks ( ts ) : on_toggles = ts . diff ( ) > 3000 shifted = np . logical_not ( on_toggles . shift ( 1 ) ) result = on_toggles & shifted count = result . sum ( ) return count'], ['Calculate the ratio of input vs . norm over a given interval .', 'def load_factor ( ts , resolution = None , norm = None ) : if norm is None : norm = ts . max ( ) if resolution is not None : ts = ts . resample ( rule = resolution ) . mean ( ) lf = ts / norm return lf'], ['get top hits after sorting by column number', 'def top_hits ( hits , num , column , reverse ) : hits . sort ( key = itemgetter ( column ) , reverse = reverse ) for hit in hits [ 0 : num ] : yield hit'], ['parse b6 output with sorting', \"def numBlast_sort ( blast , numHits , evalueT , bitT ) : header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header hmm = { h : [ ] for h in header } for line in blast : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( '\\\\t' ) line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if evalueT is not False and evalue > evalueT : continue if bitT is not False and bit < bitT : continue for i , h in zip ( line , header ) : hmm [ h ] . append ( i ) hmm = pd . DataFrame ( hmm ) for query , df in hmm . groupby ( by = [ '#query' ] ) : df = df . sort_values ( by = [ 'bitscore' ] , ascending = False ) for hit in df [ header ] . values [ 0 : numHits ] : yield hit\"], ['parse b6 output', \"def numBlast ( blast , numHits , evalueT = False , bitT = False , sort = False ) : if sort is True : for hit in numBlast_sort ( blast , numHits , evalueT , bitT ) : yield hit return header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header prev , hits = None , [ ] for line in blast : line = line . strip ( ) . split ( '\\\\t' ) ID = line [ 0 ] line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 11 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 11 , True ) : yield hit\"], ['parse hmm domain table output this version is faster but does not work unless the table is sorted', \"def numDomtblout ( domtblout , numHits , evalueT , bitT , sort ) : if sort is True : for hit in numDomtblout_sort ( domtblout , numHits , evalueT , bitT ) : yield hit return header = [ '#target name' , 'target accession' , 'tlen' , 'query name' , 'query accession' , 'qlen' , 'full E-value' , 'full score' , 'full bias' , 'domain #' , '# domains' , 'domain c-Evalue' , 'domain i-Evalue' , 'domain score' , 'domain bias' , 'hmm from' , 'hmm to' , 'seq from' , 'seq to' , 'env from' , 'env to' , 'acc' , 'target description' ] yield header prev , hits = None , [ ] for line in domtblout : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( ) desc = ' ' . join ( line [ 18 : ] ) line = line [ 0 : 18 ] line . append ( desc ) ID = line [ 0 ] + line [ 9 ] line [ 11 ] , line [ 13 ] = float ( line [ 11 ] ) , float ( line [ 13 ] ) evalue , bitscore = line [ 11 ] , line [ 13 ] line [ 11 ] , line [ 13 ] = evalue , bitscore if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 13 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 13 , True ) : yield hit\"], ['convert stockholm to fasta', \"def stock2fa ( stock ) : seqs = { } for line in stock : if line . startswith ( '#' ) is False and line . startswith ( ' ' ) is False and len ( line ) > 3 : id , seq = line . strip ( ) . split ( ) id = id . rsplit ( '/' , 1 ) [ 0 ] id = re . split ( '[0-9]\\\\|' , id , 1 ) [ - 1 ] if id not in seqs : seqs [ id ] = [ ] seqs [ id ] . append ( seq ) if line . startswith ( '//' ) : break return seqs\"], ['Return boolean time series following given week schedule .', \"def week_schedule ( index , on_time = None , off_time = None , off_days = None ) : if on_time is None : on_time = '9:00' if off_time is None : off_time = '17:00' if off_days is None : off_days = [ 'Sunday' , 'Monday' ] if not isinstance ( on_time , datetime . time ) : on_time = pd . to_datetime ( on_time , format = '%H:%M' ) . time ( ) if not isinstance ( off_time , datetime . time ) : off_time = pd . to_datetime ( off_time , format = '%H:%M' ) . time ( ) times = ( index . time >= on_time ) & ( index . time < off_time ) & ( ~ index . weekday_name . isin ( off_days ) ) return pd . Series ( times , index = index )\"], ['Draw a carpet plot of a pandas timeseries .', 'def carpet ( timeseries , ** kwargs ) : cmap = kwargs . pop ( \\'cmap\\' , cm . coolwarm ) norm = kwargs . pop ( \\'norm\\' , LogNorm ( ) ) interpolation = kwargs . pop ( \\'interpolation\\' , \\'nearest\\' ) cblabel = kwargs . pop ( \\'zlabel\\' , timeseries . name if timeseries . name else \\'\\' ) title = kwargs . pop ( \\'title\\' , \\'carpet plot: \\' + timeseries . name if timeseries . name else \\'\\' ) if timeseries . dropna ( ) . empty : print ( \\'skipped {} - no data\\' . format ( title ) ) return ts = timeseries . resample ( \\'15min\\' ) . interpolate ( ) vmin = max ( 0.1 , kwargs . pop ( \\'vmin\\' , ts [ ts > 0 ] . min ( ) ) ) vmax = max ( vmin , kwargs . pop ( \\'vmax\\' , ts . quantile ( .999 ) ) ) mpldatetimes = date2num ( ts . index . to_pydatetime ( ) ) ts . index = pd . MultiIndex . from_arrays ( [ np . floor ( mpldatetimes ) , 2 + mpldatetimes % 1 ] ) df = ts . unstack ( ) fig , ax = plt . subplots ( ) extent = [ df . columns [ 0 ] , df . columns [ - 1 ] , df . index [ - 1 ] + 0.5 , df . index [ 0 ] - 0.5 ] im = plt . imshow ( df , vmin = vmin , vmax = vmax , extent = extent , cmap = cmap , aspect = \\'auto\\' , norm = norm , interpolation = interpolation , ** kwargs ) ax . xaxis_date ( ) ax . xaxis . set_major_locator ( HourLocator ( interval = 2 ) ) ax . xaxis . set_major_formatter ( DateFormatter ( \\'%H:%M\\' ) ) ax . xaxis . grid ( True ) plt . xlabel ( \\'UTC Time\\' ) ax . yaxis_date ( ) dmin , dmax = ax . yaxis . get_data_interval ( ) number_of_days = ( num2date ( dmax ) - num2date ( dmin ) ) . days if abs ( number_of_days ) <= 35 : ax . yaxis . set_major_locator ( DayLocator ( ) ) else : ax . yaxis . set_major_locator ( AutoDateLocator ( ) ) ax . yaxis . set_major_formatter ( DateFormatter ( \"%a, %d %b %Y\" ) ) cbticks = np . logspace ( np . log10 ( vmin ) , np . log10 ( vmax ) , 11 , endpoint = True ) cb = plt . colorbar ( format = \\'%.0f\\' , ticks = cbticks ) cb . set_label ( cblabel ) plt . title ( title ) return im'], ['calculate percent identity', \"def calc_pident_ignore_gaps ( a , b ) : m = 0 mm = 0 for A , B in zip ( list ( a ) , list ( b ) ) : if A == '-' or A == '.' or B == '-' or B == '.' : continue if A == B : m += 1 else : mm += 1 try : return float ( float ( m ) / float ( ( m + mm ) ) ) * 100 except : return 0\"], ['skip column if either is a gap', \"def remove_gaps ( A , B ) : a_seq , b_seq = [ ] , [ ] for a , b in zip ( list ( A ) , list ( B ) ) : if a == '-' or a == '.' or b == '-' or b == '.' : continue a_seq . append ( a ) b_seq . append ( b ) return '' . join ( a_seq ) , '' . join ( b_seq )\"], ['compare pairs of sequences', \"def compare_seqs ( seqs ) : A , B , ignore_gaps = seqs a , b = A [ 1 ] , B [ 1 ] if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) if ignore_gaps is True : pident = calc_pident_ignore_gaps ( a , b ) else : pident = calc_pident ( a , b ) return A [ 0 ] , B [ 0 ] , pident\"], ['calculate Levenshtein ratio of sequences', \"def compare_seqs_leven ( seqs ) : A , B , ignore_gaps = seqs a , b = remove_gaps ( A [ 1 ] , B [ 1 ] ) if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) pident = lr ( a , b ) * 100 return A [ 0 ] , B [ 0 ] , pident\"], ['make pairwise sequence comparisons between aligned sequences', 'def pairwise_compare ( afa , leven , threads , print_list , ignore_gaps ) : seqs = { seq [ 0 ] : seq for seq in nr_fasta ( [ afa ] , append_index = True ) } num_seqs = len ( seqs ) pairs = ( ( i [ 0 ] , i [ 1 ] , ignore_gaps ) for i in itertools . combinations ( list ( seqs . values ( ) ) , 2 ) ) pool = multithread ( threads ) if leven is True : pident = pool . map ( compare_seqs_leven , pairs ) else : compare = pool . imap_unordered ( compare_seqs , pairs ) pident = [ i for i in tqdm ( compare , total = ( num_seqs * num_seqs ) / 2 ) ] pool . close ( ) pool . terminate ( ) pool . join ( ) return to_dictionary ( pident , print_list )'], ['print matrix of pidents to stdout', \"def print_pairwise ( pw , median = False ) : names = sorted ( set ( [ i for i in pw ] ) ) if len ( names ) != 0 : if '>' in names [ 0 ] : yield [ '#' ] + [ i . split ( '>' ) [ 1 ] for i in names if '>' in i ] else : yield [ '#' ] + names for a in names : if '>' in a : yield [ a . split ( '>' ) [ 1 ] ] + [ pw [ a ] [ b ] for b in names ] else : out = [ ] for b in names : if b in pw [ a ] : if median is False : out . append ( max ( pw [ a ] [ b ] ) ) else : out . append ( np . median ( pw [ a ] [ b ] ) ) else : out . append ( '-' ) yield [ a ] + out\"], ['print stats for comparisons', \"def print_comps ( comps ) : if comps == [ ] : print ( 'n/a' ) else : print ( '# min: %s, max: %s, mean: %s' % ( min ( comps ) , max ( comps ) , np . mean ( comps ) ) )\"], ['print min . pident within each clade and then matrix of between - clade max .', \"def compare_clades ( pw ) : names = sorted ( set ( [ i for i in pw ] ) ) for i in range ( 0 , 4 ) : wi , bt = { } , { } for a in names : for b in pw [ a ] : if ';' not in a or ';' not in b : continue pident = pw [ a ] [ b ] cA , cB = a . split ( ';' ) [ i ] , b . split ( ';' ) [ i ] if i == 0 and '_' in cA and '_' in cB : cA = cA . rsplit ( '_' , 1 ) [ 1 ] cB = cB . rsplit ( '_' , 1 ) [ 1 ] elif '>' in cA or '>' in cB : cA = cA . split ( '>' ) [ 1 ] cB = cB . split ( '>' ) [ 1 ] if cA == cB : if cA not in wi : wi [ cA ] = [ ] wi [ cA ] . append ( pident ) else : if cA not in bt : bt [ cA ] = { } if cB not in bt [ cA ] : bt [ cA ] [ cB ] = [ ] bt [ cA ] [ cB ] . append ( pident ) print ( '\\\\n# min. within' ) for clade , pidents in list ( wi . items ( ) ) : print ( '\\\\t' . join ( [ 'wi:%s' % str ( i ) , clade , str ( min ( pidents ) ) ] ) ) comps = [ ] print ( '\\\\n# max. between' ) for comp in print_pairwise ( bt ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps ) comps = [ ] print ( '\\\\n# median between' ) for comp in print_pairwise ( bt , median = True ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps )\"], ['convert matrix to dictionary of comparisons', \"def matrix2dictionary ( matrix ) : pw = { } for line in matrix : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : names = line [ 1 : ] continue a = line [ 0 ] for i , pident in enumerate ( line [ 1 : ] ) : b = names [ i ] if a not in pw : pw [ a ] = { } if b not in pw : pw [ b ] = { } if pident != '-' : pident = float ( pident ) pw [ a ] [ b ] = pident pw [ b ] [ a ] = pident return pw\"], ['Set argument parser option .', \"def setoption ( parser , metadata = None ) : parser . add_argument ( '-v' , action = 'version' , version = __version__ ) subparsers = parser . add_subparsers ( help = 'sub commands help' ) create_cmd = subparsers . add_parser ( 'create' ) create_cmd . add_argument ( 'name' , help = 'Specify Python package name.' ) create_cmd . add_argument ( '-d' , dest = 'description' , action = 'store' , help = 'Short description about your package.' ) create_cmd . add_argument ( '-a' , dest = 'author' , action = 'store' , required = True , help = 'Python package author name.' ) create_cmd . add_argument ( '-e' , dest = 'email' , action = 'store' , required = True , help = 'Python package author email address.' ) create_cmd . add_argument ( '-l' , dest = 'license' , choices = metadata . licenses ( ) . keys ( ) , default = 'GPLv3+' , help = 'Specify license. (default: %(default)s)' ) create_cmd . add_argument ( '-s' , dest = 'status' , choices = metadata . status ( ) . keys ( ) , default = 'Alpha' , help = ( 'Specify development status. ' '(default: %(default)s)' ) ) create_cmd . add_argument ( '--no-check' , action = 'store_true' , help = 'No checking package name in PyPI.' ) create_cmd . add_argument ( '--with-samples' , action = 'store_true' , help = 'Generate package with sample code.' ) group = create_cmd . add_mutually_exclusive_group ( required = True ) group . add_argument ( '-U' , dest = 'username' , action = 'store' , help = 'Specify GitHub username.' ) group . add_argument ( '-u' , dest = 'url' , action = 'store' , type = valid_url , help = 'Python package homepage url.' ) create_cmd . add_argument ( '-o' , dest = 'outdir' , action = 'store' , default = os . path . abspath ( os . path . curdir ) , help = 'Specify output directory. (default: $PWD)' ) list_cmd = subparsers . add_parser ( 'list' ) list_cmd . add_argument ( '-l' , dest = 'licenses' , action = 'store_true' , help = 'show license choices.' )\"], ['Parse argument options .', \"def parse_options ( metadata ) : parser = argparse . ArgumentParser ( description = '%(prog)s usage:' , prog = __prog__ ) setoption ( parser , metadata = metadata ) return parser\"], ['Execute main processes .', \"def main ( ) : try : pkg_version = Update ( ) if pkg_version . updatable ( ) : pkg_version . show_message ( ) metadata = control . retreive_metadata ( ) parser = parse_options ( metadata ) argvs = sys . argv if len ( argvs ) <= 1 : parser . print_help ( ) sys . exit ( 1 ) args = parser . parse_args ( ) control . print_licences ( args , metadata ) control . check_repository_existence ( args ) control . check_package_existence ( args ) control . generate_package ( args ) except ( RuntimeError , BackendFailure , Conflict ) as exc : sys . stderr . write ( '{0}\\\\n' . format ( exc ) ) sys . exit ( 1 )\"], ['Check key and set default vaule when it does not exists .', \"def _check_or_set_default_params ( self ) : if not hasattr ( self , 'date' ) : self . _set_param ( 'date' , datetime . utcnow ( ) . strftime ( '%Y-%m-%d' ) ) if not hasattr ( self , 'version' ) : self . _set_param ( 'version' , self . default_version ) if not hasattr ( self , 'description' ) or self . description is None : getattr ( self , '_set_param' ) ( 'description' , self . warning_message )\"], ['Move directory from working directory to output directory .', 'def move ( self ) : if not os . path . isdir ( self . outdir ) : os . makedirs ( self . outdir ) shutil . move ( self . tmpdir , os . path . join ( self . outdir , self . name ) )'], ['Initialize VCS repository .', 'def vcs_init ( self ) : VCS ( os . path . join ( self . outdir , self . name ) , self . pkg_data )'], ['Finds the location of the current Steam installation on Windows machines . Returns None for any non - Windows machines or for Windows machines where Steam is not installed .', 'def find_steam_location ( ) : if registry is None : return None key = registry . CreateKey ( registry . HKEY_CURRENT_USER , \"Software\\\\Valve\\\\Steam\" ) return registry . QueryValueEx ( key , \"SteamPath\" ) [ 0 ]'], ['Plot PCoA principal coordinates scaled by the relative abundances of otu_name .', 'def plot_PCoA ( cat_data , otu_name , unifrac , names , colors , xr , yr , outDir , save_as , plot_style ) : fig = plt . figure ( figsize = ( 14 , 8 ) ) ax = fig . add_subplot ( 111 ) for i , cat in enumerate ( cat_data ) : plt . scatter ( cat_data [ cat ] [ \"pc1\" ] , cat_data [ cat ] [ \"pc2\" ] , cat_data [ cat ] [ \"size\" ] , color = colors [ cat ] , alpha = 0.85 , marker = \"o\" , edgecolor = \"black\" , label = cat ) lgnd = plt . legend ( loc = \"best\" , scatterpoints = 3 , fontsize = 13 ) for i in range ( len ( colors . keys ( ) ) ) : lgnd . legendHandles [ i ] . _sizes = [ 80 ] plt . title ( \" \" . join ( otu_name . split ( \"_\" ) ) , style = \"italic\" ) plt . ylabel ( \"PC2 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 1 ] ) ) ) plt . xlabel ( \"PC1 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 0 ] ) ) ) plt . xlim ( round ( xr [ 0 ] * 1.5 , 1 ) , round ( xr [ 1 ] * 1.5 , 1 ) ) plt . ylim ( round ( yr [ 0 ] * 1.5 , 1 ) , round ( yr [ 1 ] * 1.5 , 1 ) ) if plot_style : gu . ggplot2_style ( ax ) fc = \"0.8\" else : fc = \"none\" fig . savefig ( os . path . join ( outDir , \"_\" . join ( otu_name . split ( ) ) ) + \".\" + save_as , facecolor = fc , edgecolor = \"none\" , format = save_as , bbox_inches = \"tight\" , pad_inches = 0.2 ) plt . close ( fig )'], ['Split up the column data in a biom table by mapping category value .', \"def split_by_category ( biom_cols , mapping , category_id ) : columns = defaultdict ( list ) for i , col in enumerate ( biom_cols ) : columns [ mapping [ col [ 'id' ] ] [ category_id ] ] . append ( ( i , col ) ) return columns\"], ['print line if starts with ...', \"def print_line ( l ) : print_lines = [ '# STOCKHOLM' , '#=GF' , '#=GS' , ' ' ] if len ( l . split ( ) ) == 0 : return True for start in print_lines : if l . startswith ( start ) : return True return False\"], ['convert stockholm to single line format', \"def stock2one ( stock ) : lines = { } for line in stock : line = line . strip ( ) if print_line ( line ) is True : yield line continue if line . startswith ( '//' ) : continue ID , seq = line . rsplit ( ' ' , 1 ) if ID not in lines : lines [ ID ] = '' else : seq = seq . strip ( ) lines [ ID ] += seq for ID , line in lines . items ( ) : yield '\\\\t' . join ( [ ID , line ] ) yield '\\\\n//'\"], ['Statics the methods . wut .', \"def math_func ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 0 : return_type = type ( args [ 0 ] ) if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) args = list ( ( setify ( x ) for x in args ) ) return return_type ( f ( * args , ** kwargs ) ) return wrapper\"], ['Show stats when pings are done', 'def dump_stats ( myStats ) : print ( \"\\\\n----%s PYTHON PING Statistics----\" % ( myStats . thisIP ) ) if myStats . pktsSent > 0 : myStats . fracLoss = ( myStats . pktsSent - myStats . pktsRcvd ) / myStats . pktsSent print ( ( \"%d packets transmitted, %d packets received, \" \"%0.1f%% packet loss\" ) % ( myStats . pktsSent , myStats . pktsRcvd , 100.0 * myStats . fracLoss ) ) if myStats . pktsRcvd > 0 : print ( \"round-trip (ms)  min/avg/max = %d/%0.1f/%d\" % ( myStats . minTime , myStats . totTime / myStats . pktsRcvd , myStats . maxTime ) ) print ( \"\" ) return'], ['bootstrap - py package updatable? .', 'def updatable ( self ) : if self . latest_version > self . current_version : updatable_version = self . latest_version else : updatable_version = False return updatable_version'], ['Show message updatable .', \"def show_message ( self ) : print ( 'current version: {current_version}\\\\n' 'latest version : {latest_version}' . format ( current_version = self . current_version , latest_version = self . latest_version ) )\"], ['Traverse the input otu - sequence file collect the non - unique OTU IDs and file the sequences associated with then under the unique OTU ID as defined by the input matrix .', 'def condense_otus ( otuF , nuniqueF ) : uniqueOTUs = set ( ) nuOTUs = { } for line in nuniqueF : line = line . split ( ) uOTU = line [ 0 ] for nuOTU in line [ 1 : ] : nuOTUs [ nuOTU ] = uOTU uniqueOTUs . add ( uOTU ) otuFilter = defaultdict ( list ) for line in otuF : line = line . split ( ) otuID , seqIDs = line [ 0 ] , line [ 1 : ] if otuID in uniqueOTUs : otuFilter [ otuID ] . extend ( seqIDs ) elif otuID in nuOTUs : otuFilter [ nuOTUs [ otuID ] ] . extend ( seqIDs ) return otuFilter'], ['determine if read overlaps with rna if so count bases', 'def rna_bases ( rna_cov , scaffold , bases , line ) : start = int ( line [ 3 ] ) stop = start + bases - 1 if scaffold not in rna_cov : return rna_cov for pos in rna_cov [ scaffold ] [ 2 ] : ol = get_overlap ( [ start , stop ] , pos ) rna_cov [ scaffold ] [ 0 ] += ol return rna_cov'], ['parse ggKbase scaffold - to - bin mapping - scaffolds - to - bins and bins - to - scaffolds', \"def parse_s2bins ( s2bins ) : s2b = { } b2s = { } for line in s2bins : line = line . strip ( ) . split ( ) s , b = line [ 0 ] , line [ 1 ] if 'UNK' in b : continue if len ( line ) > 2 : g = ' ' . join ( line [ 2 : ] ) else : g = 'n/a' b = '%s\\\\t%s' % ( b , g ) s2b [ s ] = b if b not in b2s : b2s [ b ] = [ ] b2s [ b ] . append ( s ) return s2b , b2s\"], ['remove any bins that don t have 16S', 'def filter_missing_rna ( s2bins , bins2s , rna_cov ) : for bin , scaffolds in list ( bins2s . items ( ) ) : c = 0 for s in scaffolds : if s in rna_cov : c += 1 if c == 0 : del bins2s [ bin ] for scaffold , bin in list ( s2bins . items ( ) ) : if bin not in bins2s : del s2bins [ scaffold ] return s2bins , bins2s'], ['calculate bin coverage', 'def calc_bin_cov ( scaffolds , cov ) : bases = sum ( [ cov [ i ] [ 0 ] for i in scaffolds if i in cov ] ) length = sum ( [ cov [ i ] [ 1 ] for i in scaffolds if i in cov ] ) if length == 0 : return 0 return float ( float ( bases ) / float ( length ) )'], ['Make sure there is at least a translation has been filled in . If a default language has been specified make sure that it exists amongst translations .', \"def clean ( self ) : super ( TranslationFormSet , self ) . clean ( ) if settings . HIDE_LANGUAGE : return if len ( self . forms ) > 0 : if settings . DEFAULT_LANGUAGE and not any ( self . errors ) : for form in self . forms : language_code = form . cleaned_data . get ( 'language_code' , None ) if language_code == settings . DEFAULT_LANGUAGE : return raise forms . ValidationError ( _ ( 'No translation provided for default language \\\\'%s\\\\'.' ) % settings . DEFAULT_LANGUAGE ) else : raise forms . ValidationError ( _ ( 'At least one translation should be provided.' ) )\"], ['If a default language has been set and is still available in self . available_languages return it and remove it from the list .', \"def _get_default_language ( self ) : assert hasattr ( self , 'available_languages' ) , 'No available languages have been generated.' assert len ( self . available_languages ) > 0 , 'No available languages to select from.' if ( settings . DEFAULT_LANGUAGE and settings . DEFAULT_LANGUAGE in self . available_languages ) or ( 'language_code' not in self . form . base_fields ) : self . available_languages . remove ( settings . DEFAULT_LANGUAGE ) return settings . DEFAULT_LANGUAGE else : return self . available_languages . pop ( 0 )\"], ['Construct the form overriding the initial value for language_code .', \"def _construct_form ( self , i , ** kwargs ) : if not settings . HIDE_LANGUAGE : self . _construct_available_languages ( ) form = super ( TranslationFormSet , self ) . _construct_form ( i , ** kwargs ) if settings . HIDE_LANGUAGE : form . instance . language_code = settings . DEFAULT_LANGUAGE else : language_code = form . instance . language_code if language_code : logger . debug ( u'Removing translation choice %s for instance %s' u' in form %d' , language_code , form . instance , i ) self . available_languages . remove ( language_code ) else : initial_language_code = self . _get_default_language ( ) logger . debug ( u'Preselecting language code %s for form %d' , initial_language_code , i ) form . initial [ 'language_code' ] = initial_language_code return form\"], ['merge separate fastq files', 'def fq_merge ( R1 , R2 ) : c = itertools . cycle ( [ 1 , 2 , 3 , 4 ] ) for r1 , r2 in zip ( R1 , R2 ) : n = next ( c ) if n == 1 : pair = [ [ ] , [ ] ] pair [ 0 ] . append ( r1 . strip ( ) ) pair [ 1 ] . append ( r2 . strip ( ) ) if n == 4 : yield pair'], ['Creates hash ring .', \"def _build_circle ( self ) : total_weight = 0 for node in self . _nodes : total_weight += self . _weights . get ( node , 1 ) for node in self . _nodes : weight = self . _weights . get ( node , 1 ) ks = math . floor ( ( 40 * len ( self . _nodes ) * weight ) / total_weight ) for i in xrange ( 0 , int ( ks ) ) : b_key = self . _md5_digest ( '%s-%s-salt' % ( node , i ) ) for l in xrange ( 0 , 4 ) : key = ( ( b_key [ 3 + l * 4 ] << 24 ) | ( b_key [ 2 + l * 4 ] << 16 ) | ( b_key [ 1 + l * 4 ] << 8 ) | b_key [ l * 4 ] ) self . _hashring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )\"], ['Return long integer for a given key that represent it place on the hash ring .', 'def _gen_key ( self , key ) : b_key = self . _md5_digest ( key ) return self . _hashi ( b_key , lambda x : x )'], ['Returns True if there exists a custom image for app_id .', 'def has_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) return any ( map ( os . path . exists , possible_paths ) )'], ['Returns the custom image associated with a given app . If there are multiple candidate images on disk one is chosen arbitrarily .', 'def get_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) existing_images = filter ( os . path . exists , possible_paths ) if len ( existing_images ) > 0 : return existing_images [ 0 ]'], ['Sets the custom image for app_id to be the image located at image_path . If there already exists a custom image for app_id it will be deleted . Returns True is setting the image was successful .', 'def set_custom_image ( user_context , app_id , image_path ) : if image_path is None : return False if not os . path . exists ( image_path ) : return False ( root , ext ) = os . path . splitext ( image_path ) if not is_valid_extension ( ext ) : return False if has_custom_image ( user_context , app_id ) : img = get_custom_image ( user_context , app_id ) assert ( img is not None ) os . remove ( img ) parent_dir = paths . custom_images_directory ( user_context ) new_path = os . path . join ( parent_dir , app_id + ext ) shutil . copyfile ( image_path , new_path ) return True'], ['Read an orthography profile from a metadata file or a default tab - separated profile file .', \"def from_file ( cls , fname , form = None ) : try : tg = TableGroup . from_file ( fname ) opfname = None except JSONDecodeError : tg = TableGroup . fromvalue ( cls . MD ) opfname = fname if len ( tg . tables ) != 1 : raise ValueError ( 'profile description must contain exactly one table' ) metadata = tg . common_props metadata . update ( fname = Path ( fname ) , form = form ) return cls ( * [ { k : None if ( k != cls . GRAPHEME_COL and v == cls . NULL ) else v for k , v in d . items ( ) } for d in tg . tables [ 0 ] . iterdicts ( fname = opfname ) ] , ** metadata )\"], ['Create a Profile instance from the Unicode graphemes found in text .', \"def from_text ( cls , text , mapping = 'mapping' ) : graphemes = Counter ( grapheme_pattern . findall ( text ) ) specs = [ OrderedDict ( [ ( cls . GRAPHEME_COL , grapheme ) , ( 'frequency' , frequency ) , ( mapping , grapheme ) ] ) for grapheme , frequency in graphemes . most_common ( ) ] return cls ( * specs )\"], ['split fasta file into separate fasta files based on list of scaffolds that belong to each separate file', \"def split_fasta ( f , id2f ) : opened = { } for seq in parse_fasta ( f ) : id = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] if id not in id2f : continue fasta = id2f [ id ] if fasta not in opened : opened [ fasta ] = '%s.fa' % fasta seq [ 1 ] += '\\\\n' with open ( opened [ fasta ] , 'a+' ) as f_out : f_out . write ( '\\\\n' . join ( seq ) )\"], ['Check whether pathname is a valid user data directory', 'def _is_user_directory ( self , pathname ) : fullpath = os . path . join ( self . userdata_location ( ) , pathname ) return os . path . isdir ( fullpath ) and pathname . isdigit ( )'], ['Returns an array of user ids for users on the filesystem', 'def local_users ( self ) : userdirs = filter ( self . _is_user_directory , os . listdir ( self . userdata_location ( ) ) ) return map ( lambda userdir : user . User ( self , int ( userdir ) ) , userdirs )'], ['Calculates degree days starting with a series of temperature equivalent values', \"def _calculate_degree_days ( temperature_equivalent , base_temperature , cooling = False ) : if cooling : ret = temperature_equivalent - base_temperature else : ret = base_temperature - temperature_equivalent ret [ ret < 0 ] = 0 prefix = 'CDD' if cooling else 'HDD' ret . name = '{}_{}' . format ( prefix , base_temperature ) return ret\"], ['Development status .', \"def status ( self ) : return { self . _acronym_status ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_status ) }\"], ['OSI Approved license .', \"def licenses ( self ) : return { self . _acronym_lic ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Remove prefix .', \"def licenses_desc ( self ) : return { self . _acronym_lic ( l ) : l . split ( self . prefix_lic ) [ 1 ] for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Convert license acronym .', \"def _acronym_lic ( self , license_statement ) : pat = re . compile ( r'\\\\(([\\\\w+\\\\W?\\\\s?]+)\\\\)' ) if pat . search ( license_statement ) : lic = pat . search ( license_statement ) . group ( 1 ) if lic . startswith ( 'CNRI' ) : acronym_licence = lic [ : 4 ] else : acronym_licence = lic . replace ( ' ' , '' ) else : acronym_licence = '' . join ( [ w [ 0 ] for w in license_statement . split ( self . prefix_lic ) [ 1 ] . split ( ) ] ) return acronym_licence\"], ['calc MD5 based on path', \"def calcMD5 ( path ) : if os . path . exists ( path ) is False : yield False else : command = [ 'md5sum' , path ] p = Popen ( command , stdout = PIPE ) for line in p . communicate ( ) [ 0 ] . splitlines ( ) : yield line . decode ( 'ascii' ) . strip ( ) . split ( ) [ 0 ] p . wait ( ) yield False\"], ['download files with wget', \"def wget ( ftp , f = False , exclude = False , name = False , md5 = False , tries = 10 ) : if f is False : f = ftp . rsplit ( '/' , 1 ) [ - 1 ] t = 0 while md5check ( f , ftp , md5 , exclude ) is not True : t += 1 if name is not False : print ( '# downloading:' , name , f ) if exclude is False : command = 'wget -q --random-wait %s' % ( ftp ) else : command = 'wget -q --random-wait -R %s %s' % ( exclude , ftp ) p = Popen ( command , shell = True ) p . communicate ( ) if t >= tries : print ( 'not downloaded:' , name , f ) return [ f , False ] return [ f , True ]\"], ['check that at least one of queries is in list l', \"def check ( line , queries ) : line = line . strip ( ) spLine = line . replace ( '.' , ' ' ) . split ( ) matches = set ( spLine ) . intersection ( queries ) if len ( matches ) > 0 : return matches , line . split ( '\\\\t' ) return matches , False\"], ['search entrez using specified database and accession', \"def entrez ( db , acc ) : c1 = [ 'esearch' , '-db' , db , '-query' , acc ] c2 = [ 'efetch' , '-db' , 'BioSample' , '-format' , 'docsum' ] p1 = Popen ( c1 , stdout = PIPE , stderr = PIPE ) p2 = Popen ( c2 , stdin = p1 . stdout , stdout = PIPE , stderr = PIPE ) return p2 . communicate ( )\"], ['attempt to use NCBI Entrez to get BioSample ID', \"def searchAccession ( acc ) : out , error = entrez ( 'genome' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'nucleotide' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'assembly' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) for error in error . splitlines ( ) : error = error . decode ( 'ascii' ) . strip ( ) if '500 Can' in error : return ( False , acc , 'no network' ) return ( False , acc , 'efetch failed' )\"], ['download genome info from NCBI', \"def getFTPs ( accessions , ftp , search , exclude , convert = False , threads = 1 , attempt = 1 , max_attempts = 2 ) : info = wget ( ftp ) [ 0 ] allMatches = [ ] for genome in open ( info , encoding = 'utf8' ) : genome = str ( genome ) matches , genomeInfo = check ( genome , accessions ) if genomeInfo is not False : f = genomeInfo [ 0 ] + search Gftp = genomeInfo [ 19 ] Gftp = Gftp + '/' + search allMatches . extend ( matches ) yield ( Gftp , f , exclude , matches ) newAccs = [ ] missing = accessions . difference ( set ( allMatches ) ) if convert is True : pool = Pool ( threads ) pool = pool . imap_unordered ( searchAccession , missing ) for newAcc in tqdm ( pool , total = len ( missing ) ) : status , accession , newAcc = newAcc if status is True : newAccs . append ( newAcc ) print ( 'not found:' , accession , '->' , newAcc ) else : for accession in missing : print ( 'not found:' , accession ) if len ( newAccs ) > 0 and attempt <= max_attempts : print ( 'convert accession attempt' , attempt ) attempt += 1 for hit in getFTPs ( set ( newAccs ) , ftp , search , exclude , convert , threads = 1 , attempt = attempt ) : yield hit\"], ['download genomes from NCBI', \"def download ( args ) : accessions , infoFTP = set ( args [ 'g' ] ) , args [ 'i' ] search , exclude = args [ 's' ] , args [ 'e' ] FTPs = getFTPs ( accessions , infoFTP , search , exclude , threads = args [ 't' ] , convert = args [ 'convert' ] ) if args [ 'test' ] is True : for genome in FTPs : print ( 'found:' , ';' . join ( genome [ - 1 ] ) , genome [ 0 ] ) return FTPs pool = Pool ( args [ 't' ] ) pool = pool . imap_unordered ( wgetGenome , FTPs ) files = [ ] for f in tqdm ( pool , total = len ( accessions ) ) : files . append ( f ) return files\"], ['remove pesky characters from fasta file header', 'def fix_fasta ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 0 ] = remove_char ( seq [ 0 ] ) if len ( seq [ 1 ] ) > 0 : yield seq'], ['Compute a DataFrame summary of a Stats object .', \"def _calc_frames ( stats ) : timings = [ ] callers = [ ] for key , values in iteritems ( stats . stats ) : timings . append ( pd . Series ( key + values [ : - 1 ] , index = timing_colnames , ) ) for caller_key , caller_values in iteritems ( values [ - 1 ] ) : callers . append ( pd . Series ( key + caller_key + caller_values , index = caller_columns , ) ) timings_df = pd . DataFrame ( timings ) callers_df = pd . DataFrame ( callers ) timings_df [ 'filename:funcname' ] = ( timings_df [ 'filename' ] + ':' + timings_df [ 'funcname' ] ) timings_df = timings_df . groupby ( 'filename:funcname' ) . sum ( ) return timings_df , callers_df\"], ['get unmapped reads', \"def unmapped ( sam , mates ) : for read in sam : if read . startswith ( '@' ) is True : continue read = read . strip ( ) . split ( ) if read [ 2 ] == '*' and read [ 6 ] == '*' : yield read elif mates is True : if read [ 2 ] == '*' or read [ 6 ] == '*' : yield read for i in read : if i == 'YT:Z:UP' : yield read\"], ['execute jobs in processes using N threads', 'def parallel ( processes , threads ) : pool = multithread ( threads ) pool . map ( run_process , processes ) pool . close ( ) pool . join ( )'], ['the final log processor that structlog requires to render .', 'def define_log_renderer ( fmt , fpath , quiet ) : if fmt : return structlog . processors . JSONRenderer ( ) if fpath is not None : return structlog . processors . JSONRenderer ( ) if sys . stderr . isatty ( ) and not quiet : return structlog . dev . ConsoleRenderer ( ) return structlog . processors . JSONRenderer ( )'], ['Add unique id type and hostname', \"def _structlog_default_keys_processor ( logger_class , log_method , event ) : global HOSTNAME if 'id' not in event : event [ 'id' ] = '%s_%s' % ( datetime . utcnow ( ) . strftime ( '%Y%m%dT%H%M%S' ) , uuid . uuid1 ( ) . hex ) if 'type' not in event : event [ 'type' ] = 'log' event [ 'host' ] = HOSTNAME return event\"], ['log processors that structlog executes before final rendering', 'def define_log_processors ( ) : return [ structlog . processors . TimeStamper ( fmt = \"iso\" ) , _structlog_default_keys_processor , structlog . stdlib . PositionalArgumentsFormatter ( ) , structlog . processors . StackInfoRenderer ( ) , structlog . processors . format_exc_info , ]'], ['configures a logger when required write to stderr or a file', 'def _configure_logger ( fmt , quiet , level , fpath , pre_hooks , post_hooks , metric_grouping_interval ) : level = getattr ( logging , level . upper ( ) ) global _GLOBAL_LOG_CONFIGURED if _GLOBAL_LOG_CONFIGURED : return def wrap_hook ( fn ) : @ wraps ( fn ) def processor ( logger , method_name , event_dict ) : fn ( event_dict ) return event_dict return processor processors = define_log_processors ( ) processors . extend ( [ wrap_hook ( h ) for h in pre_hooks ] ) if metric_grouping_interval : processors . append ( metrics_grouping_processor ) log_renderer = define_log_renderer ( fmt , fpath , quiet ) stderr_required = ( not quiet ) pretty_to_stderr = ( stderr_required and ( fmt == \"pretty\" or ( fmt is None and sys . stderr . isatty ( ) ) ) ) should_inject_pretty_renderer = ( pretty_to_stderr and not isinstance ( log_renderer , structlog . dev . ConsoleRenderer ) ) if should_inject_pretty_renderer : stderr_required = False processors . append ( StderrConsoleRenderer ( ) ) processors . append ( log_renderer ) processors . extend ( [ wrap_hook ( h ) for h in post_hooks ] ) streams = [ ] if stderr_required : streams . append ( sys . stderr ) if fpath is not None : streams . append ( open ( fpath , \\'a\\' ) ) assert len ( streams ) != 0 , \"cannot configure logger for 0 streams\" stream = streams [ 0 ] if len ( streams ) == 1 else Stream ( * streams ) atexit . register ( stream . close ) structlog . configure ( processors = processors , context_class = dict , logger_factory = LevelLoggerFactory ( stream , level = level ) , wrapper_class = BoundLevelLogger , cache_logger_on_first_use = True , ) stdlib_root_log = logging . getLogger ( ) stdlib_root_log . addHandler ( StdlibStructlogHandler ( ) ) stdlib_root_log . setLevel ( level ) _GLOBAL_LOG_CONFIGURED = True'], ['Instead of using a processor adding basic information like caller filename etc here .', 'def _add_base_info ( self , event_dict ) : f = sys . _getframe ( ) level_method_frame = f . f_back caller_frame = level_method_frame . f_back return event_dict'], ['Propagate a method call to the wrapped logger .', \"def _proxy_to_logger ( self , method_name , event , * event_args , ** event_kw ) : if isinstance ( event , bytes ) : event = event . decode ( 'utf-8' ) if event_args : event_kw [ 'positional_args' ] = event_args return super ( BoundLevelLogger , self ) . _proxy_to_logger ( method_name , event = event , ** event_kw )\"], ['Given four points of a rectangle translate the rectangle to the specified x and y coordinates and optionally change the width .', 'def translate ( rect , x , y , width = 1 ) : return ( ( rect [ 0 ] [ 0 ] + x , rect [ 0 ] [ 1 ] + y ) , ( rect [ 1 ] [ 0 ] + x , rect [ 1 ] [ 1 ] + y ) , ( rect [ 2 ] [ 0 ] + x + width , rect [ 2 ] [ 1 ] + y ) , ( rect [ 3 ] [ 0 ] + x + width , rect [ 3 ] [ 1 ] + y ) )'], ['remove problem characters from string', \"def remove_bad ( string ) : remove = [ ':' , ',' , '(' , ')' , ' ' , '|' , ';' , '\\\\'' ] for c in remove : string = string . replace ( c , '_' ) return string\"], ['make copy of sequences with short identifier', \"def get_ids ( a ) : a_id = '%s.id.fa' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) a_id_lookup = '%s.id.lookup' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) if check ( a_id ) is True : return a_id , a_id_lookup a_id_f = open ( a_id , 'w' ) a_id_lookup_f = open ( a_id_lookup , 'w' ) ids = [ ] for seq in parse_fasta ( open ( a ) ) : id = id_generator ( ) while id in ids : id = id_generator ( ) ids . append ( id ) header = seq [ 0 ] . split ( '>' ) [ 1 ] name = remove_bad ( header ) seq [ 0 ] = '>%s %s' % ( id , header ) print ( '\\\\n' . join ( seq ) , file = a_id_f ) print ( '%s\\\\t%s\\\\t%s' % ( id , name , header ) , file = a_id_lookup_f ) return a_id , a_id_lookup\"], ['convert fasta to phylip because RAxML is ridiculous', 'def convert2phylip ( convert ) : out = \\'%s.phy\\' % ( convert . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if check ( out ) is False : convert = open ( convert , \\'rU\\' ) out_f = open ( out , \\'w\\' ) alignments = AlignIO . parse ( convert , \"fasta\" ) AlignIO . write ( alignments , out , \"phylip\" ) return out'], ['run IQ - Tree', 'def run_iqtree ( phy , model , threads , cluster , node ) : if threads > 24 : ppn = 24 else : ppn = threads tree = \\'%s.treefile\\' % ( phy ) if check ( tree ) is False : if model is False : model = \\'TEST\\' dir = os . getcwd ( ) command = \\'iqtree-omp -s %s -m %s -nt %s -quiet\\' % ( phy , model , threads ) if cluster is False : p = Popen ( command , shell = True ) else : if node is False : node = \\'1\\' qsub = \\'qsub -l nodes=%s:ppn=%s -m e -N iqtree\\' % ( node , ppn ) command = \\'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree\\' % ( dir , phy , command , dir ) re_call = \\'cd %s; %s --no-fast --iq\\' % ( dir . rsplit ( \\'/\\' , 1 ) [ 0 ] , \\' \\' . join ( sys . argv ) ) p = Popen ( \\'echo \"%s;%s\" | %s\\' % ( command , re_call , qsub ) , shell = True ) p . communicate ( ) return tree'], ['get the names for sequences in the raxml tree', \"def fix_tree ( tree , a_id_lookup , out ) : if check ( out ) is False and check ( tree ) is True : tree = open ( tree ) . read ( ) for line in open ( a_id_lookup ) : id , name , header = line . strip ( ) . split ( '\\\\t' ) tree = tree . replace ( id + ':' , name + ':' ) out_f = open ( out , 'w' ) print ( tree . strip ( ) , file = out_f ) return out\"], ['Creates a new Nydus cluster from the given settings .', \"def create_cluster ( settings ) : settings = copy . deepcopy ( settings ) backend = settings . pop ( 'engine' , settings . pop ( 'backend' , None ) ) if isinstance ( backend , basestring ) : Conn = import_string ( backend ) elif backend : Conn = backend else : raise KeyError ( 'backend' ) cluster = settings . pop ( 'cluster' , None ) if not cluster : Cluster = Conn . get_cluster ( ) elif isinstance ( cluster , basestring ) : Cluster = import_string ( cluster ) else : Cluster = cluster router = settings . pop ( 'router' , None ) if not router : Router = BaseRouter elif isinstance ( router , basestring ) : Router = import_string ( router ) else : Router = router return Cluster ( router = Router , backend = Conn , ** settings )\"], ['Gets the translation of a specific field for a specific language code .', \"def _get_translation ( self , field , code ) : if not code in self . _translation_cache : translations = self . translations . select_related ( ) logger . debug ( u'Matched with field %s for language %s. Attempting lookup.' , field , code ) try : translation_obj = translations . get ( language_code = code ) except ObjectDoesNotExist : translation_obj = None self . _translation_cache [ code ] = translation_obj logger . debug ( u'Translation not found in cache.' ) else : logger . debug ( u'Translation found in cache.' ) translation_obj = self . _translation_cache . get ( code ) if not translation_obj : raise ObjectDoesNotExist field_value = getattr ( translation_obj , field ) logger . debug ( u'Found translation object %s, returning value %s.' , translation_obj , field_value ) return field_value\"], ['Wrapper to allow for easy unicode representation of an object by the specified property . If this wrapper is not able to find the right translation of the specified property it will return the default value instead .', \"def unicode_wrapper ( self , property , default = ugettext ( 'Untitled' ) ) : try : value = getattr ( self , property ) except ValueError : logger . warn ( u'ValueError rendering unicode for %s object.' , self . _meta . object_name ) value = None if not value : value = default return value\"], ['remove insertion columns from aligned fasta file', \"def strip_inserts ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 1 ] = '' . join ( [ b for b in seq [ 1 ] if b == '-' or b . isupper ( ) ] ) yield seq\"], ['Transform a string s graphemes into the mappings given in a different column in the orthography profile .', 'def transform ( self , word , column = Profile . GRAPHEME_COL , error = errors . replace ) : assert self . op , \\'method can only be called with orthography profile.\\' if column != Profile . GRAPHEME_COL and column not in self . op . column_labels : raise ValueError ( \"Column {0} not found in profile.\" . format ( column ) ) word = self . op . tree . parse ( word , error ) if column == Profile . GRAPHEME_COL : return word out = [ ] for token in word : try : target = self . op . graphemes [ token ] [ column ] except KeyError : target = self . _errors [ \\'replace\\' ] ( token ) if target is not None : if isinstance ( target , ( tuple , list ) ) : out . extend ( target ) else : out . append ( target ) return out'], ['Function to tokenize input string and return output of str with ortho rules applied .', 'def rules ( self , word ) : return self . _rules . apply ( word ) if self . _rules else word'], ['Given a string that is space - delimited on Unicode grapheme clusters group Unicode modifier letters with their preceding base characters deal with tie bars etc .', 'def combine_modifiers ( self , graphemes ) : result = [ ] temp = \"\" count = len ( graphemes ) for grapheme in reversed ( graphemes ) : count -= 1 if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Lm\" and not ord ( grapheme ) in [ 712 , 716 ] : temp = grapheme + temp if count == 0 : result [ - 1 ] = temp + result [ - 1 ] continue if len ( grapheme ) == 1 and ord ( grapheme ) in [ 712 , 716 ] : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Sk\" : if len ( result ) == 0 : result . append ( grapheme ) temp = \"\" continue else : if unicodedata . category ( result [ - 1 ] [ 0 ] ) == \"Sk\" : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue result . append ( grapheme + temp ) temp = \"\" segments = result [ : : - 1 ] i = 0 r = [ ] while i < len ( segments ) : if ord ( segments [ i ] [ - 1 ] ) in [ 865 , 860 ] : r . append ( segments [ i ] + segments [ i + 1 ] ) i += 2 else : r . append ( segments [ i ] ) i += 1 return r'], ['parse catalytic RNAs to gff format', \"def parse_catalytic ( insertion , gff ) : offset = insertion [ 'offset' ] GeneStrand = insertion [ 'strand' ] if type ( insertion [ 'intron' ] ) is not str : return gff for intron in parse_fasta ( insertion [ 'intron' ] . split ( '|' ) ) : ID , annot , strand , pos = intron [ 0 ] . split ( '>' ) [ 1 ] . split ( ) Start , End = [ int ( i ) for i in pos . split ( '-' ) ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Rfam' ) gff [ 'feature' ] . append ( 'Catalytic RNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse ORF to gff format', \"def parse_orf ( insertion , gff ) : offset = insertion [ 'offset' ] if type ( insertion [ 'orf' ] ) is not str : return gff for orf in parse_fasta ( insertion [ 'orf' ] . split ( '|' ) ) : ID = orf [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End , strand = [ int ( i ) for i in orf [ 0 ] . split ( ' # ' ) [ 1 : 4 ] ] if strand == 1 : strand = '+' else : strand = '-' GeneStrand = insertion [ 'strand' ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 annot = orf [ 0 ] . split ( ) [ 1 ] if annot == 'n/a' : annot = 'unknown' gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Prodigal and Pfam' ) gff [ 'feature' ] . append ( 'CDS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse insertion to gff format', \"def parse_insertion ( insertion , gff ) : offset = insertion [ 'offset' ] for ins in parse_fasta ( insertion [ 'insertion sequence' ] . split ( '|' ) ) : strand = insertion [ 'strand' ] ID = ins [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End = [ int ( i ) for i in ins [ 0 ] . split ( 'gene-pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] Start , End = abs ( Start + offset ) , abs ( End + offset ) if strand == '-' : Start , End = End , Start gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( insertion [ 'source' ] ) gff [ 'feature' ] . append ( 'IVS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s' % ( ID ) ) return gff\"], ['parse rRNA to gff format', \"def parse_rRNA ( insertion , seq , gff ) : offset = insertion [ 'offset' ] strand = insertion [ 'strand' ] for rRNA in parse_masked ( seq , 0 ) [ 0 ] : rRNA = '' . join ( rRNA ) Start = seq [ 1 ] . find ( rRNA ) + 1 End = Start + len ( rRNA ) - 1 if strand == '-' : Start , End = End - 2 , Start - 2 pos = ( abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 ) Start , End = min ( pos ) , max ( pos ) source = insertion [ 'source' ] annot = '%s rRNA' % ( source . split ( 'from' , 1 ) [ 0 ] ) gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'rRNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'Name=%s' % ( annot ) ) return gff\"], ['convert iTable to gff file', \"def iTable2GFF ( iTable , fa , contig = False ) : columns = [ '#seqname' , 'source' , 'feature' , 'start' , 'end' , 'score' , 'strand' , 'frame' , 'attribute' ] gff = { c : [ ] for c in columns } for insertion in iTable . iterrows ( ) : insertion = insertion [ 1 ] if insertion [ 'ID' ] not in fa : continue strand = insertion [ 'sequence' ] . split ( 'strand=' , 1 ) [ 1 ] . split ( ) [ 0 ] if contig is True : gene = [ int ( i ) for i in insertion [ 'sequence' ] . split ( 'pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] if strand == '-' : offset = - 1 * ( gene [ 1 ] ) else : offset = gene [ 0 ] else : strand = '+' gene = [ 1 , int ( insertion [ 'sequence' ] . split ( 'total-len=' , 1 ) [ 1 ] . split ( ) [ 0 ] ) ] offset = gene [ 0 ] insertion [ 'strand' ] = strand insertion [ 'offset' ] = offset source = insertion [ 'sequence' ] . split ( '::model' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ - 1 ] insertion [ 'source' ] = source geneAnnot = '%s rRNA gene' % ( source . split ( 'from' , 1 ) [ 0 ] ) geneNum = insertion [ 'sequence' ] . split ( 'seq=' , 1 ) [ 1 ] . split ( ) [ 0 ] gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'Gene' ) gff [ 'start' ] . append ( gene [ 0 ] ) gff [ 'end' ] . append ( gene [ 1 ] ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( geneNum , geneAnnot ) ) gff = parse_rRNA ( insertion , fa [ insertion [ 'ID' ] ] , gff ) gff = parse_insertion ( insertion , gff ) gff = parse_orf ( insertion , gff ) gff = parse_catalytic ( insertion , gff ) return pd . DataFrame ( gff ) [ columns ] . drop_duplicates ( )\"], ['Given an abundance table group the counts by every taxonomic level .', \"def summarize_taxa ( biom ) : tamtcounts = defaultdict ( int ) tot_seqs = 0.0 for row , col , amt in biom [ 'data' ] : tot_seqs += amt rtax = biom [ 'rows' ] [ row ] [ 'metadata' ] [ 'taxonomy' ] for i , t in enumerate ( rtax ) : t = t . strip ( ) if i == len ( rtax ) - 1 and len ( t ) > 3 and len ( rtax [ - 1 ] ) > 3 : t = 's__' + rtax [ i - 1 ] . strip ( ) . split ( '_' ) [ - 1 ] + '_' + t . split ( '_' ) [ - 1 ] tamtcounts [ t ] += amt lvlData = { lvl : levelData ( tamtcounts , tot_seqs , lvl ) for lvl in [ 'k' , 'p' , 'c' , 'o' , 'f' , 'g' , 's' ] } return tot_seqs , lvlData\"], ['Returns the path to the custom image set for this game or None if no image is set', 'def custom_image ( self , user ) : for ext in self . valid_custom_image_extensions ( ) : image_location = self . _custom_image_path ( user , ext ) if os . path . isfile ( image_location ) : return image_location return None'], ['Sets a custom image for the game . image_path should refer to an image file on disk', 'def set_image ( self , user , image_path ) : _ , ext = os . path . splitext ( image_path ) shutil . copy ( image_path , self . _custom_image_path ( user , ext ) )'], ['get a list of mapped reads', \"def sam_list ( sam ) : list = [ ] for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : list . append ( id ) return set ( list )\"], ['get a list of mapped reads require that both pairs are mapped in the sam file in order to remove the reads', \"def sam_list_paired ( sam ) : list = [ ] pair = [ '1' , '2' ] prev = '' for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : read = id . rsplit ( '/' ) [ 0 ] if read == prev : list . append ( read ) prev = read return set ( list )\"], ['require that both pairs are mapped in the sam file in order to remove the reads', \"def filter_paired ( list ) : pairs = { } filtered = [ ] for id in list : read = id . rsplit ( '/' ) [ 0 ] if read not in pairs : pairs [ read ] = [ ] pairs [ read ] . append ( id ) for read in pairs : ids = pairs [ read ] if len ( ids ) == 2 : filtered . extend ( ids ) return set ( filtered )\"], ['print fastq from sam', \"def sam2fastq ( line ) : fastq = [ ] fastq . append ( '@%s' % line [ 0 ] ) fastq . append ( line [ 9 ] ) fastq . append ( '+%s' % line [ 0 ] ) fastq . append ( line [ 10 ] ) return fastq\"], ['- check to see if the read maps with < = threshold number of mismatches - mm_option = one or both depending on whether or not one or both reads in a pair need to pass the mismatch threshold - pair can be False if read does not have a pair - make sure alignment score is not 0 which would indicate that the read was not aligned to the reference', \"def check_mismatches ( read , pair , mismatches , mm_option , req_map ) : if pair is False : mm = count_mismatches ( read ) if mm is False : return False if mismatches is False : return True if mm <= mismatches : return True r_mm = count_mismatches ( read ) p_mm = count_mismatches ( pair ) if r_mm is False and p_mm is False : return False if mismatches is False : return True if req_map is True : if r_mm is False or p_mm is False : return False if mm_option == 'one' : if ( r_mm is not False and r_mm <= mismatches ) or ( p_mm is not False and p_mm <= mismatches ) : return True if mm_option == 'both' : if r_mm is False : if p_mm <= mismatches : return True elif p_mm is False : if r_mm <= mismatches : return True elif ( r_mm is not False and r_mm <= mismatches ) and ( p_mm is not False and p_mm <= mismatches ) : return True return False\"], ['determine whether or not reads map to specific region of scaffold', 'def check_region ( read , pair , region ) : if region is False : return True for mapping in read , pair : if mapping is False : continue start , length = int ( mapping [ 3 ] ) , len ( mapping [ 9 ] ) r = [ start , start + length - 1 ] if get_overlap ( r , region ) > 0 : return True return False'], ['Returns a Steam object representing the current Steam installation on the users computer . If the user doesn t have Steam installed returns None .', \"def get_steam ( ) : helper = lambda udd : Steam ( udd ) if os . path . exists ( udd ) else None plat = platform . system ( ) if plat == 'Darwin' : return helper ( paths . default_osx_userdata_path ( ) ) if plat == 'Linux' : return helper ( paths . default_linux_userdata_path ( ) ) if plat == 'Windows' : possible_dir = winutils . find_userdata_directory ( ) return helper ( possible_dir ) if possible_dir is not None else None return None\"], ['normalize from zero to one for row or table', \"def zero_to_one ( table , option ) : if option == 'table' : m = min ( min ( table ) ) ma = max ( max ( table ) ) t = [ ] for row in table : t_row = [ ] if option != 'table' : m , ma = min ( row ) , max ( row ) for i in row : if ma == m : t_row . append ( 0 ) else : t_row . append ( ( i - m ) / ( ma - m ) ) t . append ( t_row ) return t\"], ['calculate percent of total', \"def pertotal ( table , option ) : if option == 'table' : total = sum ( [ i for line in table for i in line ] ) t = [ ] for row in table : t_row = [ ] if option != 'table' : total = sum ( row ) for i in row : if total == 0 : t_row . append ( 0 ) else : t_row . append ( i / total * 100 ) t . append ( t_row ) return t\"], ['scale table based on the column with the largest sum', 'def scale ( table ) : t = [ ] columns = [ [ ] for i in table [ 0 ] ] for row in table : for i , v in enumerate ( row ) : columns [ i ] . append ( v ) sums = [ float ( sum ( i ) ) for i in columns ] scale_to = float ( max ( sums ) ) scale_factor = [ scale_to / i for i in sums if i != 0 ] for row in table : t . append ( [ a * b for a , b in zip ( row , scale_factor ) ] ) return t'], ['fit to normal distribution', \"def norm ( table ) : print ( '# norm dist is broken' , file = sys . stderr ) exit ( ) from matplotlib . pyplot import hist as hist t = [ ] for i in table : t . append ( np . ndarray . tolist ( hist ( i , bins = len ( i ) , normed = True ) [ 0 ] ) ) return t\"], ['log transform each value in table', 'def log_trans ( table ) : t = [ ] all = [ item for sublist in table for item in sublist ] if min ( all ) == 0 : scale = min ( [ i for i in all if i != 0 ] ) * 10e-10 else : scale = 0 for i in table : t . append ( np . ndarray . tolist ( np . log10 ( [ j + scale for j in i ] ) ) ) return t'], ['box - cox transform table', 'def box_cox ( table ) : from scipy . stats import boxcox as bc t = [ ] for i in table : if min ( i ) == 0 : scale = min ( [ j for j in i if j != 0 ] ) * 10e-10 else : scale = 0 t . append ( np . ndarray . tolist ( bc ( np . array ( [ j + scale for j in i ] ) ) [ 0 ] ) ) return t'], ['inverse hyperbolic sine transformation', 'def inh ( table ) : t = [ ] for i in table : t . append ( np . ndarray . tolist ( np . arcsinh ( i ) ) ) return t'], ['from SparCC - randomly draw from the corresponding posterior Dirichlet distribution with a uniform prior', 'def diri ( table ) : t = [ ] for i in table : a = [ j + 1 for j in i ] t . append ( np . ndarray . tolist ( np . random . mtrand . dirichlet ( a ) ) ) return t'], ['Given a list of sample IDs generate unique n - base barcodes for each . Note that only 4^n unique barcodes are possible .', \"def generate_barcodes ( nIds , codeLen = 12 ) : def next_code ( b , c , i ) : return c [ : i ] + b + ( c [ i + 1 : ] if i < - 1 else '' ) def rand_base ( ) : return random . choice ( [ 'A' , 'T' , 'C' , 'G' ] ) def rand_seq ( n ) : return '' . join ( [ rand_base ( ) for _ in range ( n ) ] ) hpf = re . compile ( 'aaaa|cccc|gggg|tttt' , re . IGNORECASE ) while True : codes = [ rand_seq ( codeLen ) ] if ( hpf . search ( codes [ 0 ] ) is None ) : break idx = 0 while len ( codes ) < nIds : idx -= 1 if idx < - codeLen : idx = - 1 codes . append ( rand_seq ( codeLen ) ) else : nc = next_code ( rand_base ( ) , codes [ - 1 ] , idx ) if hpf . search ( nc ) is None : codes . append ( nc ) codes = list ( set ( codes ) ) return codes\"], ['Given a sample ID and a mapping modify a Sanger FASTA file to include the barcode and primer in the sequence data and change the description line as needed .', \"def scrobble_data_dir ( dataDir , sampleMap , outF , qualF = None , idopt = None , utf16 = False ) : seqcount = 0 outfiles = [ osp . split ( outF . name ) [ 1 ] ] if qualF : outfiles . append ( osp . split ( qualF . name ) [ 1 ] ) for item in os . listdir ( dataDir ) : if item in outfiles or not osp . isfile ( os . path . join ( dataDir , item ) ) : continue if osp . splitext ( item ) [ 1 ] in file_types [ 'fasta' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'fasta' ) for record in records : if isinstance ( idopt , tuple ) : sep , field = idopt sampleID = record . id . split ( sep ) [ field - 1 ] else : sampleID = osp . splitext ( item ) [ 0 ] record . seq = ( sampleMap [ sampleID ] . barcode + sampleMap [ sampleID ] . primer + record . seq ) SeqIO . write ( record , outF , 'fasta' ) seqcount += 1 fh . close ( ) elif qualF and osp . splitext ( item ) [ 1 ] in file_types [ 'qual' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'qual' ) for record in records : mi = sampleMap [ sampleMap . keys ( ) [ 0 ] ] quals = [ 40 for _ in range ( len ( mi . barcode ) + len ( mi . primer ) ) ] record . letter_annotations [ 'phred_quality' ] [ 0 : 0 ] = quals SeqIO . write ( record , qualF , 'qual' ) fh . close ( ) return seqcount\"], ['Uses the built - in argparse module to handle command - line options for the program .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Convert Sanger-sequencing \\\\                                     derived data files for use with the \\\\                                     metagenomics analysis program QIIME, by \\\\                                     extracting Sample ID information, adding\\\\                                     barcodes and primers to the sequence \\\\                                     data, and outputting a mapping file and\\\\                                     single FASTA-formatted sequence file \\\\                                     formed by concatenating all input data.\" ) parser . add_argument ( \\'-i\\' , \\'--input_dir\\' , required = True , help = \"The directory containing sequence data files. \\\\                              Assumes all data files are placed in this \\\\                              directory. For files organized within folders by\\\\                              sample, use -s in addition.\" ) parser . add_argument ( \\'-m\\' , \\'--map_file\\' , default = \\'map.txt\\' , help = \"QIIME-formatted mapping file linking Sample IDs \\\\                              with barcodes and primers.\" ) parser . add_argument ( \\'-o\\' , \\'--output\\' , default = \\'output.fasta\\' , metavar = \\'OUTPUT_FILE\\' , help = \"Single file containing all sequence data found \\\\                              in input_dir, FASTA-formatted with barcode and \\\\                              primer preprended to sequence. If the -q option \\\\                              is passed, any quality data will also be output \\\\                              to a single file of the same name with a .qual \\\\                              extension.\" ) parser . add_argument ( \\'-b\\' , \\'--barcode_length\\' , type = int , default = 12 , help = \"Length of the generated barcode sequences. \\\\                              Default is 12 (QIIME default), minimum is 8.\" ) parser . add_argument ( \\'-q\\' , \\'--qual\\' , action = \\'store_true\\' , default = False , help = \"Instruct the program to look for quality \\\\                              input files\" ) parser . add_argument ( \\'-u\\' , \\'--utf16\\' , action = \\'store_true\\' , default = False , help = \"UTF-16 encoded input files\" ) parser . add_argument ( \\'-t\\' , \\'--treatment\\' , help = \"Inserts an additional column into the mapping \\\\                              file specifying some treatment or other variable\\\\                              that separates the current set of sequences \\\\                              from any other set of seqeunces. For example:\\\\                              -t DiseaseState=healthy\" ) sidGroup = parser . add_mutually_exclusive_group ( required = True ) sidGroup . add_argument ( \\'-d\\' , \\'--identifier_pattern\\' , action = ValidateIDPattern , nargs = 2 , metavar = ( \\'SEPARATOR\\' , \\'FIELD_NUMBER\\' ) , help = \"Indicates how to extract the Sample ID from \\\\                               the description line. Specify two things: \\\\                               1. Field separator, 2. Field number of Sample \\\\                               ID (1 or greater). If the separator is a space \\\\                               or tab, use \\\\s or \\\\\\\\t respectively. \\\\                               Example: >ka-SampleID-2091, use -i - 2, \\\\                               indicating - is the separator and the Sample ID\\\\                               is field #2.\" ) sidGroup . add_argument ( \\'-f\\' , \\'--filename_sample_id\\' , action = \\'store_true\\' , default = False , help = \\'Specify that the program should\\\\                          the name of each fasta file as the Sample ID for use\\\\                          in the mapping file. This is meant to be used when \\\\                          all sequence data for a sample is stored in a single\\\\                          file.\\' ) return parser . parse_args ( )'], ['Applies the arcsine square root transform to the given BIOM - format table', 'def arcsin_sqrt ( biom_tbl ) : arcsint = lambda data , id_ , md : np . arcsin ( np . sqrt ( data ) ) tbl_relabd = relative_abd ( biom_tbl ) tbl_asin = tbl_relabd . transform ( arcsint , inplace = False ) return tbl_asin'], ['parse sam file and check mapping quality', \"def parse_sam ( sam , qual ) : for line in sam : if line . startswith ( '@' ) : continue line = line . strip ( ) . split ( ) if int ( line [ 4 ] ) == 0 or int ( line [ 4 ] ) < qual : continue yield line\"], ['reverse completement stats', \"def rc_stats ( stats ) : rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } rcs = [ ] for pos in reversed ( stats ) : rc = { } rc [ 'reference frequencey' ] = pos [ 'reference frequency' ] rc [ 'consensus frequencey' ] = pos [ 'consensus frequency' ] rc [ 'In' ] = pos [ 'In' ] rc [ 'Del' ] = pos [ 'Del' ] rc [ 'ref' ] = rc_nucs [ pos [ 'ref' ] ] rc [ 'consensus' ] = ( rc_nucs [ pos [ 'consensus' ] [ 0 ] ] , pos [ 'consensus' ] [ 1 ] ) for base , stat in list ( pos . items ( ) ) : if base in rc_nucs : rc [ rc_nucs [ base ] ] = stat rcs . append ( rc ) return rcs\"], ['parse codon nucleotide positions in range start - > end wrt strand', 'def parse_codons ( ref , start , end , strand ) : codon = [ ] c = cycle ( [ 1 , 2 , 3 ] ) ref = ref [ start - 1 : end ] if strand == - 1 : ref = rc_stats ( ref ) for pos in ref : n = next ( c ) codon . append ( pos ) if n == 3 : yield codon codon = [ ]'], ['calculate coverage for positions in range start - > end', 'def calc_coverage ( ref , start , end , length , nucs ) : ref = ref [ start - 1 : end ] bases = 0 for pos in ref : for base , count in list ( pos . items ( ) ) : if base in nucs : bases += count return float ( bases ) / float ( length )'], ['parse gbk file', \"def parse_gbk ( gbks ) : for gbk in gbks : for record in SeqIO . parse ( open ( gbk ) , 'genbank' ) : for feature in record . features : if feature . type == 'gene' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : continue if feature . type == 'CDS' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : pass start = int ( feature . location . start ) + int ( feature . qualifiers [ 'codon_start' ] [ 0 ] ) end , strand = int ( feature . location . end ) , feature . location . strand if strand is None : strand = 1 else : strand = - 1 contig = record . id yield contig , [ locus , [ start , end , strand ] , feature . qualifiers ]\"], ['parse gene call information from Prodigal fasta output', \"def parse_fasta_annotations ( fastas , annot_tables , trans_table ) : if annot_tables is not False : annots = { } for table in annot_tables : for cds in open ( table ) : ID , start , end , strand = cds . strip ( ) . split ( ) annots [ ID ] = [ start , end , int ( strand ) ] for fasta in fastas : for seq in parse_fasta ( fasta ) : if ( '# ;gc_cont' not in seq [ 0 ] and '# ID=' not in seq [ 0 ] ) and annot_tables is False : print ( '# specify fasta from Prodigal or annotations table (-t)' , file = sys . stderr ) exit ( ) if 'ID=' in seq [ 0 ] : ID = seq [ 0 ] . rsplit ( 'ID=' , 1 ) [ 1 ] . split ( ';' , 1 ) [ 0 ] contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_%s' % ( ID ) , 1 ) [ 0 ] else : contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_' , 1 ) [ 0 ] locus = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] if ( '# ;gc_cont' in seq [ 0 ] or '# ID=' in seq [ 0 ] ) : info = seq [ 0 ] . split ( ' # ' ) start , end , strand = int ( info [ 1 ] ) , int ( info [ 2 ] ) , info [ 3 ] if strand == '1' : strand = 1 else : strand = - 1 product = [ '' . join ( info [ 4 ] . split ( ) [ 1 : ] ) ] else : start , end , strand = annots [ locus ] product = seq [ 0 ] . split ( ' ' , 1 ) [ 1 ] info = { 'transl_table' : [ trans_table ] , 'translation' : [ seq [ 1 ] ] , 'product' : product } yield contig , [ locus , [ start , end , strand ] , info ]\"], ['parse annotations in either gbk or Prodigal fasta format', 'def parse_annotations ( annots , fmt , annot_tables , trans_table ) : annotations = { } if fmt is False : for contig , feature in parse_gbk ( annots ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) else : for contig , feature in parse_fasta_annotations ( annots , annot_tables , trans_table ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) return annotations'], ['convert codon to amino acid', \"def codon2aa ( codon , trans_table ) : return Seq ( '' . join ( codon ) , IUPAC . ambiguous_dna ) . translate ( table = trans_table ) [ 0 ]\"], ['find consensus base based on nucleotide frequencies', \"def find_consensus ( bases ) : nucs = [ 'A' , 'T' , 'G' , 'C' , 'N' ] total = sum ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) try : top = max ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) except : bases [ 'consensus' ] = ( 'N' , 'n/a' ) bases [ 'consensus frequency' ] = 'n/a' bases [ 'reference frequency' ] = 'n/a' return bases top = [ ( nuc , bases [ nuc ] ) for nuc in bases if bases [ nuc ] == top ] if top [ 0 ] [ 1 ] == 0 : bases [ 'consensus' ] = ( 'n/a' , 0 ) else : bases [ 'consensus' ] = random . choice ( top ) if total == 0 : c_freq = 'n/a' ref_freq = 'n/a' else : c_freq = float ( bases [ 'consensus' ] [ 1 ] ) / float ( total ) if bases [ 'ref' ] not in bases : ref_freq = 0 else : ref_freq = float ( bases [ bases [ 'ref' ] ] ) / float ( total ) bases [ 'consensus frequency' ] = c_freq bases [ 'reference frequency' ] = ref_freq return bases\"], ['print consensensus sequences for each genome and sample', \"def print_consensus ( genomes ) : cons = { } for genome , contigs in list ( genomes . items ( ) ) : cons [ genome ] = { } for contig , samples in list ( contigs . items ( ) ) : for sample , stats in list ( samples . items ( ) ) : if sample not in cons [ genome ] : cons [ genome ] [ sample ] = { } seq = cons [ genome ] [ sample ] [ contig ] = [ ] for pos , ps in enumerate ( stats [ 'bp_stats' ] , 1 ) : ref , consensus = ps [ 'ref' ] , ps [ 'consensus' ] [ 0 ] if consensus == 'n/a' : consensus = ref . lower ( ) seq . append ( consensus ) for genome , samples in cons . items ( ) : for sample , contigs in samples . items ( ) : fn = '%s.%s.consensus.fa' % ( genome , sample ) f = open ( fn , 'w' ) for contig , seq in contigs . items ( ) : print ( '>%s' % ( contig ) , file = f ) print ( '' . join ( seq ) , file = f ) f . close ( ) return cons\"], ['calculate genome coverage from scaffold coverage table', \"def parse_cov ( cov_table , scaffold2genome ) : size = { } mapped = { } for line in open ( cov_table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : samples = line [ 1 : ] samples = [ i . rsplit ( '/' , 1 ) [ - 1 ] . split ( '.' , 1 ) [ 0 ] for i in samples ] continue scaffold , length = line [ 0 ] . split ( ': ' ) length = float ( length ) covs = [ float ( i ) for i in line [ 1 : ] ] bases = [ c * length for c in covs ] if scaffold not in scaffold2genome : continue genome = scaffold2genome [ scaffold ] if genome not in size : size [ genome ] = 0 mapped [ genome ] = { sample : 0 for sample in samples } size [ genome ] += length for sample , count in zip ( samples , bases ) : mapped [ genome ] [ sample ] += count coverage = { 'genome' : [ ] , 'genome size (bp)' : [ ] , 'sample' : [ ] , 'coverage' : [ ] } for genome , length in size . items ( ) : for sample in samples : cov = mapped [ genome ] [ sample ] / length coverage [ 'genome' ] . append ( genome ) coverage [ 'genome size (bp)' ] . append ( length ) coverage [ 'sample' ] . append ( sample ) coverage [ 'coverage' ] . append ( cov ) return pd . DataFrame ( coverage )\"], ['calculate genome coverage from scaffold coverage', 'def genome_coverage ( covs , s2b ) : COV = [ ] for cov in covs : COV . append ( parse_cov ( cov , s2b ) ) return pd . concat ( COV )'], ['convert s2b files to dictionary', \"def parse_s2bs ( s2bs ) : s2b = { } for s in s2bs : for line in open ( s ) : line = line . strip ( ) . split ( '\\\\t' ) s , b = line [ 0 ] , line [ 1 ] s2b [ s ] = b return s2b\"], ['convert fastas to s2b dictionary', \"def fa2s2b ( fastas ) : s2b = { } for fa in fastas : for seq in parse_fasta ( fa ) : s = seq [ 0 ] . split ( '>' , 1 ) [ 1 ] . split ( ) [ 0 ] s2b [ s ] = fa . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 1 ) [ 0 ] return s2b\"], ['Filters out sequences with too much ambiguity as defined by the method parameters .', \"def filter_ambiguity ( records , percent = 0.5 ) : seqs = [ ] count = 0 for record in records : if record . seq . count ( 'N' ) / float ( len ( record ) ) < percent : seqs . append ( record ) count += 1 return seqs , count\"], ['Search package .', 'def package_existent ( name ) : try : response = requests . get ( PYPI_URL . format ( name ) ) if response . ok : msg = ( \\'[error] \"{0}\" is registered already in PyPI.\\\\n\\' \\'\\\\tSpecify another package name.\\' ) . format ( name ) raise Conflict ( msg ) except ( socket . gaierror , Timeout , ConnectionError , HTTPError ) as exc : raise BackendFailure ( exc )'], ['add index to id to make it unique wrt ids', \"def append_index_id ( id , ids ) : index = 1 mod = '%s_%s' % ( id , index ) while mod in ids : index += 1 mod = '%s_%s' % ( id , index ) ids . append ( mod ) return mod , ids\"], ['de - replicate fastas based on sequence names', \"def de_rep ( fastas , append_index , return_original = False ) : ids = [ ] for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) id = header [ 0 ] if id not in ids : ids . append ( id ) if return_original is True : yield [ header , seq ] else : yield seq elif append_index == True : new , ids = append_index_id ( id , ids ) if return_original is True : yield [ header , [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ] ] else : yield [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ]\"], ['Request data associated with postcode .', \"def get ( postcode ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) url = '%s/postcode/%s.json' % ( END_POINT , postcode ) return _get_json_resp ( url )\"], ['Request all postcode data within distance miles of postcode .', \"def get_from_postcode ( postcode , distance ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) return _get_from ( distance , 'postcode=%s' % postcode )\"], ['Checks if latitude and longitude correct', 'def _check_point ( self , lat , lng ) : if abs ( lat ) > 90 or abs ( lng ) > 180 : msg = \"Illegal lat and/or lng, (%s, %s) provided.\" % ( lat , lng ) raise IllegalPointException ( msg )'], ['Checks for cached responses before requesting from web - service', 'def _lookup ( self , skip_cache , fun , * args , ** kwargs ) : if args not in self . cache or skip_cache : self . cache [ args ] = fun ( * args , ** kwargs ) return self . cache [ args ]'], ['Calls postcodes . get_nearest but checks correctness of lat and long and by default utilises a local cache .', 'def get_nearest ( self , lat , lng , skip_cache = False ) : lat , lng = float ( lat ) , float ( lng ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_nearest , lat , lng )'], ['Calls postcodes . get_from_postcode but checks correctness of distance and by default utilises a local cache .', 'def get_from_postcode ( self , postcode , distance , skip_cache = False ) : distance = float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) postcode = postcode . lower ( ) . replace ( \\' \\' , \\'\\' ) return self . _lookup ( skip_cache , get_from_postcode , postcode , float ( distance ) )'], ['Calls postcodes . get_from_geo but checks the correctness of all arguments and by default utilises a local cache .', 'def get_from_geo ( self , lat , lng , distance , skip_cache = False ) : lat , lng , distance = float ( lat ) , float ( lng ) , float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_from_geo , lat , lng , distance )'], ['get coordinates of insertions from insertion - masked sequence', 'def insertions_from_masked ( seq ) : insertions = [ ] prev = True for i , base in enumerate ( seq ) : if base . isupper ( ) and prev is True : insertions . append ( [ ] ) prev = False elif base . islower ( ) : insertions [ - 1 ] . append ( i ) prev = True return [ [ min ( i ) , max ( i ) ] for i in insertions if i != [ ] ]'], ['get insertion information from header', \"def seq_info ( names , id2names , insertions , sequences ) : seqs = { } for name in names : id = id2names [ name ] gene = name . split ( 'fromHMM::' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ 1 ] model = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( '=' , 1 ) [ 1 ] . split ( ) [ 0 ] i_gene_pos = insertions [ id ] i_model_pos = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( 'model-pos(ins-len)=' ) [ 1 ] . split ( ) [ 0 ] . split ( ';' ) i_info = [ ] for i , ins in enumerate ( i_gene_pos ) : model_pos = i_model_pos [ i ] . split ( '-' ) [ 1 ] . split ( '(' ) [ 0 ] length = i_model_pos [ i ] . split ( '(' ) [ 1 ] . split ( ')' ) [ 0 ] iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s' % ( id , ( i + 1 ) , ( i + 1 ) , ins [ 0 ] , ins [ 1 ] , model_pos ) iseq = sequences [ id ] [ 1 ] [ ins [ 0 ] : ( ins [ 1 ] + 1 ) ] iseq = [ iheader , iseq ] info = [ ins , model_pos , length , iseq , [ ] , [ ] ] i_info . append ( info ) seqs [ id ] = [ gene , model , i_info ] return seqs\"], ['make sure thresh % feature is contained within insertion', 'def check_overlap ( pos , ins , thresh ) : ins_pos = ins [ 0 ] ins_len = ins [ 2 ] ol = overlap ( ins_pos , pos ) feat_len = pos [ 1 ] - pos [ 0 ] + 1 if float ( ol ) / float ( feat_len ) >= thresh : return True return False'], ['length of largest insertion', 'def max_insertion ( seqs , gene , domain ) : seqs = [ i [ 2 ] for i in list ( seqs . values ( ) ) if i [ 2 ] != [ ] and i [ 0 ] == gene and i [ 1 ] == domain ] lengths = [ ] for seq in seqs : for ins in seq : lengths . append ( int ( ins [ 2 ] ) ) if lengths == [ ] : return 100 return max ( lengths )'], ['get length of model', \"def model_length ( gene , domain ) : if gene == '16S' : domain2max = { 'E_coli_K12' : int ( 1538 ) , 'bacteria' : int ( 1689 ) , 'archaea' : int ( 1563 ) , 'eukarya' : int ( 2652 ) } return domain2max [ domain ] elif gene == '23S' : domain2max = { 'E_coli_K12' : int ( 2903 ) , 'bacteria' : int ( 3146 ) , 'archaea' : int ( 3774 ) , 'eukarya' : int ( 9079 ) } return domain2max [ domain ] else : print ( sys . stderr , '# length unknown for gene: %s, domain: %s' % ( gene , domain ) ) exit ( )\"], ['setup unique marker for every orf annotation - change size if necessary', \"def setup_markers ( seqs ) : family2marker = { } markers = cycle ( [ '^' , 'p' , '*' , '+' , 'x' , 'd' , '|' , 'v' , '>' , '<' , '8' ] ) size = 60 families = [ ] for seq in list ( seqs . values ( ) ) : for insertion in seq [ 2 ] : for family in list ( insertion [ - 1 ] . values ( ) ) : if family not in families : families . append ( family ) for family in families : marker = next ( markers ) if marker == '^' : size = size * 0.5 family2marker [ family ] = [ marker , size ] return family2marker\"], ['plot insertions for each gene and domain', 'def plot_by_gene_and_domain ( name , seqs , tax , id2name ) : for gene in set ( [ seq [ 0 ] for seq in list ( seqs . values ( ) ) ] ) : for domain in set ( [ seq [ 1 ] for seq in list ( seqs . values ( ) ) ] ) : plot_insertions ( name , seqs , gene , domain , tax , id2name )'], ['get the description for each ORF', \"def get_descriptions ( fastas ) : id2desc = { } for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ' ' ) id = header [ 0 ] if len ( header ) > 1 : desc = ' ' . join ( header [ 1 : ] ) else : desc = 'n/a' length = float ( len ( [ i for i in seq [ 1 ] . strip ( ) if i != '*' ] ) ) id2desc [ id ] = [ fasta , desc , length ] return id2desc\"], ['optimize later? slow ... should combine with calculate_threshold module', \"def print_genome_matrix ( hits , fastas , id2desc , file_name ) : out = open ( file_name , 'w' ) fastas = sorted ( fastas ) print ( '## percent identity between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : average = '-' else : average = numpy . average ( [ hits [ fasta ] [ other ] [ i ] [ 3 ] for i in hits [ fasta ] [ other ] ] ) line . append ( str ( average ) ) print ( '\\\\t' . join ( line ) , file = out ) print ( '' , file = out ) print ( '## percent of orfs that are orthologous between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : percent = '-' else : orthologs = float ( len ( hits [ fasta ] [ other ] ) ) orfs = float ( len ( [ i for i in id2desc if id2desc [ i ] [ 0 ] == fasta ] ) ) percent = float ( orthologs / orfs ) * 100 line . append ( str ( percent ) ) print ( '\\\\t' . join ( line ) , file = out )\"], ['compare genome to self to get the best possible bit score for each ORF', \"def self_compare ( fastas , id2desc , algorithm ) : for fasta in fastas : blast = open ( search ( fasta , fasta , method = algorithm , alignment = 'local' ) ) for hit in best_blast ( blast , 1 ) : id , bit = hit [ 0 ] . split ( ) [ 0 ] , float ( hit [ - 1 ] ) id2desc [ id ] . append ( bit ) return id2desc\"], ['if thresholds are not specififed calculate based on the distribution of normalized bit scores', \"def calc_thresholds ( rbh , file_name , thresholds = [ False , False , False , False ] , stdevs = 2 ) : calc_threshold = thresholds [ - 1 ] norm_threshold = { } for pair in itertools . permutations ( [ i for i in rbh ] , 2 ) : if pair [ 0 ] not in norm_threshold : norm_threshold [ pair [ 0 ] ] = { } norm_threshold [ pair [ 0 ] ] [ pair [ 1 ] ] = { } out = open ( file_name , 'w' ) print ( '#### summary of rbh comparisons\\\\n' , file = out ) comparisons = [ ] for genome in rbh : for compare in rbh [ genome ] : pair = '' . join ( sorted ( [ genome , compare ] ) ) if pair in comparisons : continue comparisons . append ( pair ) scores = { 'percent identity' : [ ] , 'e-value' : [ ] , 'bit score' : [ ] , 'normalized bit score' : [ ] , 'alignment length fraction' : [ ] } print ( '### blast between %s and %s\\\\n' % ( genome , compare ) , file = out ) for id in rbh [ genome ] [ compare ] : pident , length_fraction , e , bit , norm_bit = rbh [ genome ] [ compare ] [ id ] [ 3 : ] scores [ 'percent identity' ] . append ( pident ) scores [ 'alignment length fraction' ] . append ( length_fraction ) scores [ 'e-value' ] . append ( e ) scores [ 'bit score' ] . append ( bit ) scores [ 'normalized bit score' ] . append ( norm_bit ) if calc_threshold is True : norms = scores [ 'normalized bit score' ] average = numpy . average ( norms ) std = numpy . std ( norms ) normal_thresh = average - ( std * stdevs ) print ( '## average normalized bit score: %s' % average , file = out ) print ( '## standard deviation of normalized bit scores: %s' % std , file = out ) print ( '## normalized bit score threshold set to: %s\\\\n' % ( normal_thresh ) , file = out ) norm_threshold [ genome ] [ compare ] , norm_threshold [ compare ] [ genome ] = normal_thresh , normal_thresh for score in scores : print ( '## %s' % ( score ) , file = out ) if len ( scores [ score ] ) > 0 : print ( '## average: %s' % numpy . average ( scores [ score ] ) , file = out ) print ( '' , file = out ) out . close ( ) if calc_threshold is True : return thresholds [ 0 : - 1 ] + [ norm_threshold ] else : return thresholds\"], ['make and split a rbh network', \"def neto ( fastas , algorithm = 'usearch' , e = 0.01 , bit = 40 , length = .65 , norm_bit = False ) : thresholds = [ e , bit , length , norm_bit ] id2desc = get_descriptions ( fastas ) id2desc = self_compare ( fastas , id2desc , algorithm ) hits = compare_genomes ( fastas , id2desc , algorithm ) calc_thresholds ( hits , file_name = 'fbh.scores.summary.txt' ) rbh_network ( id2desc , hits , file_name = 'fbh.network.edges.txt' ) hits , rbh = find_rbh ( hits , id2desc ) thresholds = calc_thresholds ( rbh , 'rbh.scores.summary.txt' , thresholds ) g = rbh_network ( id2desc , rbh , file_name = 'rbh.network.edges.txt' ) filtered_g , filtered_rbh = rbh_network ( id2desc , rbh , 'rbh.filtered.network.edges.txt' , thresholds ) calc_thresholds ( filtered_rbh , file_name = 'rbh.filtered.scores.summary.txt' ) print_summary ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.nodes.txt' ) print_network_matrix ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.matrix.txt' ) print_genome_matrix ( filtered_rbh , fastas , id2desc , file_name = 'rbh.filtered.network.genome_matrix.txt' ) split_g = split_network ( filtered_g , id2desc , file_name = 'rbh.filtered.split.network.edges.txt' ) print_summary ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.nodes.txt' ) print_network_matrix ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.matrix.txt' ) return split_g\"], ['Collapses multiple dimensions into a single raster_info complex struct', \"def _parse_raster_info ( self , prop = RASTER_INFO ) : raster_info = { } . fromkeys ( _iso_definitions [ prop ] , u'' ) raster_info [ 'dimensions' ] = get_default_for_complex_sub ( prop = prop , subprop = 'dimensions' , value = parse_property ( self . _xml_tree , None , self . _data_map , '_ri_num_dims' ) , xpath = self . _data_map [ '_ri_num_dims' ] ) xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] for dimension in parse_complex_list ( self . _xml_tree , xpath_root , xpath_map , RASTER_DIMS ) : dimension_type = dimension [ 'type' ] . lower ( ) if dimension_type == 'vertical' : raster_info [ 'vertical_count' ] = dimension [ 'size' ] elif dimension_type == 'column' : raster_info [ 'column_count' ] = dimension [ 'size' ] raster_info [ 'x_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) elif dimension_type == 'row' : raster_info [ 'row_count' ] = dimension [ 'size' ] raster_info [ 'y_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) return raster_info if any ( raster_info [ k ] for k in raster_info ) else { }\"], ['Derives multiple dimensions from a single raster_info complex struct', \"def _update_raster_info ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = update_props . pop ( 'values' ) xroot , xpath = None , self . _data_map [ '_ri_num_dims' ] raster_info = [ update_property ( tree_to_update , xroot , xpath , prop , values . get ( 'dimensions' , u'' ) ) ] xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] v_dimension = { } if values . get ( 'vertical_count' ) : v_dimension = v_dimension . fromkeys ( xpath_map , u'' ) v_dimension [ 'type' ] = 'vertical' v_dimension [ 'size' ] = values . get ( 'vertical_count' , u'' ) x_dimension = { } if values . get ( 'column_count' ) or values . get ( 'x_resolution' ) : x_dimension = x_dimension . fromkeys ( xpath_map , u'' ) x_dimension [ 'type' ] = 'column' x_dimension [ 'size' ] = values . get ( 'column_count' , u'' ) x_dimension [ 'value' ] = values . get ( 'x_resolution' , u'' ) y_dimension = { } if values . get ( 'row_count' ) or values . get ( 'y_resolution' ) : y_dimension = y_dimension . fromkeys ( xpath_map , u'' ) y_dimension [ 'type' ] = 'row' y_dimension [ 'size' ] = values . get ( 'row_count' , u'' ) y_dimension [ 'value' ] = values . get ( 'y_resolution' , u'' ) update_props [ 'prop' ] = RASTER_DIMS update_props [ 'values' ] = [ v_dimension , x_dimension , y_dimension ] raster_info += update_complex_list ( xpath_root = xpath_root , xpath_map = xpath_map , ** update_props ) return raster_info\"], ['Removes primitive type tags from an XPATH', 'def _trim_xpath ( self , xpath , prop ) : xroot = self . _get_xroot_for ( prop ) if xroot is None and isinstance ( xpath , string_types ) : xtags = xpath . split ( XPATH_DELIM ) if xtags [ - 1 ] in _iso_tag_primitives : xroot = XPATH_DELIM . join ( xtags [ : - 1 ] ) return xroot'], ['Generates the app id for a given shortcut . Steam uses app ids as a unique identifier for games but since shortcuts dont have a canonical serverside representation they need to be generated on the fly . The important part about this function is that it will generate the same app id as Steam does for a given shortcut', \"def shortcut_app_id ( shortcut ) : algorithm = Crc ( width = 32 , poly = 0x04C11DB7 , reflect_in = True , xor_in = 0xffffffff , reflect_out = True , xor_out = 0xffffffff ) crc_input = '' . join ( [ shortcut . exe , shortcut . name ] ) high_32 = algorithm . bit_by_bit ( crc_input ) | 0x80000000 full_64 = ( high_32 << 32 ) | 0x02000000 return str ( full_64 )\"], ['Execute git config .', \"def _config ( self ) : cfg_wr = self . repo . config_writer ( ) cfg_wr . add_section ( 'user' ) cfg_wr . set_value ( 'user' , 'name' , self . metadata . author ) cfg_wr . set_value ( 'user' , 'email' , self . metadata . email ) cfg_wr . release ( )\"], ['Execute git remote add .', \"def _remote_add ( self ) : self . repo . create_remote ( 'origin' , 'git@github.com:{username}/{repo}.git' . format ( username = self . metadata . username , repo = self . metadata . name ) )\"], ['Starts execution of the script', 'def start ( self ) : try : self . args . func ( ) except SystemExit as e : if e . code != 0 : raise except KeyboardInterrupt : self . log . warning ( \"exited via keyboard interrupt\" ) except : self . log . exception ( \"exited start function\" ) finally : self . _flush_metrics_q . put ( None , block = True ) self . _flush_metrics_q . put ( None , block = True , timeout = 1 ) self . log . debug ( \"exited_successfully\" )'], ['Define basic command - line arguments required by the script .', 'def define_baseargs ( self , parser ) : parser . add_argument ( \\'--name\\' , default = sys . argv [ 0 ] , help = \\'Name to identify this instance\\' ) parser . add_argument ( \\'--log-level\\' , default = None , help = \\'Logging level as picked from the logging module\\' ) parser . add_argument ( \\'--log-format\\' , default = None , choices = ( \"json\" , \"pretty\" , ) , help = ( \"Force the format of the logs. By default, if the \" \"command is from a terminal, print colorful logs. \" \"Otherwise print json.\" ) , ) parser . add_argument ( \\'--log-file\\' , default = None , help = \\'Writes logs to log file if specified, default: %(default)s\\' , ) parser . add_argument ( \\'--quiet\\' , default = False , action = \"store_true\" , help = \\'if true, does not print logs to stderr, default: %(default)s\\' , ) parser . add_argument ( \\'--metric-grouping-interval\\' , default = None , type = int , help = \\'To group metrics based on time interval ex:10 i.e;(10 sec)\\' , ) parser . add_argument ( \\'--debug\\' , default = False , action = \"store_true\" , help = \\'To run the code in debug mode\\' , )'], ['Basically turns payload that looks like \\\\\\\\ n to . In the calling function if this function returns no object is added for that payload .', \"def cleanup_payload ( self , payload ) : p = payload . replace ( '\\\\n' , '' ) p = p . rstrip ( ) p = p . lstrip ( ) return p\"], ['Ensures complex property types have the correct default values', \"def get_default_for ( prop , value ) : prop = prop . strip ( '_' ) val = reduce_value ( value ) if prop in _COMPLEX_LISTS : return wrap_value ( val ) elif prop in _COMPLEX_STRUCTS : return val or { } else : return u'' if val is None else val\"], ['Either update the tree the default way or call the custom updater', \"def update_property ( tree_to_update , xpath_root , xpaths , prop , values , supported = None ) : if supported and prop . startswith ( '_' ) and prop . strip ( '_' ) in supported : values = u'' else : values = get_default_for ( prop , values ) if not xpaths : return [ ] elif not isinstance ( xpaths , ParserProperty ) : return _update_property ( tree_to_update , xpath_root , xpaths , values ) else : return xpaths . set_prop ( tree_to_update = tree_to_update , prop = prop , values = values )\"], ['Default update operation for a single parser property . If xpaths contains one xpath then one element per value will be inserted at that location in the tree_to_update ; otherwise the number of values must match the number of xpaths .', \"def _update_property ( tree_to_update , xpath_root , xpaths , values ) : def update_element ( elem , idx , root , path , vals ) : has_root = bool ( root and len ( path ) > len ( root ) and path . startswith ( root ) ) path , attr = get_xpath_tuple ( path ) if attr : removed = [ get_element ( elem , path ) ] remove_element_attributes ( removed [ 0 ] , attr ) elif not has_root : removed = wrap_value ( remove_element ( elem , path ) ) else : path = get_xpath_branch ( root , path ) removed = [ ] if idx != 0 else [ remove_element ( e , path , True ) for e in get_elements ( elem , root ) ] if not vals : return removed items = [ ] for i , val in enumerate ( wrap_value ( vals ) ) : elem_to_update = elem if has_root : elem_to_update = insert_element ( elem , ( i + idx ) , root ) val = val . decode ( 'utf-8' ) if not isinstance ( val , string_types ) else val if not attr : items . append ( insert_element ( elem_to_update , i , path , val ) ) elif path : items . append ( insert_element ( elem_to_update , i , path , ** { attr : val } ) ) else : set_element_attributes ( elem_to_update , ** { attr : val } ) items . append ( elem_to_update ) return items xpaths = reduce_value ( xpaths ) values = filter_empty ( values ) if isinstance ( xpaths , string_types ) : return update_element ( tree_to_update , 0 , xpath_root , xpaths , values ) else : each = [ ] for index , xpath in enumerate ( xpaths ) : value = values [ index ] if values else None each . extend ( update_element ( tree_to_update , index , xpath_root , xpath , value ) ) return each\"], ['Default validation for single complex data structure', \"def validate_complex ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for complex_prop , complex_val in iteritems ( value ) : complex_key = '.' . join ( ( prop , complex_prop ) ) if complex_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) validate_type ( complex_key , complex_val , ( string_types , list ) )\"], ['Default validation for Attribute Details data structure', \"def validate_complex_list ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for idx , complex_struct in enumerate ( wrap_value ( value ) ) : cs_idx = prop + '[' + str ( idx ) + ']' validate_type ( cs_idx , complex_struct , dict ) for cs_prop , cs_val in iteritems ( complex_struct ) : cs_key = '.' . join ( ( cs_idx , cs_prop ) ) if cs_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) if not isinstance ( cs_val , list ) : validate_type ( cs_key , cs_val , ( string_types , list ) ) else : for list_idx , list_val in enumerate ( cs_val ) : list_prop = cs_key + '[' + str ( list_idx ) + ']' validate_type ( list_prop , list_val , string_types )\"], ['Default validation for Date Types data structure', \"def validate_dates ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) date_keys = set ( value ) if date_keys : if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys : if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = _complex_definitions [ DATES ] if xpath_map is None else xpath_map _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) date_type = value [ DATE_TYPE ] if date_type not in DATE_TYPES : _validation_error ( 'dates.type' , None , date_type , DATE_TYPES ) date_vals = value [ DATE_VALUES ] validate_type ( 'dates.values' , date_vals , list ) dates_len = len ( date_vals ) if date_type == DATE_TYPE_MISSING and dates_len != 0 : _validation_error ( 'len(dates.values)' , None , dates_len , 0 ) if date_type == DATE_TYPE_SINGLE and dates_len != 1 : _validation_error ( 'len(dates.values)' , None , dates_len , 1 ) if date_type == DATE_TYPE_RANGE and dates_len != 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 2 ) if date_type == DATE_TYPE_MULTIPLE and dates_len < 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 'at least two' ) for idx , date in enumerate ( date_vals ) : date_key = 'dates.value[' + str ( idx ) + ']' validate_type ( date_key , date , string_types )\"], ['Default validation for Process Steps data structure', \"def validate_process_steps ( prop , value ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) procstep_keys = set ( _complex_definitions [ prop ] ) for idx , procstep in enumerate ( wrap_value ( value ) ) : ps_idx = prop + '[' + str ( idx ) + ']' validate_type ( ps_idx , procstep , dict ) for ps_prop , ps_val in iteritems ( procstep ) : ps_key = '.' . join ( ( ps_idx , ps_prop ) ) if ps_prop not in procstep_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( procstep_keys ) ) ) ) if ps_prop != 'sources' : validate_type ( ps_key , ps_val , string_types ) else : validate_type ( ps_key , ps_val , ( string_types , list ) ) for src_idx , src_val in enumerate ( wrap_value ( ps_val ) ) : src_key = ps_key + '[' + str ( src_idx ) + ']' validate_type ( src_key , src_val , string_types )\"], ['Default validation for all types', 'def validate_type ( prop , value , expected ) : if value is not None and not isinstance ( value , expected ) : _validation_error ( prop , type ( value ) . __name__ , None , expected )'], ['Default validation for updated properties', \"def _validation_error ( prop , prop_type , prop_value , expected ) : if prop_type is None : attrib = 'value' assigned = prop_value else : attrib = 'type' assigned = prop_type raise ValidationError ( 'Invalid property {attrib} for {prop}:\\\\n\\\\t{attrib}: {assigned}\\\\n\\\\texpected: {expected}' , attrib = attrib , prop = prop , assigned = assigned , expected = expected , invalid = { prop : prop_value } if attrib == 'value' else { } )\"], ['Calls the getter with no arguments and returns its value', 'def get_prop ( self , prop ) : if self . _parser is None : raise ConfigurationError ( \\'Cannot call ParserProperty.\"get_prop\" with no parser configured\\' ) return self . _parser ( prop ) if prop else self . _parser ( )'], ['Returns a boolean representing whether these commands can be grouped together or not .', \"def can_group_commands ( command , next_command ) : multi_capable_commands = ( 'get' , 'set' , 'delete' ) if next_command is None : return False name = command . get_name ( ) if name not in multi_capable_commands : return False if name != next_command . get_name ( ) : return False if grouped_args_for_command ( command ) != grouped_args_for_command ( next_command ) : return False if command . get_kwargs ( ) != next_command . get_kwargs ( ) : return False return True\"], ['define ribosomal proteins and location of curated databases', \"def find_databases ( databases ) : proteins = [ 'L15' , 'L18' , 'L6' , 'S8' , 'L5' , 'L24' , 'L14' , 'S17' , 'L16' , 'S3' , 'L22' , 'S19' , 'L2' , 'L4' , 'L3' , 'S10' ] protein_databases = { 'L14' : 'rpL14_JGI_MDM.filtered.faa' , 'L15' : 'rpL15_JGI_MDM.filtered.faa' , 'L16' : 'rpL16_JGI_MDM.filtered.faa' , 'L18' : 'rpL18_JGI_MDM.filtered.faa' , 'L22' : 'rpL22_JGI_MDM.filtered.faa' , 'L24' : 'rpL24_JGI_MDM.filtered.faa' , 'L2' : 'rpL2_JGI_MDM.filtered.faa' , 'L3' : 'rpL3_JGI_MDM.filtered.faa' , 'L4' : 'rpL4_JGI_MDM.filtered.faa' , 'L5' : 'rpL5_JGI_MDM.filtered.faa' , 'L6' : 'rpL6_JGI_MDM.filtered.faa' , 'S10' : 'rpS10_JGI_MDM.filtered.faa' , 'S17' : 'rpS17_JGI_MDM.filtered.faa' , 'S19' : 'rpS19_JGI_MDM.filtered.faa' , 'S3' : 'rpS3_JGI_MDM.filtered.faa' , 'S8' : 'rpS8_JGI_MDM.filtered.faa' } protein_databases = { key : '%s/%s' % ( databases , database ) for key , database in list ( protein_databases . items ( ) ) } return proteins , protein_databases\"], ['which protein has the best hit the one to the right or to the left?', 'def find_next ( start , stop , i2hits ) : if start not in i2hits and stop in i2hits : index = stop elif stop not in i2hits and start in i2hits : index = start elif start not in i2hits and stop not in i2hits : index = choice ( [ start , stop ] ) i2hits [ index ] = [ [ False ] ] else : A , B = i2hits [ start ] [ 0 ] , i2hits [ stop ] [ 0 ] if B [ 10 ] <= A [ 10 ] : index = stop else : index = start if index == start : nstart = start - 1 nstop = stop else : nstop = stop + 1 nstart = start match = i2hits [ index ] [ 0 ] rp = match [ - 1 ] return index , nstart , nstop , rp , match'], ['determine which hits represent real ribosomal proteins identify each in syntenic block max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold', 'def find_ribosomal ( rps , scaffolds , s2rp , min_hits , max_hits_rp , max_errors ) : for scaffold , proteins in list ( s2rp . items ( ) ) : hits = { p : [ i for i in sorted ( hits , key = itemgetter ( 10 ) ) ] [ 0 : max_hits_rp ] for p , hits in list ( proteins . items ( ) ) if len ( hits ) > 0 } if len ( hits ) < min_hits : continue best = sorted ( [ hit [ 0 ] + [ p ] for p , hit in list ( hits . items ( ) ) ] , key = itemgetter ( 10 ) ) [ 0 ] block = find_block ( rps , scaffolds [ scaffold ] , hits , best , max_errors ) if ( len ( block ) - 1 ) >= min_hits : yield scaffold , block'], ['Parse the rep set file and remove all sequences not associated with unique OTUs .', 'def filter_rep_set ( inF , otuSet ) : seqs = [ ] for record in SeqIO . parse ( inF , \"fasta\" ) : if record . id in otuSet : seqs . append ( record ) return seqs'], ['Update the text for each element at the configured path if attribute matches', \"def _update_report_item ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = wrap_value ( update_props [ 'values' ] ) xroot = self . _get_xroot_for ( prop ) attr_key = 'type' attr_val = u'' if prop == 'attribute_accuracy' : attr_val = 'DQQuanAttAcc' elif prop == 'dataset_completeness' : attr_val = 'DQCompOm' for elem in get_elements ( tree_to_update , xroot ) : if get_element_attributes ( elem ) . get ( attr_key ) == attr_val : clear_element ( elem ) remove_empty_element ( tree_to_update , xroot ) attrs = { attr_key : attr_val } updated = [ ] for idx , value in enumerate ( values ) : elem = insert_element ( tree_to_update , idx , xroot , ** attrs ) updated . append ( insert_element ( elem , idx , 'measDesc' , value ) ) return updated\"], ['Clear the specified interrupt bit in the interrupt status register .', 'def _clear_interrupt ( self , intbit ) : int_status = self . _device . readU8 ( VCNL4010_INTSTAT ) int_status &= ~ intbit self . _device . write8 ( VCNL4010_INTSTAT , int_status )'], ['Swaps two nodes', 'def move ( self ) : a = random . randint ( 0 , len ( self . state ) - 1 ) b = random . randint ( 0 , len ( self . state ) - 1 ) self . state [ [ a , b ] ] = self . state [ [ b , a ] ]'], ['A bool - if the certificate should be self - signed .', 'def self_signed ( self , value ) : self . _self_signed = bool ( value ) if self . _self_signed : self . _issuer = None'], ['Grabs the first URL out of a asn1crypto . x509 . CRLDistributionPoints object', \"def _get_crl_url ( self , distribution_points ) : if distribution_points is None : return None for distribution_point in distribution_points : name = distribution_point [ 'distribution_point' ] if name . name == 'full_name' and name . chosen [ 0 ] . name == 'uniform_resource_identifier' : return name . chosen [ 0 ] . chosen . native return None\"], ['A bool - if the certificate should have the OCSP no check extension . Only applicable to certificates created for signing OCSP responses . Such certificates should normally be issued for a very short period of time since they are effectively whitelisted by clients .', 'def ocsp_no_check ( self , value ) : if value is None : self . _ocsp_no_check = None else : self . _ocsp_no_check = bool ( value )'], ['Removes empty line .', \"def emptylineless ( parser , token ) : nodelist = parser . parse ( ( 'endemptylineless' , ) ) parser . delete_first_token ( ) return EmptylinelessNode ( nodelist )\"], ['Do an HTTP PURGE of the given asset . The URL is run through urlparse and must point to the varnish instance not the varnishadm', \"def http_purge_url ( url ) : url = urlparse ( url ) connection = HTTPConnection ( url . hostname , url . port or 80 ) path = url . path or '/' connection . request ( 'PURGE' , '%s?%s' % ( path , url . query ) if url . query else path , '' , { 'Host' : '%s:%s' % ( url . hostname , url . port ) if url . port else url . hostname } ) response = connection . getresponse ( ) if response . status != 200 : logging . error ( 'Purge failed with status: %s' % response . status ) return response\"], ['Non - threaded batch command runner returning output results', \"def run ( addr , * commands , ** kwargs ) : results = [ ] handler = VarnishHandler ( addr , ** kwargs ) for cmd in commands : if isinstance ( cmd , tuple ) and len ( cmd ) > 1 : results . extend ( [ getattr ( handler , c [ 0 ] . replace ( '.' , '_' ) ) ( * c [ 1 : ] ) for c in cmd ] ) else : results . append ( getattr ( handler , cmd . replace ( '.' , '_' ) ) ( * commands [ 1 : ] ) ) break handler . close ( ) return results\"], ['add stylesheet files in HTML head', 'def add_stylesheets ( self , * css_files ) : for css_file in css_files : self . main_soup . style . append ( self . _text_file ( css_file ) )'], ['add javascripts files in HTML body', \"def add_javascripts ( self , * js_files ) : if self . main_soup . script is None : script_tag = self . main_soup . new_tag ( 'script' ) self . main_soup . body . append ( script_tag ) for js_file in js_files : self . main_soup . script . append ( self . _text_file ( js_file ) )\"], ['return the object in a file', \"def export ( self ) : with open ( self . export_url , 'w' , encoding = 'utf-8' ) as file : file . write ( self . build ( ) ) if self . open_browser : webbrowser . open_new_tab ( self . export_url )\"], ['convert Markdown text as html . return the html file as string', \"def build ( self ) : markdown_html = markdown . markdown ( self . markdown_text , extensions = [ TocExtension ( ) , 'fenced_code' , 'markdown_checklist.extension' , 'markdown.extensions.tables' ] ) markdown_soup = BeautifulSoup ( markdown_html , 'html.parser' ) if markdown_soup . find ( 'code' , attrs = { 'class' : 'mermaid' } ) : self . _add_mermaid_js ( ) for dot_tag in markdown_soup . find_all ( 'code' , attrs = { 'class' : 'dotgraph' } ) : grap_svg = self . _text_to_graphiz ( dot_tag . string ) graph_soup = BeautifulSoup ( grap_svg , 'html.parser' ) dot_tag . parent . replaceWith ( graph_soup ) self . main_soup . body . append ( markdown_soup ) return self . main_soup . prettify ( )\"], ['return the content of a file', \"def _text_file ( self , url ) : try : with open ( url , 'r' , encoding = 'utf-8' ) as file : return file . read ( ) except FileNotFoundError : print ( 'File `{}` not found' . format ( url ) ) sys . exit ( 0 )\"], ['create a graphviz graph from text', \"def _text_to_graphiz ( self , text ) : dot = Source ( text , format = 'svg' ) return dot . pipe ( ) . decode ( 'utf-8' )\"], ['add js libraries and css files of mermaid js_file', \"def _add_mermaid_js ( self ) : self . add_javascripts ( '{}/js/jquery-1.11.3.min.js' . format ( self . resources_path ) ) self . add_javascripts ( '{}/js/mermaid.min.js' . format ( self . resources_path ) ) self . add_stylesheets ( '{}/css/mermaid.css' . format ( self . resources_path ) ) self . main_soup . script . append ( 'mermaid.initialize({startOnLoad:true  });' )\"], ['Get a character set with individual members or ranges .', 'def getCharacterSet ( self ) : chars = u\\'\\' c = None cnt = 1 start = 0 while True : escaped_slash = False c = self . next ( ) if self . lookahead ( ) == u\\'-\\' and not c == u\\'\\\\\\\\\\' : f = c self . next ( ) c = self . next ( ) if not c or ( c in self . meta_chars ) : raise StringGenerator . SyntaxError ( u\"unexpected end of class range\" ) chars += self . getCharacterRange ( f , c ) elif c == u\\'\\\\\\\\\\' : if self . lookahead ( ) in self . meta_chars : c = self . next ( ) chars += c continue elif self . lookahead ( ) in self . string_code : c = self . next ( ) chars += self . string_code [ c ] elif c and c not in self . meta_chars : chars += c if c == u\\']\\' : if self . lookahead ( ) == u\\'{\\' : [ start , cnt ] = self . getQuantifier ( ) else : start = - 1 cnt = 1 break if c and c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped character in class definition: %s\" % c ) if not c : break return StringGenerator . CharacterSet ( chars , start , cnt )'], ['Get a sequence of non - special characters .', 'def getLiteral ( self ) : chars = u\\'\\' c = self . current ( ) while True : if c and c == u\"\\\\\\\\\" : c = self . next ( ) if c : chars += c continue elif not c or ( c in self . meta_chars ) : break else : chars += c if self . lookahead ( ) and self . lookahead ( ) in self . meta_chars : break c = self . next ( ) return StringGenerator . Literal ( chars )'], ['Get a sequence of nodes .', 'def getSequence ( self , level = 0 ) : seq = [ ] op = \\'\\' left_operand = None right_operand = None sequence_closed = False while True : c = self . next ( ) if not c : break if c and c not in self . meta_chars : seq . append ( self . getLiteral ( ) ) elif c and c == u\\'$\\' and self . lookahead ( ) == u\\'{\\' : seq . append ( self . getSource ( ) ) elif c == u\\'[\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getCharacterSet ( ) ) elif c == u\\'(\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getSequence ( level + 1 ) ) elif c == u\\')\\' and not self . last ( ) == u\\'\\\\\\\\\\' : if level == 0 : raise StringGenerator . SyntaxError ( u\"Extra closing parenthesis\" ) sequence_closed = True break elif c == u\\'|\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c elif c == u\\'&\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c else : if c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped special character: %s\" % c ) if op and not left_operand : if not seq or len ( seq ) < 1 : raise StringGenerator . SyntaxError ( u\"Operator: %s with no left operand\" % op ) left_operand = seq . pop ( ) elif op and len ( seq ) >= 1 and left_operand : right_operand = seq . pop ( ) if op == u\\'|\\' : seq . append ( StringGenerator . SequenceOR ( [ left_operand , right_operand ] ) ) elif op == u\\'&\\' : seq . append ( StringGenerator . SequenceAND ( [ left_operand , right_operand ] ) ) op = u\\'\\' left_operand = None right_operand = None if op : raise StringGenerator . SyntaxError ( u\"Operator: %s with no right operand\" % op ) if level > 0 and not sequence_closed : raise StringGenerator . SyntaxError ( u\"Missing closing parenthesis\" ) return StringGenerator . Sequence ( seq )'], ['Print the parse tree and then call render for an example .', 'def dump ( self , ** kwargs ) : import sys if not self . seq : self . seq = self . getSequence ( ) print ( \"StringGenerator version: %s\" % ( __version__ ) ) print ( \"Python version: %s\" % sys . version ) self . seq . dump ( ) return self . render ( ** kwargs )'], ['Return a list of generated strings .', 'def render_list ( self , cnt , unique = False , progress_callback = None , ** kwargs ) : rendered_list = [ ] i = 0 total_attempts = 0 while True : if i >= cnt : break if total_attempts > cnt * self . unique_attempts_factor : raise StringGenerator . UniquenessError ( u\"couldn\\'t satisfy uniqueness\" ) s = self . render ( ** kwargs ) if unique : if not s in rendered_list : rendered_list . append ( s ) i += 1 else : rendered_list . append ( s ) i += 1 total_attempts += 1 if progress_callback and callable ( progress_callback ) : progress_callback ( i , cnt ) return rendered_list'], ['Establish the connection . This is done automatically for you .', 'def connect ( self ) : self . conn = boto . connect_s3 ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL ) self . bucket = self . conn . get_bucket ( self . AWS_STORAGE_BUCKET_NAME ) self . k = Key ( self . bucket )'], ['Connect to Cloud Front . This is done automatically for you when needed .', 'def connect_cloudfront ( self ) : \"Connect to Cloud Front. This is done automatically for you when needed.\" self . conn_cloudfront = connect_cloudfront ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL )'], ['Create a folder on S3 .', 'def mkdir ( self , target_folder ) : self . printv ( \"Making directory: %s\" % target_folder ) self . k . key = re . sub ( r\"^/|/$\" , \"\" , target_folder ) + \"/\" self . k . set_contents_from_string ( \\'\\' ) self . k . close ( )'], ['Delete the path and anything under the path .', 'def rm ( self , path ) : list_of_files = list ( self . ls ( path ) ) if list_of_files : if len ( list_of_files ) == 1 : self . bucket . delete_key ( list_of_files [ 0 ] ) else : self . bucket . delete_keys ( list_of_files ) self . printv ( \"Deleted: %s\" % list_of_files ) else : logger . error ( \"There was nothing to remove under %s\" , path )'], ['Copy a file to s3 .', 'def __put_key ( self , local_file , target_file , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , source = \"filename\" ) : action_word = \"moving\" if del_after_upload else \"copying\" try : self . k . key = target_file if source == \"filename\" : self . k . set_contents_from_filename ( local_file , self . AWS_HEADERS ) elif source == \"fileobj\" : self . k . set_contents_from_file ( local_file , self . AWS_HEADERS ) elif source == \"string\" : self . k . set_contents_from_string ( local_file , self . AWS_HEADERS ) else : raise Exception ( \"%s is not implemented as a source.\" % source ) self . k . set_acl ( acl ) self . k . close ( ) self . printv ( \"%s %s to %s\" % ( action_word , local_file , target_file ) ) if del_after_upload and source == \"filename\" : try : os . remove ( local_file ) except : logger . error ( \"Unable to delete the file: \" , local_file , exc_info = True ) return True except : logger . error ( \"Error in writing to %s\" , target_file , exc_info = True ) return False'], ['Copy a file or folder from local to s3 .', 'def cp ( self , local_path , target_path , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , invalidate = False ) : result = None if overwrite : list_of_files = [ ] else : list_of_files = self . ls ( folder = target_path , begin_from_file = \"\" , num = - 1 , get_grants = False , all_grant_data = False ) if local_path . endswith ( \"/*\" ) : local_path = local_path [ : - 2 ] target_path = re . sub ( r\"^/|/$\" , \"\" , target_path ) else : local_base_name = os . path . basename ( local_path ) local_path = re . sub ( r\"/$\" , \"\" , local_path ) target_path = re . sub ( r\"^/\" , \"\" , target_path ) if not target_path . endswith ( local_base_name ) : target_path = os . path . join ( target_path , local_base_name ) if os . path . exists ( local_path ) : result = self . __find_files_and_copy ( local_path , target_path , acl , del_after_upload , overwrite , invalidate , list_of_files ) else : result = { \\'file_does_not_exist\\' : local_path } logger . error ( \"trying to upload to s3 but file doesn\\'t exist: %s\" % local_path ) return result'], ['Similar to Linux mv command .', \"def mv ( self , local_file , target_file , acl = 'public-read' , overwrite = True , invalidate = False ) : self . cp ( local_file , target_file , acl = acl , del_after_upload = True , overwrite = overwrite , invalidate = invalidate )\"], ['Deal with saving cropduster images to S3 . Cropduster is a Django library for resizing editorial images . S3utils was originally written to put cropduster images on S3 bucket .', 'def cp_cropduster_image ( self , the_image_path , del_after_upload = False , overwrite = False , invalidate = False ) : local_file = os . path . join ( settings . MEDIA_ROOT , the_image_path ) if os . path . exists ( local_file ) : the_image_crops_path = os . path . splitext ( the_image_path ) [ 0 ] the_image_crops_path_full_path = os . path . join ( settings . MEDIA_ROOT , the_image_crops_path ) self . cp ( local_path = local_file , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , ) self . cp ( local_path = the_image_crops_path_full_path + \"/*\" , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_crops_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , )'], ['sets permissions for a file on S3', \"def chmod ( self , target_file , acl = 'public-read' ) : self . k . key = target_file self . k . set_acl ( acl ) self . k . close ( )\"], ['Get the list of files and permissions from S3 .', 'def ll ( self , folder = \"\" , begin_from_file = \"\" , num = - 1 , all_grant_data = False ) : return self . ls ( folder = folder , begin_from_file = begin_from_file , num = num , get_grants = True , all_grant_data = all_grant_data )'], ['Get the path from a given url including the querystring .', 'def get_path ( url ) : url = urlsplit ( url ) path = url . path if url . query : path += \"?{}\" . format ( url . query ) return path'], ['Reads data from disk and generates CSV files .', \"def run ( self ) : if not os . path . exists ( self . output ) : try : os . mkdir ( self . output ) except : print 'failed to create output directory %s' % self . output if not os . path . isdir ( self . output ) : print 'invalid output directory %s' % self . output sys . exit ( 1 ) visitors = [ _CompaniesCSV ( self . output ) , _ActivitiesCSV ( self . output ) , _ActivitiesSeenCSV ( self . output ) , _QSACSV ( self . output ) , ] for path in glob . glob ( os . path . join ( self . input , '*.json' ) ) : with open ( path , 'r' ) as f : try : data = json . load ( f , encoding = 'utf-8' ) except ValueError : continue for visitor in visitors : visitor . visit ( data )\"], ['Process a list of simple string field definitions and assign their order based on prefix .', \"def process_fields ( self , fields ) : result = [ ] strip = '' . join ( self . PREFIX_MAP ) for field in fields : direction = self . PREFIX_MAP [ '' ] if field [ 0 ] in self . PREFIX_MAP : direction = self . PREFIX_MAP [ field [ 0 ] ] field = field . lstrip ( strip ) result . append ( ( field , direction ) ) return result\"], ['Firms search in rubric', \"def search_in_rubric ( self , ** kwargs ) : point = kwargs . pop ( 'point' , False ) if point : kwargs [ 'point' ] = '%s,%s' % point bound = kwargs . pop ( 'bound' , False ) if bound : kwargs [ 'bound[point1]' ] = bound [ 0 ] kwargs [ 'bound[point2]' ] = bound [ 1 ] filters = kwargs . pop ( 'filters' , False ) if filters : for k , v in filters . items ( ) : kwargs [ 'filters[%s]' % k ] = v return self . _search_in_rubric ( ** kwargs )\"], ['Refresh the list and the screen', 'def refresh ( self ) : self . _screen . force_update ( ) self . _screen . refresh ( ) self . _update ( 1 )'], ['Mark an action as started', 'def start ( self , activity , action ) : try : self . _start_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . refresh ( )'], ['Mark a task as completed', 'def stop ( self , activity , action ) : try : self . _remove_running_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . _mark_action_completed ( activity , action ) self . refresh ( )'], ['Move laggard tasks over', 'def finish ( self , status ) : retox_log . info ( \"Completing %s with status %s\" % ( self . name , status ) ) result = Screen . COLOUR_GREEN if not status else Screen . COLOUR_RED self . palette [ \\'title\\' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , result ) for item in list ( self . _task_view . options ) : self . _task_view . options . remove ( item ) self . _completed_view . options . append ( item ) self . refresh ( )'], ['Reset the frame between jobs', \"def reset ( self ) : self . palette [ 'title' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , Screen . COLOUR_BLUE ) self . _completed_view . options = [ ] self . _task_view . options = [ ] self . refresh ( )\"], ['Returns the available kwargs of the called class', 'def default_arguments ( cls ) : func = cls . __init__ args = func . __code__ . co_varnames defaults = func . __defaults__ index = - len ( defaults ) return { k : v for k , v in zip ( args [ index : ] , defaults ) }'], ['Recreate the class based in your args multiple uses', 'def recreate ( cls , * args , ** kwargs ) : cls . check_arguments ( kwargs ) first_is_callable = True if any ( args ) and callable ( args [ 0 ] ) else False signature = cls . default_arguments ( ) allowed_arguments = { k : v for k , v in kwargs . items ( ) if k in signature } if ( any ( allowed_arguments ) or any ( args ) ) and not first_is_callable : if any ( args ) and not first_is_callable : return cls ( args [ 0 ] , ** allowed_arguments ) elif any ( allowed_arguments ) : return cls ( ** allowed_arguments ) return cls . instances [ - 1 ] if any ( cls . instances ) else cls ( )'], ['Put warnings of arguments whose can t be handle by the class', 'def check_arguments ( cls , passed ) : defaults = list ( cls . default_arguments ( ) . keys ( ) ) template = ( \"Pass arg {argument:!r} in {cname:!r}, can be a typo? \" \"Supported key arguments: {defaults}\" ) fails = [ ] for arg in passed : if arg not in defaults : warn ( template . format ( argument = arg , cname = cls . __name__ , defaults = defaults ) ) fails . append ( arg ) return any ( fails )'], ['process the specified type then process its children', 'def process ( self , data , type , history ) : if type in history : return if type . enum ( ) : return history . append ( type ) resolved = type . resolve ( ) value = None if type . multi_occurrence ( ) : value = [ ] else : if len ( resolved ) > 0 : if resolved . mixed ( ) : value = Factory . property ( resolved . name ) md = value . __metadata__ md . sxtype = resolved else : value = Factory . object ( resolved . name ) md = value . __metadata__ md . sxtype = resolved md . ordering = self . ordering ( resolved ) setattr ( data , type . name , value ) if value is not None : data = value if not isinstance ( data , list ) : self . add_attributes ( data , resolved ) for child , ancestry in resolved . children ( ) : if self . skip_child ( child , ancestry ) : continue self . process ( data , child , history [ : ] )'], ['get whether or not to skip the specified child', 'def skip_child ( self , child , ancestry ) : if child . any ( ) : return True for x in ancestry : if x . choice ( ) : return True return False'], ['Checks whether knocks are enabled for the model given as argument', \"def active_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : return True return _thread_locals . knock_enabled . get ( obj . __class__ , True )\"], ['Context manager to suspend sending knocks for the given model', \"def pause_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : _thread_locals . knock_enabled = { } obj . __class__ . _disconnect ( ) _thread_locals . knock_enabled [ obj . __class__ ] = False yield _thread_locals . knock_enabled [ obj . __class__ ] = True obj . __class__ . _connect ( )\"], ['Loop over the report progress', 'def _loopreport ( self ) : while 1 : eventlet . sleep ( 0.2 ) ac2popenlist = { } for action in self . session . _actions : for popen in action . _popenlist : if popen . poll ( ) is None : lst = ac2popenlist . setdefault ( action . activity , [ ] ) lst . append ( popen ) if not action . _popenlist and action in self . _actionmayfinish : super ( RetoxReporter , self ) . logaction_finish ( action ) self . _actionmayfinish . remove ( action ) self . screen . draw_next_frame ( repeat = False )'], ['Send markdown email', \"def send ( email , subject = None , from_email = None , to_email = None , cc = None , bcc = None , reply_to = None , smtp = None ) : if is_string ( email ) : email = EmailContent ( email ) from_email = sanitize_email_address ( from_email or email . headers . get ( 'from' ) ) to_email = sanitize_email_address ( to_email or email . headers . get ( 'to' ) ) cc = sanitize_email_address ( cc or email . headers . get ( 'cc' ) ) bcc = sanitize_email_address ( bcc or email . headers . get ( 'bcc' ) ) reply_to = sanitize_email_address ( reply_to or email . headers . get ( 'reply-to' ) ) message_args = { 'html' : email . html , 'text' : email . text , 'subject' : ( subject or email . headers . get ( 'subject' , '' ) ) , 'mail_from' : from_email , 'mail_to' : to_email } if cc : message_args [ 'cc' ] = cc if bcc : message_args [ 'bcc' ] = bcc if reply_to : message_args [ 'headers' ] = { 'reply-to' : reply_to } message = emails . Message ( ** message_args ) for filename , data in email . inline_images : message . attach ( filename = filename , content_disposition = 'inline' , data = data ) message . send ( smtp = smtp )\"], ['Process timezone casting and conversion .', 'def _process_tz ( self , dt , naive , tz ) : def _tz ( t ) : if t in ( None , \\'naive\\' ) : return t if t == \\'local\\' : if __debug__ and not localtz : raise ValueError ( \"Requested conversion to local timezone, but `localtz` not installed.\" ) t = localtz if not isinstance ( t , tzinfo ) : if __debug__ and not localtz : raise ValueError ( \"The `pytz` package must be installed to look up timezone: \" + repr ( t ) ) t = get_tz ( t ) if not hasattr ( t , \\'normalize\\' ) and get_tz : t = get_tz ( t . tzname ( dt ) ) return t naive = _tz ( naive ) tz = _tz ( tz ) if not dt . tzinfo and naive : if hasattr ( naive , \\'localize\\' ) : dt = naive . localize ( dt ) else : dt = dt . replace ( tzinfo = naive ) if not tz : return dt if hasattr ( tz , \\'normalize\\' ) : dt = tz . normalize ( dt . astimezone ( tz ) ) elif tz == \\'naive\\' : dt = dt . replace ( tzinfo = None ) else : dt = dt . astimezone ( tz ) return dt'], ['Trigger assignment of default values .', 'def _prepare_defaults ( self ) : for name , field in self . __fields__ . items ( ) : if field . assign : getattr ( self , name )'], ['Convert data coming in from the MongoDB wire driver into a Document instance .', \"def from_mongo ( cls , doc ) : if doc is None : return None if isinstance ( doc , Document ) : return doc if cls . __type_store__ and cls . __type_store__ in doc : cls = load ( doc [ cls . __type_store__ ] , 'marrow.mongo.document' ) instance = cls ( _prepare_defaults = False ) instance . __data__ = doc instance . _prepare_defaults ( ) return instance\"], ['Retrieve and remove a value from the backing store optionally with a default .', 'def pop ( self , name , default = SENTINEL ) : if default is SENTINEL : return self . __data__ . pop ( name ) return self . __data__ . pop ( name , default )'], ['A basic operation operating on a single value .', 'def _op ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _op ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) if other is not None : other = f . transformer . foreign ( other , ( f , self . _document ) ) return Filter ( { self . _name : { operation : other } } )'], ['An iterative operation operating on multiple values . Consumes iterators to construct a concrete list at time of execution .', 'def _iop ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _iop ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) def _t ( o ) : for value in o : yield None if value is None else f . transformer . foreign ( value , ( f , self . _document ) ) other = other if len ( other ) > 1 else other [ 0 ] values = list ( _t ( other ) ) return Filter ( { self . _name : { operation : values } } )']]\n",
            "[['Return either the full or truncated version of a QIIME - formatted taxonomy string .', 'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]'], ['Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .', 'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )'], ['Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .', 'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle'], ['Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .', 'def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table'], ['Parses the unifrac results file into a dictionary', 'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )'], ['Function to parse data from older version of unifrac file obtained from Qiime version 1 . 8 and earlier .', 'def parse_unifrac_v1_8 ( unifrac , file_data ) : for line in file_data : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ - 2 ] . split ( \"\\\\t\" ) [ 1 : ] ] unifrac [ \"varexp\" ] = [ float ( entry ) for entry in file_data [ - 1 ] . split ( \"\\\\t\" ) [ 1 : ] ] return unifrac'], ['Function to parse data from newer version of unifrac file obtained from Qiime version 1 . 9 and later .', 'def parse_unifrac_v1_9 ( unifrac , file_data ) : unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ 0 ] . split ( \"\\\\t\" ) ] unifrac [ \"varexp\" ] = [ float ( entry ) * 100 for entry in file_data [ 3 ] . split ( \"\\\\t\" ) ] for line in file_data [ 8 : ] : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] return unifrac'], ['Determine color - category mapping . If color_column was specified then map the category names to color values . Otherwise use the palettable colors to automatically generate a set of colors for the group values .', 'def color_mapping ( sample_map , header , group_column , color_column = None ) : group_colors = OrderedDict ( ) group_gather = gather_categories ( sample_map , header , [ group_column ] ) if color_column is not None : color_gather = gather_categories ( sample_map , header , [ color_column ] ) for group in group_gather : for color in color_gather : if group_gather [ group ] . sids . intersection ( color_gather [ color ] . sids ) : group_colors [ group ] = color else : bcolors = itertools . cycle ( Set3_12 . hex_colors ) for group in group_gather : group_colors [ group ] = bcolors . next ( ) return group_colors'], ['return reverse completment of read', \"def rev_c ( read ) : rc = [ ] rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } for base in read : rc . extend ( rc_nucs [ base . upper ( ) ] ) return rc [ : : - 1 ]\"], ['randomly shuffle genome', \"def shuffle_genome ( genome , cat , fraction = float ( 100 ) , plot = True , alpha = 0.1 , beta = 100000 , min_length = 1000 , max_length = 200000 ) : header = '>randomized_%s' % ( genome . name ) sequence = list ( '' . join ( [ i [ 1 ] for i in parse_fasta ( genome ) ] ) ) length = len ( sequence ) shuffled = [ ] while sequence is not False : s = int ( random . gammavariate ( alpha , beta ) ) if s <= min_length or s >= max_length : continue if len ( sequence ) < s : seq = sequence [ 0 : ] else : seq = sequence [ 0 : s ] sequence = sequence [ s : ] shuffled . append ( '' . join ( seq ) ) if sequence == [ ] : break random . shuffle ( shuffled ) if fraction == float ( 100 ) : subset = shuffled else : max_pieces = int ( length * fraction / 100 ) subset , total = [ ] , 0 for fragment in shuffled : length = len ( fragment ) if total + length <= max_pieces : subset . append ( fragment ) total += length else : diff = max_pieces - total subset . append ( fragment [ 0 : diff ] ) break if cat is True : yield [ header , '' . join ( subset ) ] else : for i , seq in enumerate ( subset ) : yield [ '%s fragment:%s' % ( header , i ) , seq ]\"], ['If the fit contains statistically insignificant parameters remove them . Returns a pruned fit where all parameters have p - values of the t - statistic below p_max', \"def _prune ( self , fit , p_max ) : def remove_from_model_desc ( x , model_desc ) : rhs_termlist = [ ] for t in model_desc . rhs_termlist : if not t . factors : rhs_termlist . append ( t ) elif not x == t . factors [ 0 ] . _varname : rhs_termlist . append ( t ) md = ModelDesc ( model_desc . lhs_termlist , rhs_termlist ) return md corrected_model_desc = ModelDesc ( fit . model . formula . lhs_termlist [ : ] , fit . model . formula . rhs_termlist [ : ] ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass while pars_to_prune : corrected_model_desc = remove_from_model_desc ( pars_to_prune [ 0 ] , corrected_model_desc ) fit = fm . ols ( corrected_model_desc , data = self . df ) . fit ( ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass return fit\"], ['Return the best fit based on rsquared', 'def find_best_rsquared ( list_of_fits ) : res = sorted ( list_of_fits , key = lambda x : x . rsquared ) return res [ - 1 ]'], ['Return a df with predictions and confidence interval', \"def _predict ( self , fit , df ) : df_res = df . copy ( ) if 'Intercept' in fit . model . exog_names : df_res [ 'Intercept' ] = 1.0 df_res [ 'predicted' ] = fit . predict ( df_res ) if not self . allow_negative_predictions : df_res . loc [ df_res [ 'predicted' ] < 0 , 'predicted' ] = 0 prstd , interval_l , interval_u = wls_prediction_std ( fit , df_res [ fit . model . exog_names ] , alpha = 1 - self . confint ) df_res [ 'interval_l' ] = interval_l df_res [ 'interval_u' ] = interval_u if 'Intercept' in df_res : df_res . drop ( labels = [ 'Intercept' ] , axis = 1 , inplace = True ) return df_res\"], ['Calculate the relative abundance of each OTUID in a Sample .', 'def relative_abundance ( biomf , sampleIDs = None ) : if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating relative abundances: The sampleIDs provided do\" \" not match the sampleIDs in biom file. Please double check the sampleIDs\" \" provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) norm_biomf = biomf . norm ( inplace = False ) return { sample : { otuID : norm_biomf . get_value_by_ids ( otuID , sample ) for otuID in otuIDs } for sample in sampleIDs }'], ['Calculate the mean OTU abundance percentage .', 'def mean_otu_pct_abundance ( ra , otuIDs ) : sids = ra . keys ( ) otumeans = defaultdict ( int ) for oid in otuIDs : otumeans [ oid ] = sum ( [ ra [ sid ] [ oid ] for sid in sids if oid in ra [ sid ] ] ) / len ( sids ) * 100 return otumeans'], ['Calculate the mean relative abundance percentage .', 'def MRA ( biomf , sampleIDs = None , transform = None ) : ra = relative_abundance ( biomf , sampleIDs ) if transform is not None : ra = { sample : { otuID : transform ( abd ) for otuID , abd in ra [ sample ] . items ( ) } for sample in ra . keys ( ) } otuIDs = biomf . ids ( axis = \"observation\" ) return mean_otu_pct_abundance ( ra , otuIDs )'], ['Calculate the total number of sequences in each OTU or SampleID .', 'def raw_abundance ( biomf , sampleIDs = None , sample_abd = True ) : results = defaultdict ( int ) if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating raw total abundances: The sampleIDs provided \" \"do not match the sampleIDs in biom file. Please double check the \" \"sampleIDs provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) for sampleID in sampleIDs : for otuID in otuIDs : abd = biomf . get_value_by_ids ( otuID , sampleID ) if sample_abd : results [ sampleID ] += abd else : results [ otuID ] += abd return results'], ['Function to transform the total abundance calculation for each sample ID to another format based on user given transformation function .', 'def transform_raw_abundance ( biomf , fn = math . log10 , sampleIDs = None , sample_abd = True ) : totals = raw_abundance ( biomf , sampleIDs , sample_abd ) return { sid : fn ( abd ) for sid , abd in totals . items ( ) }'], ['Compute the Mann - Whitney U test for unequal group sample sizes .', 'def print_MannWhitneyU ( div_calc ) : try : x = div_calc . values ( ) [ 0 ] . values ( ) y = div_calc . values ( ) [ 1 ] . values ( ) except : return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \" \"significance testing.\" T , p = stats . mannwhitneyu ( x , y ) print \"\\\\nMann-Whitney U test statistic:\" , T print \"Two-tailed p-value: {}\" . format ( 2 * p )'], ['Compute the Kruskal - Wallis H - test for independent samples . A typical rule is that each group must have at least 5 measurements .', 'def print_KruskalWallisH ( div_calc ) : calc = defaultdict ( list ) try : for k1 , v1 in div_calc . iteritems ( ) : for k2 , v2 in v1 . iteritems ( ) : calc [ k1 ] . append ( v2 ) except : return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \" \"significance testing.\" h , p = stats . kruskal ( * calc . values ( ) ) print \"\\\\nKruskal-Wallis H-test statistic for {} groups: {}\" . format ( str ( len ( div_calc ) ) , h ) print \"p-value: {}\" . format ( p )'], ['Parses the given options passed in at the command line .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Calculate the alpha diversity\\\\                                     of a set of samples using one or more \\\\                                     metrics and output a kernal density \\\\                                     estimator-smoothed histogram of the \\\\                                     results.\" ) parser . add_argument ( \"-m\" , \"--map_file\" , help = \"QIIME mapping file.\" ) parser . add_argument ( \"-i\" , \"--biom_fp\" , help = \"Path to the BIOM table\" ) parser . add_argument ( \"-c\" , \"--category\" , help = \"Specific category from the mapping file.\" ) parser . add_argument ( \"-d\" , \"--diversity\" , default = [ \"shannon\" ] , nargs = \"+\" , help = \"The alpha diversity metric. Default \\\\                             value is \\'shannon\\', which will calculate the Shannon\\\\                             entropy. Multiple metrics can be specified (space separated).\\\\                             The full list of metrics is available at:\\\\                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\\                             Beta diversity metrics will be supported in the future.\" ) parser . add_argument ( \"--x_label\" , default = [ None ] , nargs = \"+\" , help = \"The name of the diversity metric to be displayed on the\\\\                        plot as the X-axis label. If multiple metrics are specified,\\\\                        then multiple entries for the X-axis label should be given.\" ) parser . add_argument ( \"--color_by\" , help = \"A column name in the mapping file containing\\\\                              hexadecimal (#FF0000) color values that will\\\\                              be used to color the groups. Each sample ID must\\\\                              have a color entry.\" ) parser . add_argument ( \"--plot_title\" , default = \"\" , help = \"A descriptive title that will appear at the top \\\\                        of the output plot. Surround with quotes if there are\\\\                        spaces in the title.\" ) parser . add_argument ( \"-o\" , \"--output_dir\" , default = \".\" , help = \"The directory plots will be saved to.\" ) parser . add_argument ( \"--image_type\" , default = \"png\" , help = \"The type of image to save: png, svg, pdf, eps, etc...\" ) parser . add_argument ( \"--save_calculations\" , help = \"Path and name of text file to store the calculated \" \"diversity metrics.\" ) parser . add_argument ( \"--suppress_stats\" , action = \"store_true\" , help = \"Do not display \" \"significance testing results which are shown by default.\" ) parser . add_argument ( \"--show_available_metrics\" , action = \"store_true\" , help = \"Supply this parameter to see which alpha diversity metrics \" \" are available for usage. No calculations will be performed\" \" if this parameter is provided.\" ) return parser . parse_args ( )'], ['make blast db', \"def blastdb ( fasta , maxfile = 10000000 ) : db = fasta . rsplit ( '.' , 1 ) [ 0 ] type = check_type ( fasta ) if type == 'nucl' : type = [ 'nhr' , type ] else : type = [ 'phr' , type ] if os . path . exists ( '%s.%s' % ( db , type [ 0 ] ) ) is False and os . path . exists ( '%s.00.%s' % ( db , type [ 0 ] ) ) is False : print ( '# ... making blastdb for: %s' % ( fasta ) , file = sys . stderr ) os . system ( 'makeblastdb \\\\                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' % ( fasta , db , type [ 1 ] , maxfile ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['make usearch db', \"def usearchdb ( fasta , alignment = 'local' , usearch_loc = 'usearch' ) : if '.udb' in fasta : print ( '# ... database found: %s' % ( fasta ) , file = sys . stderr ) return fasta type = check_type ( fasta ) db = '%s.%s.udb' % ( fasta . rsplit ( '.' , 1 ) [ 0 ] , type ) if os . path . exists ( db ) is False : print ( '# ... making usearch db for: %s' % ( fasta ) , file = sys . stderr ) if alignment == 'local' : os . system ( '%s -makeudb_ublast %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) elif alignment == 'global' : os . system ( '%s -makeudb_usearch %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['Pretty print .', \"def _pp ( dict_data ) : for key , val in dict_data . items ( ) : print ( '{0:<11}: {1}' . format ( key , val ) )\"], ['Print licenses .', \"def print_licences ( params , metadata ) : if hasattr ( params , 'licenses' ) : if params . licenses : _pp ( metadata . licenses_desc ( ) ) sys . exit ( 0 )\"], ['Check repository existence .', 'def check_repository_existence ( params ) : repodir = os . path . join ( params . outdir , params . name ) if os . path . isdir ( repodir ) : raise Conflict ( \\'Package repository \"{0}\" has already exists.\\' . format ( repodir ) )'], ['Generate package repository .', 'def generate_package ( params ) : pkg_data = package . PackageData ( params ) pkg_tree = package . PackageTree ( pkg_data ) pkg_tree . generate ( ) pkg_tree . move ( ) VCS ( os . path . join ( pkg_tree . outdir , pkg_tree . name ) , pkg_tree . pkg_data )'], ['print single reads to stderr', \"def print_single ( line , rev ) : if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] fq = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] print ( '\\\\n' . join ( fq ) , file = sys . stderr )\"], ['convert sam to fastq', \"def sam2fastq ( sam , singles = False , force = False ) : L , R = None , None for line in sam : if line . startswith ( '@' ) is True : continue line = line . strip ( ) . split ( ) bit = [ True if i == '1' else False for i in bin ( int ( line [ 1 ] ) ) . split ( 'b' ) [ 1 ] [ : : - 1 ] ] while len ( bit ) < 8 : bit . append ( False ) pair , proper , na , nap , rev , mrev , left , right = bit if pair is False : if singles is True : print_single ( line , rev ) continue if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] if left is True : if L is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if L is not None : L = None continue L = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if R is not None : yield L yield R L , R = None , None if right is True : if R is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if R is not None : R = None continue R = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if L is not None : yield L yield R L , R = None , None\"], ['sort sam file', 'def sort_sam ( sam , sort ) : tempdir = \\'%s/\\' % ( os . path . abspath ( sam ) . rsplit ( \\'/\\' , 1 ) [ 0 ] ) if sort is True : mapping = \\'%s.sorted.sam\\' % ( sam . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if sam != \\'-\\' : if os . path . exists ( mapping ) is False : os . system ( \"\\\\                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\\                    \" % ( sbuffer , tempdir , mapping , sam ) ) else : mapping = \\'stdin-sam.sorted.sam\\' p = Popen ( \"sort -k1 --buffer-size=%sG -T %s -o %s\" % ( sbuffer , tempdir , mapping ) , stdin = sys . stdin , shell = True ) p . communicate ( ) mapping = open ( mapping ) else : if sam == \\'-\\' : mapping = sys . stdin else : mapping = open ( sam ) return mapping'], ['randomly subset sam file', \"def sub_sam ( sam , percent , sort = True , sbuffer = False ) : mapping = sort_sam ( sam , sort ) pool = [ 1 for i in range ( 0 , percent ) ] + [ 0 for i in range ( 0 , 100 - percent ) ] c = cycle ( [ 1 , 2 ] ) for line in mapping : line = line . strip ( ) . split ( ) if line [ 0 ] . startswith ( '@' ) : yield line continue if int ( line [ 1 ] ) <= 20 : if random . choice ( pool ) == 1 : yield line else : n = next ( c ) if n == 1 : prev = line if n == 2 and random . choice ( pool ) == 1 : yield prev yield line\"], ['convert fq to fa', \"def fq2fa ( fq ) : c = cycle ( [ 1 , 2 , 3 , 4 ] ) for line in fq : n = next ( c ) if n == 1 : seq = [ '>%s' % ( line . strip ( ) . split ( '@' , 1 ) [ 1 ] ) ] if n == 2 : seq . append ( line . strip ( ) ) yield seq\"], ['Converts the returned value of wrapped function to the type of the first arg or to the type specified by a kwarg key return_type s value .', \"def change_return_type ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) elif len ( args ) > 0 : return_type = type ( args [ 0 ] ) return return_type ( f ( * args , ** kwargs ) ) else : return f ( * args , ** kwargs ) return wrapper\"], ['Converts all args to set type via self . setify function .', 'def convert_args_to_sets ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : args = ( setify ( x ) for x in args ) return f ( * args , ** kwargs ) return wrapper'], ['Membuat objek - objek entri dari laman yang diambil .', \"def _init_entri ( self , laman ) : sup = BeautifulSoup ( laman . text , 'html.parser' ) estr = '' for label in sup . find ( 'hr' ) . next_siblings : if label . name == 'hr' : self . entri . append ( Entri ( estr ) ) break if label . name == 'h2' : if estr : self . entri . append ( Entri ( estr ) ) estr = '' estr += str ( label ) . strip ( )\"], ['Memproses kata dasar yang ada dalam nama entri .', \"def _init_kata_dasar ( self , dasar ) : for tiap in dasar : kata = tiap . find ( 'a' ) dasar_no = kata . find ( 'sup' ) kata = ambil_teks_dalam_label ( kata ) self . kata_dasar . append ( kata + ' [{}]' . format ( dasar_no . text . strip ( ) ) if dasar_no else kata )\"], ['Mengembalikan hasil serialisasi objek Entri ini .', 'def serialisasi ( self ) : return { \"nama\" : self . nama , \"nomor\" : self . nomor , \"kata_dasar\" : self . kata_dasar , \"pelafalan\" : self . pelafalan , \"bentuk_tidak_baku\" : self . bentuk_tidak_baku , \"varian\" : self . varian , \"makna\" : [ makna . serialisasi ( ) for makna in self . makna ] }'], ['Mengembalikan representasi string untuk semua makna entri ini .', 'def _makna ( self ) : if len ( self . makna ) > 1 : return \\'\\\\n\\' . join ( str ( i ) + \". \" + str ( makna ) for i , makna in enumerate ( self . makna , 1 ) ) return str ( self . makna [ 0 ] )'], ['Mengembalikan representasi string untuk nama entri ini .', 'def _nama ( self ) : hasil = self . nama if self . nomor : hasil += \" [{}]\" . format ( self . nomor ) if self . kata_dasar : hasil = \" » \". j oin( s elf. k ata_dasar)      » \" + h sil return hasil'], ['Mengembalikan representasi string untuk varian entri ini . Dapat digunakan untuk Varian maupun Bentuk tidak baku .', 'def _varian ( self , varian ) : if varian == self . bentuk_tidak_baku : nama = \"Bentuk tidak baku\" elif varian == self . varian : nama = \"Varian\" else : return \\'\\' return nama + \\': \\' + \\', \\' . join ( varian )'], ['Memproses kelas kata yang ada dalam makna .', \"def _init_kelas ( self , makna_label ) : kelas = makna_label . find ( color = 'red' ) lain = makna_label . find ( color = 'darkgreen' ) info = makna_label . find ( color = 'green' ) if kelas : kelas = kelas . find_all ( 'span' ) if lain : self . kelas = { lain . text . strip ( ) : lain [ 'title' ] . strip ( ) } self . submakna = lain . next_sibling . strip ( ) self . submakna += ' ' + makna_label . find ( color = 'grey' ) . text . strip ( ) else : self . kelas = { k . text . strip ( ) : k [ 'title' ] . strip ( ) for k in kelas } if kelas else { } self . info = info . text . strip ( ) if info else ''\"], ['Memproses contoh yang ada dalam makna .', \"def _init_contoh ( self , makna_label ) : indeks = makna_label . text . find ( ': ' ) if indeks != - 1 : contoh = makna_label . text [ indeks + 2 : ] . strip ( ) self . contoh = contoh . split ( '; ' ) else : self . contoh = [ ]\"], ['Mengembalikan hasil serialisasi objek Makna ini .', 'def serialisasi ( self ) : return { \"kelas\" : self . kelas , \"submakna\" : self . submakna , \"info\" : self . info , \"contoh\" : self . contoh }'], ['Build sphinx documentation .', 'def build_sphinx ( pkg_data , projectdir ) : try : version , _minor_version = pkg_data . version . rsplit ( \\'.\\' , 1 ) except ValueError : version = pkg_data . version args = \\' \\' . join ( ( \\'sphinx-quickstart\\' , \\'--sep\\' , \\'-q\\' , \\'-p \"{name}\"\\' , \\'-a \"{author}\"\\' , \\'-v \"{version}\"\\' , \\'-r \"{release}\"\\' , \\'-l en\\' , \\'--suffix=.rst\\' , \\'--master=index\\' , \\'--ext-autodoc\\' , \\'--ext-viewcode\\' , \\'--makefile\\' , \\'{projectdir}\\' ) ) . format ( name = pkg_data . name , author = pkg_data . author , version = version , release = pkg_data . version , projectdir = projectdir ) if subprocess . call ( shlex . split ( args ) ) == 0 : _touch_gitkeep ( projectdir )'], ['make bowtie db', \"def bowtiedb ( fa , keepDB ) : btdir = '%s/bt2' % ( os . getcwd ( ) ) if not os . path . exists ( btdir ) : os . mkdir ( btdir ) btdb = '%s/%s' % ( btdir , fa . rsplit ( '/' , 1 ) [ - 1 ] ) if keepDB is True : if os . path . exists ( '%s.1.bt2' % ( btdb ) ) : return btdb p = subprocess . Popen ( 'bowtie2-build -q %s %s' % ( fa , btdb ) , shell = True ) p . communicate ( ) return btdb\"], ['generate bowtie2 command', \"def bowtie ( sam , btd , f , r , u , opt , no_shrink , threads ) : bt2 = 'bowtie2 -x %s -p %s ' % ( btd , threads ) if f is not False : bt2 += '-1 %s -2 %s ' % ( f , r ) if u is not False : bt2 += '-U %s ' % ( u ) bt2 += opt if no_shrink is False : if f is False : bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' | shrinksam -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' > %s.sam' % ( sam ) return bt2\"], ['map all read sets against all fasta files', \"def crossmap ( fas , reads , options , no_shrink , keepDB , threads , cluster , nodes ) : if cluster is True : threads = '48' btc = [ ] for fa in fas : btd = bowtiedb ( fa , keepDB ) F , R , U = reads if F is not False : if U is False : u = False for i , f in enumerate ( F ) : r = R [ i ] if U is not False : u = U [ i ] sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , f . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) else : f = False r = False for u in U : sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , u . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) if cluster is False : for i in btc : p = subprocess . Popen ( i , shell = True ) p . communicate ( ) else : ID = '' . join ( random . choice ( [ str ( i ) for i in range ( 0 , 9 ) ] ) for _ in range ( 5 ) ) for node , commands in enumerate ( chunks ( btc , nodes ) , 1 ) : bs = open ( '%s/crossmap-qsub.%s.%s.sh' % ( os . getcwd ( ) , ID , node ) , 'w' ) print ( '\\\\n' . join ( commands ) , file = bs ) bs . close ( ) p = subprocess . Popen ( 'qsub -V -N crossmap %s' % ( bs . name ) , shell = True ) p . communicate ( )\"], ['Returns a connection object from the router given args .', \"def get_conn ( self , * args , ** kwargs ) : connections = self . __connections_for ( 'get_conn' , args = args , kwargs = kwargs ) if len ( connections ) is 1 : return connections [ 0 ] else : return connections\"], ['return the non - direct init if the direct algorithm has been selected .', 'def __get_nondirect_init ( self , init ) : crc = init for i in range ( self . Width ) : bit = crc & 0x01 if bit : crc ^= self . Poly crc >>= 1 if bit : crc |= self . MSB_Mask return crc & self . Mask'], ['reflect a data word i . e . reverts the bit order .', 'def reflect ( self , data , width ) : x = data & 0x01 for i in range ( width - 1 ) : data >>= 1 x = ( x << 1 ) | ( data & 0x01 ) return x'], ['Classic simple and slow CRC implementation . This function iterates bit by bit over the augmented input message and returns the calculated CRC value at the end .', 'def bit_by_bit ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] register = self . NonDirectInit for octet in in_data : if self . ReflectIn : octet = self . reflect ( octet , 8 ) for i in range ( 8 ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) | ( ( octet >> ( 7 - i ) ) & 0x01 ) if topbit : register ^= self . Poly for i in range ( self . Width ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) if topbit : register ^= self . Poly if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['This function generates the CRC table used for the table_driven CRC algorithm . The Python version cannot handle tables of an index width other than 8 . See the generated C code for tables with different sizes instead .', 'def gen_table ( self ) : table_length = 1 << self . TableIdxWidth tbl = [ 0 ] * table_length for i in range ( table_length ) : register = i if self . ReflectIn : register = self . reflect ( register , self . TableIdxWidth ) register = register << ( self . Width - self . TableIdxWidth + self . CrcShift ) for j in range ( self . TableIdxWidth ) : if register & ( self . MSB_Mask << self . CrcShift ) != 0 : register = ( register << 1 ) ^ ( self . Poly << self . CrcShift ) else : register = ( register << 1 ) if self . ReflectIn : register = self . reflect ( register >> self . CrcShift , self . Width ) << self . CrcShift tbl [ i ] = register & ( self . Mask << self . CrcShift ) return tbl'], ['The Standard table_driven CRC algorithm .', 'def table_driven ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] tbl = self . gen_table ( ) register = self . DirectInit << self . CrcShift if not self . ReflectIn : for octet in in_data : tblidx = ( ( register >> ( self . Width - self . TableIdxWidth + self . CrcShift ) ) ^ octet ) & 0xff register = ( ( register << ( self . TableIdxWidth - self . CrcShift ) ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = register >> self . CrcShift else : register = self . reflect ( register , self . Width + self . CrcShift ) << self . CrcShift for octet in in_data : tblidx = ( ( register >> self . CrcShift ) ^ octet ) & 0xff register = ( ( register >> self . TableIdxWidth ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = self . reflect ( register , self . Width + self . CrcShift ) & self . Mask if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['parse masked sequence into non - masked and masked regions', 'def parse_masked ( seq , min_len ) : nm , masked = [ ] , [ [ ] ] prev = None for base in seq [ 1 ] : if base . isupper ( ) : nm . append ( base ) if masked != [ [ ] ] and len ( masked [ - 1 ] ) < min_len : nm . extend ( masked [ - 1 ] ) del masked [ - 1 ] prev = False elif base . islower ( ) : if prev is False : masked . append ( [ ] ) masked [ - 1 ] . append ( base ) prev = True return nm , masked'], ['remove masked regions from fasta file as long as they are longer than min_len', \"def strip_masked ( fasta , min_len , print_masked ) : for seq in parse_fasta ( fasta ) : nm , masked = parse_masked ( seq , min_len ) nm = [ '%s removed_masked >=%s' % ( seq [ 0 ] , min_len ) , '' . join ( nm ) ] yield [ 0 , nm ] if print_masked is True : for i , m in enumerate ( [ i for i in masked if i != [ ] ] , 1 ) : m = [ '%s insertion:%s' % ( seq [ 0 ] , i ) , '' . join ( m ) ] yield [ 1 , m ]\"], ['Return arcsine transformed relative abundance from a BIOM format file .', 'def get_relative_abundance ( biomfile ) : biomf = biom . load_table ( biomfile ) norm_biomf = biomf . norm ( inplace = False ) rel_abd = { } for sid in norm_biomf . ids ( ) : rel_abd [ sid ] = { } for otuid in norm_biomf . ids ( \"observation\" ) : otuname = oc . otu_name ( norm_biomf . metadata ( otuid , axis = \"observation\" ) [ \"taxonomy\" ] ) otuname = \" \" . join ( otuname . split ( \"_\" ) ) abd = norm_biomf . get_value_by_ids ( otuid , sid ) rel_abd [ sid ] [ otuname ] = abd ast_rel_abd = bc . arcsine_sqrt_transform ( rel_abd ) return ast_rel_abd'], ['Find an OTU ID in a Newick - format tree . Return the starting position of the ID or None if not found .', 'def find_otu ( otuid , tree ) : for m in re . finditer ( otuid , tree ) : before , after = tree [ m . start ( ) - 1 ] , tree [ m . start ( ) + len ( otuid ) ] if before in [ \"(\" , \",\" , \")\" ] and after in [ \":\" , \";\" ] : return m . start ( ) return None'], ['Replace the OTU ids in the Newick phylogenetic tree format with truncated OTU names', 'def newick_replace_otuids ( tree , biomf ) : for val , id_ , md in biomf . iter ( axis = \"observation\" ) : otu_loc = find_otu ( id_ , tree ) if otu_loc is not None : tree = tree [ : otu_loc ] + oc . otu_name ( md [ \"taxonomy\" ] ) + tree [ otu_loc + len ( id_ ) : ] return tree'], ['return genome info for choosing representative', \"def genome_info ( genome , info ) : try : scg = info [ '#SCGs' ] dups = info [ '#SCG duplicates' ] length = info [ 'genome size (bp)' ] return [ scg - dups , length , genome ] except : return [ False , False , info [ 'genome size (bp)' ] , genome ]\"], ['choose represenative genome and print cluster information', \"def print_clusters ( fastas , info , ANI ) : header = [ '#cluster' , 'num. genomes' , 'rep.' , 'genome' , '#SCGs' , '#SCG duplicates' , 'genome size (bp)' , 'fragments' , 'list' ] yield header in_cluster = [ ] for cluster_num , cluster in enumerate ( connected_components ( ANI ) ) : cluster = sorted ( [ genome_info ( genome , info [ genome ] ) for genome in cluster ] , key = lambda x : x [ 0 : ] , reverse = True ) rep = cluster [ 0 ] [ - 1 ] cluster = [ i [ - 1 ] for i in cluster ] size = len ( cluster ) for genome in cluster : in_cluster . append ( genome ) try : stats = [ size , rep , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] except : stats = [ size , rep , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] if rep == genome : stats = [ '*%s' % ( cluster_num ) ] + stats else : stats = [ cluster_num ] + stats yield stats try : start = cluster_num + 1 except : start = 0 fastas = set ( [ i . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] for i in fastas ] ) for cluster_num , genome in enumerate ( fastas . difference ( set ( in_cluster ) ) , start ) : try : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] except : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] yield stats\"], ['convert ggKbase genome info tables to dictionary', \"def parse_ggKbase_tables ( tables , id_type ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'name' ) : header = line header [ 4 ] = 'genome size (bp)' header [ 12 ] = '#SCGs' header [ 13 ] = '#SCG duplicates' continue name , code , info = line [ 0 ] , line [ 1 ] , line info = [ to_int ( i ) for i in info ] if id_type is False : if 'UNK' in code or 'unknown' in code : code = name if ( name != code ) and ( name and code in g2info ) : print ( '# duplicate name or code in table(s)' , file = sys . stderr ) print ( '# %s and/or %s' % ( name , code ) , file = sys . stderr ) exit ( ) if name not in g2info : g2info [ name ] = { item : stat for item , stat in zip ( header , info ) } if code not in g2info : g2info [ code ] = { item : stat for item , stat in zip ( header , info ) } else : if id_type == 'name' : ID = name elif id_type == 'code' : ID = code else : print ( '# specify name or code column using -id' , file = sys . stderr ) exit ( ) ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['convert checkM genome info tables to dictionary', \"def parse_checkM_tables ( tables ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'Bin Id' ) : header = line header [ 8 ] = 'genome size (bp)' header [ 5 ] = '#SCGs' header [ 6 ] = '#SCG duplicates' continue ID , info = line [ 0 ] , line info = [ to_int ( i ) for i in info ] ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['get genome lengths', \"def genome_lengths ( fastas , info ) : if info is False : info = { } for genome in fastas : name = genome . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] if name in info : continue length = 0 fragments = 0 for seq in parse_fasta ( genome ) : length += len ( seq [ 1 ] ) fragments += 1 info [ name ] = { 'genome size (bp)' : length , '# contigs' : fragments } return info\"], ['Returns a list of db keys to route the given call to .', 'def get_dbs ( self , attr , args , kwargs , ** fkwargs ) : if not self . _ready : if not self . setup_router ( args = args , kwargs = kwargs , ** fkwargs ) : raise self . UnableToSetupRouter ( ) retval = self . _pre_routing ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) if retval is not None : args , kwargs = retval if not ( args or kwargs ) : return self . cluster . hosts . keys ( ) try : db_nums = self . _route ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) except Exception as e : self . _handle_exception ( e ) db_nums = [ ] return self . _post_routing ( attr = attr , db_nums = db_nums , args = args , kwargs = kwargs , ** fkwargs )'], ['Call method to perform any setup', 'def setup_router ( self , args , kwargs , ** fkwargs ) : self . _ready = self . _setup_router ( args = args , kwargs = kwargs , ** fkwargs ) return self . _ready'], ['Perform routing and return db_nums', 'def _route ( self , attr , args , kwargs , ** fkwargs ) : return self . cluster . hosts . keys ( )'], ['Iterates through all connections which were previously listed as unavailable and marks any that have expired their retry_timeout as being up .', 'def check_down_connections ( self ) : now = time . time ( ) for db_num , marked_down_at in self . _down_connections . items ( ) : if marked_down_at + self . retry_timeout <= now : self . mark_connection_up ( db_num )'], ['Marks all connections which were previously listed as unavailable as being up .', 'def flush_down_connections ( self ) : self . _get_db_attempts = 0 for db_num in self . _down_connections . keys ( ) : self . mark_connection_up ( db_num )'], ['Compute standby power', \"def standby ( df , resolution = '24h' , time_window = None ) : if df . empty : raise EmptyDataFrame ( ) df = pd . DataFrame ( df ) def parse_time ( t ) : if isinstance ( t , numbers . Number ) : return pd . Timestamp . utcfromtimestamp ( t ) . time ( ) else : return pd . Timestamp ( t ) . time ( ) if time_window is not None : t_start = parse_time ( time_window [ 0 ] ) t_end = parse_time ( time_window [ 1 ] ) if t_start > t_end : df = df [ ( df . index . time >= t_start ) | ( df . index . time < t_end ) ] else : df = df [ ( df . index . time >= t_start ) & ( df . index . time < t_end ) ] return df . resample ( resolution ) . min ( )\"], ['Compute the share of the standby power in the total consumption .', \"def share_of_standby ( df , resolution = '24h' , time_window = None ) : p_sb = standby ( df , resolution , time_window ) df = df . resample ( resolution ) . mean ( ) p_tot = df . sum ( ) p_standby = p_sb . sum ( ) share_standby = p_standby / p_tot res = share_standby . iloc [ 0 ] return res\"], ['Toggle counter for gas boilers', 'def count_peaks ( ts ) : on_toggles = ts . diff ( ) > 3000 shifted = np . logical_not ( on_toggles . shift ( 1 ) ) result = on_toggles & shifted count = result . sum ( ) return count'], ['Calculate the ratio of input vs . norm over a given interval .', 'def load_factor ( ts , resolution = None , norm = None ) : if norm is None : norm = ts . max ( ) if resolution is not None : ts = ts . resample ( rule = resolution ) . mean ( ) lf = ts / norm return lf'], ['get top hits after sorting by column number', 'def top_hits ( hits , num , column , reverse ) : hits . sort ( key = itemgetter ( column ) , reverse = reverse ) for hit in hits [ 0 : num ] : yield hit'], ['parse b6 output with sorting', \"def numBlast_sort ( blast , numHits , evalueT , bitT ) : header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header hmm = { h : [ ] for h in header } for line in blast : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( '\\\\t' ) line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if evalueT is not False and evalue > evalueT : continue if bitT is not False and bit < bitT : continue for i , h in zip ( line , header ) : hmm [ h ] . append ( i ) hmm = pd . DataFrame ( hmm ) for query , df in hmm . groupby ( by = [ '#query' ] ) : df = df . sort_values ( by = [ 'bitscore' ] , ascending = False ) for hit in df [ header ] . values [ 0 : numHits ] : yield hit\"], ['parse b6 output', \"def numBlast ( blast , numHits , evalueT = False , bitT = False , sort = False ) : if sort is True : for hit in numBlast_sort ( blast , numHits , evalueT , bitT ) : yield hit return header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header prev , hits = None , [ ] for line in blast : line = line . strip ( ) . split ( '\\\\t' ) ID = line [ 0 ] line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 11 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 11 , True ) : yield hit\"], ['parse hmm domain table output this version is faster but does not work unless the table is sorted', \"def numDomtblout ( domtblout , numHits , evalueT , bitT , sort ) : if sort is True : for hit in numDomtblout_sort ( domtblout , numHits , evalueT , bitT ) : yield hit return header = [ '#target name' , 'target accession' , 'tlen' , 'query name' , 'query accession' , 'qlen' , 'full E-value' , 'full score' , 'full bias' , 'domain #' , '# domains' , 'domain c-Evalue' , 'domain i-Evalue' , 'domain score' , 'domain bias' , 'hmm from' , 'hmm to' , 'seq from' , 'seq to' , 'env from' , 'env to' , 'acc' , 'target description' ] yield header prev , hits = None , [ ] for line in domtblout : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( ) desc = ' ' . join ( line [ 18 : ] ) line = line [ 0 : 18 ] line . append ( desc ) ID = line [ 0 ] + line [ 9 ] line [ 11 ] , line [ 13 ] = float ( line [ 11 ] ) , float ( line [ 13 ] ) evalue , bitscore = line [ 11 ] , line [ 13 ] line [ 11 ] , line [ 13 ] = evalue , bitscore if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 13 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 13 , True ) : yield hit\"], ['convert stockholm to fasta', \"def stock2fa ( stock ) : seqs = { } for line in stock : if line . startswith ( '#' ) is False and line . startswith ( ' ' ) is False and len ( line ) > 3 : id , seq = line . strip ( ) . split ( ) id = id . rsplit ( '/' , 1 ) [ 0 ] id = re . split ( '[0-9]\\\\|' , id , 1 ) [ - 1 ] if id not in seqs : seqs [ id ] = [ ] seqs [ id ] . append ( seq ) if line . startswith ( '//' ) : break return seqs\"], ['Return boolean time series following given week schedule .', \"def week_schedule ( index , on_time = None , off_time = None , off_days = None ) : if on_time is None : on_time = '9:00' if off_time is None : off_time = '17:00' if off_days is None : off_days = [ 'Sunday' , 'Monday' ] if not isinstance ( on_time , datetime . time ) : on_time = pd . to_datetime ( on_time , format = '%H:%M' ) . time ( ) if not isinstance ( off_time , datetime . time ) : off_time = pd . to_datetime ( off_time , format = '%H:%M' ) . time ( ) times = ( index . time >= on_time ) & ( index . time < off_time ) & ( ~ index . weekday_name . isin ( off_days ) ) return pd . Series ( times , index = index )\"], ['Draw a carpet plot of a pandas timeseries .', 'def carpet ( timeseries , ** kwargs ) : cmap = kwargs . pop ( \\'cmap\\' , cm . coolwarm ) norm = kwargs . pop ( \\'norm\\' , LogNorm ( ) ) interpolation = kwargs . pop ( \\'interpolation\\' , \\'nearest\\' ) cblabel = kwargs . pop ( \\'zlabel\\' , timeseries . name if timeseries . name else \\'\\' ) title = kwargs . pop ( \\'title\\' , \\'carpet plot: \\' + timeseries . name if timeseries . name else \\'\\' ) if timeseries . dropna ( ) . empty : print ( \\'skipped {} - no data\\' . format ( title ) ) return ts = timeseries . resample ( \\'15min\\' ) . interpolate ( ) vmin = max ( 0.1 , kwargs . pop ( \\'vmin\\' , ts [ ts > 0 ] . min ( ) ) ) vmax = max ( vmin , kwargs . pop ( \\'vmax\\' , ts . quantile ( .999 ) ) ) mpldatetimes = date2num ( ts . index . to_pydatetime ( ) ) ts . index = pd . MultiIndex . from_arrays ( [ np . floor ( mpldatetimes ) , 2 + mpldatetimes % 1 ] ) df = ts . unstack ( ) fig , ax = plt . subplots ( ) extent = [ df . columns [ 0 ] , df . columns [ - 1 ] , df . index [ - 1 ] + 0.5 , df . index [ 0 ] - 0.5 ] im = plt . imshow ( df , vmin = vmin , vmax = vmax , extent = extent , cmap = cmap , aspect = \\'auto\\' , norm = norm , interpolation = interpolation , ** kwargs ) ax . xaxis_date ( ) ax . xaxis . set_major_locator ( HourLocator ( interval = 2 ) ) ax . xaxis . set_major_formatter ( DateFormatter ( \\'%H:%M\\' ) ) ax . xaxis . grid ( True ) plt . xlabel ( \\'UTC Time\\' ) ax . yaxis_date ( ) dmin , dmax = ax . yaxis . get_data_interval ( ) number_of_days = ( num2date ( dmax ) - num2date ( dmin ) ) . days if abs ( number_of_days ) <= 35 : ax . yaxis . set_major_locator ( DayLocator ( ) ) else : ax . yaxis . set_major_locator ( AutoDateLocator ( ) ) ax . yaxis . set_major_formatter ( DateFormatter ( \"%a, %d %b %Y\" ) ) cbticks = np . logspace ( np . log10 ( vmin ) , np . log10 ( vmax ) , 11 , endpoint = True ) cb = plt . colorbar ( format = \\'%.0f\\' , ticks = cbticks ) cb . set_label ( cblabel ) plt . title ( title ) return im'], ['calculate percent identity', \"def calc_pident_ignore_gaps ( a , b ) : m = 0 mm = 0 for A , B in zip ( list ( a ) , list ( b ) ) : if A == '-' or A == '.' or B == '-' or B == '.' : continue if A == B : m += 1 else : mm += 1 try : return float ( float ( m ) / float ( ( m + mm ) ) ) * 100 except : return 0\"], ['skip column if either is a gap', \"def remove_gaps ( A , B ) : a_seq , b_seq = [ ] , [ ] for a , b in zip ( list ( A ) , list ( B ) ) : if a == '-' or a == '.' or b == '-' or b == '.' : continue a_seq . append ( a ) b_seq . append ( b ) return '' . join ( a_seq ) , '' . join ( b_seq )\"], ['compare pairs of sequences', \"def compare_seqs ( seqs ) : A , B , ignore_gaps = seqs a , b = A [ 1 ] , B [ 1 ] if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) if ignore_gaps is True : pident = calc_pident_ignore_gaps ( a , b ) else : pident = calc_pident ( a , b ) return A [ 0 ] , B [ 0 ] , pident\"], ['calculate Levenshtein ratio of sequences', \"def compare_seqs_leven ( seqs ) : A , B , ignore_gaps = seqs a , b = remove_gaps ( A [ 1 ] , B [ 1 ] ) if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) pident = lr ( a , b ) * 100 return A [ 0 ] , B [ 0 ] , pident\"], ['make pairwise sequence comparisons between aligned sequences', 'def pairwise_compare ( afa , leven , threads , print_list , ignore_gaps ) : seqs = { seq [ 0 ] : seq for seq in nr_fasta ( [ afa ] , append_index = True ) } num_seqs = len ( seqs ) pairs = ( ( i [ 0 ] , i [ 1 ] , ignore_gaps ) for i in itertools . combinations ( list ( seqs . values ( ) ) , 2 ) ) pool = multithread ( threads ) if leven is True : pident = pool . map ( compare_seqs_leven , pairs ) else : compare = pool . imap_unordered ( compare_seqs , pairs ) pident = [ i for i in tqdm ( compare , total = ( num_seqs * num_seqs ) / 2 ) ] pool . close ( ) pool . terminate ( ) pool . join ( ) return to_dictionary ( pident , print_list )'], ['print matrix of pidents to stdout', \"def print_pairwise ( pw , median = False ) : names = sorted ( set ( [ i for i in pw ] ) ) if len ( names ) != 0 : if '>' in names [ 0 ] : yield [ '#' ] + [ i . split ( '>' ) [ 1 ] for i in names if '>' in i ] else : yield [ '#' ] + names for a in names : if '>' in a : yield [ a . split ( '>' ) [ 1 ] ] + [ pw [ a ] [ b ] for b in names ] else : out = [ ] for b in names : if b in pw [ a ] : if median is False : out . append ( max ( pw [ a ] [ b ] ) ) else : out . append ( np . median ( pw [ a ] [ b ] ) ) else : out . append ( '-' ) yield [ a ] + out\"], ['print stats for comparisons', \"def print_comps ( comps ) : if comps == [ ] : print ( 'n/a' ) else : print ( '# min: %s, max: %s, mean: %s' % ( min ( comps ) , max ( comps ) , np . mean ( comps ) ) )\"], ['print min . pident within each clade and then matrix of between - clade max .', \"def compare_clades ( pw ) : names = sorted ( set ( [ i for i in pw ] ) ) for i in range ( 0 , 4 ) : wi , bt = { } , { } for a in names : for b in pw [ a ] : if ';' not in a or ';' not in b : continue pident = pw [ a ] [ b ] cA , cB = a . split ( ';' ) [ i ] , b . split ( ';' ) [ i ] if i == 0 and '_' in cA and '_' in cB : cA = cA . rsplit ( '_' , 1 ) [ 1 ] cB = cB . rsplit ( '_' , 1 ) [ 1 ] elif '>' in cA or '>' in cB : cA = cA . split ( '>' ) [ 1 ] cB = cB . split ( '>' ) [ 1 ] if cA == cB : if cA not in wi : wi [ cA ] = [ ] wi [ cA ] . append ( pident ) else : if cA not in bt : bt [ cA ] = { } if cB not in bt [ cA ] : bt [ cA ] [ cB ] = [ ] bt [ cA ] [ cB ] . append ( pident ) print ( '\\\\n# min. within' ) for clade , pidents in list ( wi . items ( ) ) : print ( '\\\\t' . join ( [ 'wi:%s' % str ( i ) , clade , str ( min ( pidents ) ) ] ) ) comps = [ ] print ( '\\\\n# max. between' ) for comp in print_pairwise ( bt ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps ) comps = [ ] print ( '\\\\n# median between' ) for comp in print_pairwise ( bt , median = True ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps )\"], ['convert matrix to dictionary of comparisons', \"def matrix2dictionary ( matrix ) : pw = { } for line in matrix : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : names = line [ 1 : ] continue a = line [ 0 ] for i , pident in enumerate ( line [ 1 : ] ) : b = names [ i ] if a not in pw : pw [ a ] = { } if b not in pw : pw [ b ] = { } if pident != '-' : pident = float ( pident ) pw [ a ] [ b ] = pident pw [ b ] [ a ] = pident return pw\"], ['Set argument parser option .', \"def setoption ( parser , metadata = None ) : parser . add_argument ( '-v' , action = 'version' , version = __version__ ) subparsers = parser . add_subparsers ( help = 'sub commands help' ) create_cmd = subparsers . add_parser ( 'create' ) create_cmd . add_argument ( 'name' , help = 'Specify Python package name.' ) create_cmd . add_argument ( '-d' , dest = 'description' , action = 'store' , help = 'Short description about your package.' ) create_cmd . add_argument ( '-a' , dest = 'author' , action = 'store' , required = True , help = 'Python package author name.' ) create_cmd . add_argument ( '-e' , dest = 'email' , action = 'store' , required = True , help = 'Python package author email address.' ) create_cmd . add_argument ( '-l' , dest = 'license' , choices = metadata . licenses ( ) . keys ( ) , default = 'GPLv3+' , help = 'Specify license. (default: %(default)s)' ) create_cmd . add_argument ( '-s' , dest = 'status' , choices = metadata . status ( ) . keys ( ) , default = 'Alpha' , help = ( 'Specify development status. ' '(default: %(default)s)' ) ) create_cmd . add_argument ( '--no-check' , action = 'store_true' , help = 'No checking package name in PyPI.' ) create_cmd . add_argument ( '--with-samples' , action = 'store_true' , help = 'Generate package with sample code.' ) group = create_cmd . add_mutually_exclusive_group ( required = True ) group . add_argument ( '-U' , dest = 'username' , action = 'store' , help = 'Specify GitHub username.' ) group . add_argument ( '-u' , dest = 'url' , action = 'store' , type = valid_url , help = 'Python package homepage url.' ) create_cmd . add_argument ( '-o' , dest = 'outdir' , action = 'store' , default = os . path . abspath ( os . path . curdir ) , help = 'Specify output directory. (default: $PWD)' ) list_cmd = subparsers . add_parser ( 'list' ) list_cmd . add_argument ( '-l' , dest = 'licenses' , action = 'store_true' , help = 'show license choices.' )\"], ['Parse argument options .', \"def parse_options ( metadata ) : parser = argparse . ArgumentParser ( description = '%(prog)s usage:' , prog = __prog__ ) setoption ( parser , metadata = metadata ) return parser\"], ['Execute main processes .', \"def main ( ) : try : pkg_version = Update ( ) if pkg_version . updatable ( ) : pkg_version . show_message ( ) metadata = control . retreive_metadata ( ) parser = parse_options ( metadata ) argvs = sys . argv if len ( argvs ) <= 1 : parser . print_help ( ) sys . exit ( 1 ) args = parser . parse_args ( ) control . print_licences ( args , metadata ) control . check_repository_existence ( args ) control . check_package_existence ( args ) control . generate_package ( args ) except ( RuntimeError , BackendFailure , Conflict ) as exc : sys . stderr . write ( '{0}\\\\n' . format ( exc ) ) sys . exit ( 1 )\"], ['Check key and set default vaule when it does not exists .', \"def _check_or_set_default_params ( self ) : if not hasattr ( self , 'date' ) : self . _set_param ( 'date' , datetime . utcnow ( ) . strftime ( '%Y-%m-%d' ) ) if not hasattr ( self , 'version' ) : self . _set_param ( 'version' , self . default_version ) if not hasattr ( self , 'description' ) or self . description is None : getattr ( self , '_set_param' ) ( 'description' , self . warning_message )\"], ['Move directory from working directory to output directory .', 'def move ( self ) : if not os . path . isdir ( self . outdir ) : os . makedirs ( self . outdir ) shutil . move ( self . tmpdir , os . path . join ( self . outdir , self . name ) )'], ['Initialize VCS repository .', 'def vcs_init ( self ) : VCS ( os . path . join ( self . outdir , self . name ) , self . pkg_data )'], ['Finds the location of the current Steam installation on Windows machines . Returns None for any non - Windows machines or for Windows machines where Steam is not installed .', 'def find_steam_location ( ) : if registry is None : return None key = registry . CreateKey ( registry . HKEY_CURRENT_USER , \"Software\\\\Valve\\\\Steam\" ) return registry . QueryValueEx ( key , \"SteamPath\" ) [ 0 ]'], ['Plot PCoA principal coordinates scaled by the relative abundances of otu_name .', 'def plot_PCoA ( cat_data , otu_name , unifrac , names , colors , xr , yr , outDir , save_as , plot_style ) : fig = plt . figure ( figsize = ( 14 , 8 ) ) ax = fig . add_subplot ( 111 ) for i , cat in enumerate ( cat_data ) : plt . scatter ( cat_data [ cat ] [ \"pc1\" ] , cat_data [ cat ] [ \"pc2\" ] , cat_data [ cat ] [ \"size\" ] , color = colors [ cat ] , alpha = 0.85 , marker = \"o\" , edgecolor = \"black\" , label = cat ) lgnd = plt . legend ( loc = \"best\" , scatterpoints = 3 , fontsize = 13 ) for i in range ( len ( colors . keys ( ) ) ) : lgnd . legendHandles [ i ] . _sizes = [ 80 ] plt . title ( \" \" . join ( otu_name . split ( \"_\" ) ) , style = \"italic\" ) plt . ylabel ( \"PC2 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 1 ] ) ) ) plt . xlabel ( \"PC1 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 0 ] ) ) ) plt . xlim ( round ( xr [ 0 ] * 1.5 , 1 ) , round ( xr [ 1 ] * 1.5 , 1 ) ) plt . ylim ( round ( yr [ 0 ] * 1.5 , 1 ) , round ( yr [ 1 ] * 1.5 , 1 ) ) if plot_style : gu . ggplot2_style ( ax ) fc = \"0.8\" else : fc = \"none\" fig . savefig ( os . path . join ( outDir , \"_\" . join ( otu_name . split ( ) ) ) + \".\" + save_as , facecolor = fc , edgecolor = \"none\" , format = save_as , bbox_inches = \"tight\" , pad_inches = 0.2 ) plt . close ( fig )'], ['Split up the column data in a biom table by mapping category value .', \"def split_by_category ( biom_cols , mapping , category_id ) : columns = defaultdict ( list ) for i , col in enumerate ( biom_cols ) : columns [ mapping [ col [ 'id' ] ] [ category_id ] ] . append ( ( i , col ) ) return columns\"], ['print line if starts with ...', \"def print_line ( l ) : print_lines = [ '# STOCKHOLM' , '#=GF' , '#=GS' , ' ' ] if len ( l . split ( ) ) == 0 : return True for start in print_lines : if l . startswith ( start ) : return True return False\"], ['convert stockholm to single line format', \"def stock2one ( stock ) : lines = { } for line in stock : line = line . strip ( ) if print_line ( line ) is True : yield line continue if line . startswith ( '//' ) : continue ID , seq = line . rsplit ( ' ' , 1 ) if ID not in lines : lines [ ID ] = '' else : seq = seq . strip ( ) lines [ ID ] += seq for ID , line in lines . items ( ) : yield '\\\\t' . join ( [ ID , line ] ) yield '\\\\n//'\"], ['Statics the methods . wut .', \"def math_func ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 0 : return_type = type ( args [ 0 ] ) if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) args = list ( ( setify ( x ) for x in args ) ) return return_type ( f ( * args , ** kwargs ) ) return wrapper\"], ['Show stats when pings are done', 'def dump_stats ( myStats ) : print ( \"\\\\n----%s PYTHON PING Statistics----\" % ( myStats . thisIP ) ) if myStats . pktsSent > 0 : myStats . fracLoss = ( myStats . pktsSent - myStats . pktsRcvd ) / myStats . pktsSent print ( ( \"%d packets transmitted, %d packets received, \" \"%0.1f%% packet loss\" ) % ( myStats . pktsSent , myStats . pktsRcvd , 100.0 * myStats . fracLoss ) ) if myStats . pktsRcvd > 0 : print ( \"round-trip (ms)  min/avg/max = %d/%0.1f/%d\" % ( myStats . minTime , myStats . totTime / myStats . pktsRcvd , myStats . maxTime ) ) print ( \"\" ) return'], ['bootstrap - py package updatable? .', 'def updatable ( self ) : if self . latest_version > self . current_version : updatable_version = self . latest_version else : updatable_version = False return updatable_version'], ['Show message updatable .', \"def show_message ( self ) : print ( 'current version: {current_version}\\\\n' 'latest version : {latest_version}' . format ( current_version = self . current_version , latest_version = self . latest_version ) )\"], ['Traverse the input otu - sequence file collect the non - unique OTU IDs and file the sequences associated with then under the unique OTU ID as defined by the input matrix .', 'def condense_otus ( otuF , nuniqueF ) : uniqueOTUs = set ( ) nuOTUs = { } for line in nuniqueF : line = line . split ( ) uOTU = line [ 0 ] for nuOTU in line [ 1 : ] : nuOTUs [ nuOTU ] = uOTU uniqueOTUs . add ( uOTU ) otuFilter = defaultdict ( list ) for line in otuF : line = line . split ( ) otuID , seqIDs = line [ 0 ] , line [ 1 : ] if otuID in uniqueOTUs : otuFilter [ otuID ] . extend ( seqIDs ) elif otuID in nuOTUs : otuFilter [ nuOTUs [ otuID ] ] . extend ( seqIDs ) return otuFilter'], ['determine if read overlaps with rna if so count bases', 'def rna_bases ( rna_cov , scaffold , bases , line ) : start = int ( line [ 3 ] ) stop = start + bases - 1 if scaffold not in rna_cov : return rna_cov for pos in rna_cov [ scaffold ] [ 2 ] : ol = get_overlap ( [ start , stop ] , pos ) rna_cov [ scaffold ] [ 0 ] += ol return rna_cov'], ['parse ggKbase scaffold - to - bin mapping - scaffolds - to - bins and bins - to - scaffolds', \"def parse_s2bins ( s2bins ) : s2b = { } b2s = { } for line in s2bins : line = line . strip ( ) . split ( ) s , b = line [ 0 ] , line [ 1 ] if 'UNK' in b : continue if len ( line ) > 2 : g = ' ' . join ( line [ 2 : ] ) else : g = 'n/a' b = '%s\\\\t%s' % ( b , g ) s2b [ s ] = b if b not in b2s : b2s [ b ] = [ ] b2s [ b ] . append ( s ) return s2b , b2s\"], ['remove any bins that don t have 16S', 'def filter_missing_rna ( s2bins , bins2s , rna_cov ) : for bin , scaffolds in list ( bins2s . items ( ) ) : c = 0 for s in scaffolds : if s in rna_cov : c += 1 if c == 0 : del bins2s [ bin ] for scaffold , bin in list ( s2bins . items ( ) ) : if bin not in bins2s : del s2bins [ scaffold ] return s2bins , bins2s'], ['calculate bin coverage', 'def calc_bin_cov ( scaffolds , cov ) : bases = sum ( [ cov [ i ] [ 0 ] for i in scaffolds if i in cov ] ) length = sum ( [ cov [ i ] [ 1 ] for i in scaffolds if i in cov ] ) if length == 0 : return 0 return float ( float ( bases ) / float ( length ) )'], ['Make sure there is at least a translation has been filled in . If a default language has been specified make sure that it exists amongst translations .', \"def clean ( self ) : super ( TranslationFormSet , self ) . clean ( ) if settings . HIDE_LANGUAGE : return if len ( self . forms ) > 0 : if settings . DEFAULT_LANGUAGE and not any ( self . errors ) : for form in self . forms : language_code = form . cleaned_data . get ( 'language_code' , None ) if language_code == settings . DEFAULT_LANGUAGE : return raise forms . ValidationError ( _ ( 'No translation provided for default language \\\\'%s\\\\'.' ) % settings . DEFAULT_LANGUAGE ) else : raise forms . ValidationError ( _ ( 'At least one translation should be provided.' ) )\"], ['If a default language has been set and is still available in self . available_languages return it and remove it from the list .', \"def _get_default_language ( self ) : assert hasattr ( self , 'available_languages' ) , 'No available languages have been generated.' assert len ( self . available_languages ) > 0 , 'No available languages to select from.' if ( settings . DEFAULT_LANGUAGE and settings . DEFAULT_LANGUAGE in self . available_languages ) or ( 'language_code' not in self . form . base_fields ) : self . available_languages . remove ( settings . DEFAULT_LANGUAGE ) return settings . DEFAULT_LANGUAGE else : return self . available_languages . pop ( 0 )\"], ['Construct the form overriding the initial value for language_code .', \"def _construct_form ( self , i , ** kwargs ) : if not settings . HIDE_LANGUAGE : self . _construct_available_languages ( ) form = super ( TranslationFormSet , self ) . _construct_form ( i , ** kwargs ) if settings . HIDE_LANGUAGE : form . instance . language_code = settings . DEFAULT_LANGUAGE else : language_code = form . instance . language_code if language_code : logger . debug ( u'Removing translation choice %s for instance %s' u' in form %d' , language_code , form . instance , i ) self . available_languages . remove ( language_code ) else : initial_language_code = self . _get_default_language ( ) logger . debug ( u'Preselecting language code %s for form %d' , initial_language_code , i ) form . initial [ 'language_code' ] = initial_language_code return form\"], ['merge separate fastq files', 'def fq_merge ( R1 , R2 ) : c = itertools . cycle ( [ 1 , 2 , 3 , 4 ] ) for r1 , r2 in zip ( R1 , R2 ) : n = next ( c ) if n == 1 : pair = [ [ ] , [ ] ] pair [ 0 ] . append ( r1 . strip ( ) ) pair [ 1 ] . append ( r2 . strip ( ) ) if n == 4 : yield pair'], ['Creates hash ring .', \"def _build_circle ( self ) : total_weight = 0 for node in self . _nodes : total_weight += self . _weights . get ( node , 1 ) for node in self . _nodes : weight = self . _weights . get ( node , 1 ) ks = math . floor ( ( 40 * len ( self . _nodes ) * weight ) / total_weight ) for i in xrange ( 0 , int ( ks ) ) : b_key = self . _md5_digest ( '%s-%s-salt' % ( node , i ) ) for l in xrange ( 0 , 4 ) : key = ( ( b_key [ 3 + l * 4 ] << 24 ) | ( b_key [ 2 + l * 4 ] << 16 ) | ( b_key [ 1 + l * 4 ] << 8 ) | b_key [ l * 4 ] ) self . _hashring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )\"], ['Return long integer for a given key that represent it place on the hash ring .', 'def _gen_key ( self , key ) : b_key = self . _md5_digest ( key ) return self . _hashi ( b_key , lambda x : x )'], ['Returns True if there exists a custom image for app_id .', 'def has_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) return any ( map ( os . path . exists , possible_paths ) )'], ['Returns the custom image associated with a given app . If there are multiple candidate images on disk one is chosen arbitrarily .', 'def get_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) existing_images = filter ( os . path . exists , possible_paths ) if len ( existing_images ) > 0 : return existing_images [ 0 ]'], ['Sets the custom image for app_id to be the image located at image_path . If there already exists a custom image for app_id it will be deleted . Returns True is setting the image was successful .', 'def set_custom_image ( user_context , app_id , image_path ) : if image_path is None : return False if not os . path . exists ( image_path ) : return False ( root , ext ) = os . path . splitext ( image_path ) if not is_valid_extension ( ext ) : return False if has_custom_image ( user_context , app_id ) : img = get_custom_image ( user_context , app_id ) assert ( img is not None ) os . remove ( img ) parent_dir = paths . custom_images_directory ( user_context ) new_path = os . path . join ( parent_dir , app_id + ext ) shutil . copyfile ( image_path , new_path ) return True'], ['Read an orthography profile from a metadata file or a default tab - separated profile file .', \"def from_file ( cls , fname , form = None ) : try : tg = TableGroup . from_file ( fname ) opfname = None except JSONDecodeError : tg = TableGroup . fromvalue ( cls . MD ) opfname = fname if len ( tg . tables ) != 1 : raise ValueError ( 'profile description must contain exactly one table' ) metadata = tg . common_props metadata . update ( fname = Path ( fname ) , form = form ) return cls ( * [ { k : None if ( k != cls . GRAPHEME_COL and v == cls . NULL ) else v for k , v in d . items ( ) } for d in tg . tables [ 0 ] . iterdicts ( fname = opfname ) ] , ** metadata )\"], ['Create a Profile instance from the Unicode graphemes found in text .', \"def from_text ( cls , text , mapping = 'mapping' ) : graphemes = Counter ( grapheme_pattern . findall ( text ) ) specs = [ OrderedDict ( [ ( cls . GRAPHEME_COL , grapheme ) , ( 'frequency' , frequency ) , ( mapping , grapheme ) ] ) for grapheme , frequency in graphemes . most_common ( ) ] return cls ( * specs )\"], ['split fasta file into separate fasta files based on list of scaffolds that belong to each separate file', \"def split_fasta ( f , id2f ) : opened = { } for seq in parse_fasta ( f ) : id = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] if id not in id2f : continue fasta = id2f [ id ] if fasta not in opened : opened [ fasta ] = '%s.fa' % fasta seq [ 1 ] += '\\\\n' with open ( opened [ fasta ] , 'a+' ) as f_out : f_out . write ( '\\\\n' . join ( seq ) )\"], ['Check whether pathname is a valid user data directory', 'def _is_user_directory ( self , pathname ) : fullpath = os . path . join ( self . userdata_location ( ) , pathname ) return os . path . isdir ( fullpath ) and pathname . isdigit ( )'], ['Returns an array of user ids for users on the filesystem', 'def local_users ( self ) : userdirs = filter ( self . _is_user_directory , os . listdir ( self . userdata_location ( ) ) ) return map ( lambda userdir : user . User ( self , int ( userdir ) ) , userdirs )'], ['Calculates degree days starting with a series of temperature equivalent values', \"def _calculate_degree_days ( temperature_equivalent , base_temperature , cooling = False ) : if cooling : ret = temperature_equivalent - base_temperature else : ret = base_temperature - temperature_equivalent ret [ ret < 0 ] = 0 prefix = 'CDD' if cooling else 'HDD' ret . name = '{}_{}' . format ( prefix , base_temperature ) return ret\"], ['Development status .', \"def status ( self ) : return { self . _acronym_status ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_status ) }\"], ['OSI Approved license .', \"def licenses ( self ) : return { self . _acronym_lic ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Remove prefix .', \"def licenses_desc ( self ) : return { self . _acronym_lic ( l ) : l . split ( self . prefix_lic ) [ 1 ] for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Convert license acronym .', \"def _acronym_lic ( self , license_statement ) : pat = re . compile ( r'\\\\(([\\\\w+\\\\W?\\\\s?]+)\\\\)' ) if pat . search ( license_statement ) : lic = pat . search ( license_statement ) . group ( 1 ) if lic . startswith ( 'CNRI' ) : acronym_licence = lic [ : 4 ] else : acronym_licence = lic . replace ( ' ' , '' ) else : acronym_licence = '' . join ( [ w [ 0 ] for w in license_statement . split ( self . prefix_lic ) [ 1 ] . split ( ) ] ) return acronym_licence\"], ['calc MD5 based on path', \"def calcMD5 ( path ) : if os . path . exists ( path ) is False : yield False else : command = [ 'md5sum' , path ] p = Popen ( command , stdout = PIPE ) for line in p . communicate ( ) [ 0 ] . splitlines ( ) : yield line . decode ( 'ascii' ) . strip ( ) . split ( ) [ 0 ] p . wait ( ) yield False\"], ['download files with wget', \"def wget ( ftp , f = False , exclude = False , name = False , md5 = False , tries = 10 ) : if f is False : f = ftp . rsplit ( '/' , 1 ) [ - 1 ] t = 0 while md5check ( f , ftp , md5 , exclude ) is not True : t += 1 if name is not False : print ( '# downloading:' , name , f ) if exclude is False : command = 'wget -q --random-wait %s' % ( ftp ) else : command = 'wget -q --random-wait -R %s %s' % ( exclude , ftp ) p = Popen ( command , shell = True ) p . communicate ( ) if t >= tries : print ( 'not downloaded:' , name , f ) return [ f , False ] return [ f , True ]\"], ['check that at least one of queries is in list l', \"def check ( line , queries ) : line = line . strip ( ) spLine = line . replace ( '.' , ' ' ) . split ( ) matches = set ( spLine ) . intersection ( queries ) if len ( matches ) > 0 : return matches , line . split ( '\\\\t' ) return matches , False\"], ['search entrez using specified database and accession', \"def entrez ( db , acc ) : c1 = [ 'esearch' , '-db' , db , '-query' , acc ] c2 = [ 'efetch' , '-db' , 'BioSample' , '-format' , 'docsum' ] p1 = Popen ( c1 , stdout = PIPE , stderr = PIPE ) p2 = Popen ( c2 , stdin = p1 . stdout , stdout = PIPE , stderr = PIPE ) return p2 . communicate ( )\"], ['attempt to use NCBI Entrez to get BioSample ID', \"def searchAccession ( acc ) : out , error = entrez ( 'genome' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'nucleotide' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'assembly' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) for error in error . splitlines ( ) : error = error . decode ( 'ascii' ) . strip ( ) if '500 Can' in error : return ( False , acc , 'no network' ) return ( False , acc , 'efetch failed' )\"], ['download genome info from NCBI', \"def getFTPs ( accessions , ftp , search , exclude , convert = False , threads = 1 , attempt = 1 , max_attempts = 2 ) : info = wget ( ftp ) [ 0 ] allMatches = [ ] for genome in open ( info , encoding = 'utf8' ) : genome = str ( genome ) matches , genomeInfo = check ( genome , accessions ) if genomeInfo is not False : f = genomeInfo [ 0 ] + search Gftp = genomeInfo [ 19 ] Gftp = Gftp + '/' + search allMatches . extend ( matches ) yield ( Gftp , f , exclude , matches ) newAccs = [ ] missing = accessions . difference ( set ( allMatches ) ) if convert is True : pool = Pool ( threads ) pool = pool . imap_unordered ( searchAccession , missing ) for newAcc in tqdm ( pool , total = len ( missing ) ) : status , accession , newAcc = newAcc if status is True : newAccs . append ( newAcc ) print ( 'not found:' , accession , '->' , newAcc ) else : for accession in missing : print ( 'not found:' , accession ) if len ( newAccs ) > 0 and attempt <= max_attempts : print ( 'convert accession attempt' , attempt ) attempt += 1 for hit in getFTPs ( set ( newAccs ) , ftp , search , exclude , convert , threads = 1 , attempt = attempt ) : yield hit\"], ['download genomes from NCBI', \"def download ( args ) : accessions , infoFTP = set ( args [ 'g' ] ) , args [ 'i' ] search , exclude = args [ 's' ] , args [ 'e' ] FTPs = getFTPs ( accessions , infoFTP , search , exclude , threads = args [ 't' ] , convert = args [ 'convert' ] ) if args [ 'test' ] is True : for genome in FTPs : print ( 'found:' , ';' . join ( genome [ - 1 ] ) , genome [ 0 ] ) return FTPs pool = Pool ( args [ 't' ] ) pool = pool . imap_unordered ( wgetGenome , FTPs ) files = [ ] for f in tqdm ( pool , total = len ( accessions ) ) : files . append ( f ) return files\"], ['remove pesky characters from fasta file header', 'def fix_fasta ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 0 ] = remove_char ( seq [ 0 ] ) if len ( seq [ 1 ] ) > 0 : yield seq'], ['Compute a DataFrame summary of a Stats object .', \"def _calc_frames ( stats ) : timings = [ ] callers = [ ] for key , values in iteritems ( stats . stats ) : timings . append ( pd . Series ( key + values [ : - 1 ] , index = timing_colnames , ) ) for caller_key , caller_values in iteritems ( values [ - 1 ] ) : callers . append ( pd . Series ( key + caller_key + caller_values , index = caller_columns , ) ) timings_df = pd . DataFrame ( timings ) callers_df = pd . DataFrame ( callers ) timings_df [ 'filename:funcname' ] = ( timings_df [ 'filename' ] + ':' + timings_df [ 'funcname' ] ) timings_df = timings_df . groupby ( 'filename:funcname' ) . sum ( ) return timings_df , callers_df\"], ['get unmapped reads', \"def unmapped ( sam , mates ) : for read in sam : if read . startswith ( '@' ) is True : continue read = read . strip ( ) . split ( ) if read [ 2 ] == '*' and read [ 6 ] == '*' : yield read elif mates is True : if read [ 2 ] == '*' or read [ 6 ] == '*' : yield read for i in read : if i == 'YT:Z:UP' : yield read\"], ['execute jobs in processes using N threads', 'def parallel ( processes , threads ) : pool = multithread ( threads ) pool . map ( run_process , processes ) pool . close ( ) pool . join ( )'], ['the final log processor that structlog requires to render .', 'def define_log_renderer ( fmt , fpath , quiet ) : if fmt : return structlog . processors . JSONRenderer ( ) if fpath is not None : return structlog . processors . JSONRenderer ( ) if sys . stderr . isatty ( ) and not quiet : return structlog . dev . ConsoleRenderer ( ) return structlog . processors . JSONRenderer ( )'], ['Add unique id type and hostname', \"def _structlog_default_keys_processor ( logger_class , log_method , event ) : global HOSTNAME if 'id' not in event : event [ 'id' ] = '%s_%s' % ( datetime . utcnow ( ) . strftime ( '%Y%m%dT%H%M%S' ) , uuid . uuid1 ( ) . hex ) if 'type' not in event : event [ 'type' ] = 'log' event [ 'host' ] = HOSTNAME return event\"], ['log processors that structlog executes before final rendering', 'def define_log_processors ( ) : return [ structlog . processors . TimeStamper ( fmt = \"iso\" ) , _structlog_default_keys_processor , structlog . stdlib . PositionalArgumentsFormatter ( ) , structlog . processors . StackInfoRenderer ( ) , structlog . processors . format_exc_info , ]'], ['configures a logger when required write to stderr or a file', 'def _configure_logger ( fmt , quiet , level , fpath , pre_hooks , post_hooks , metric_grouping_interval ) : level = getattr ( logging , level . upper ( ) ) global _GLOBAL_LOG_CONFIGURED if _GLOBAL_LOG_CONFIGURED : return def wrap_hook ( fn ) : @ wraps ( fn ) def processor ( logger , method_name , event_dict ) : fn ( event_dict ) return event_dict return processor processors = define_log_processors ( ) processors . extend ( [ wrap_hook ( h ) for h in pre_hooks ] ) if metric_grouping_interval : processors . append ( metrics_grouping_processor ) log_renderer = define_log_renderer ( fmt , fpath , quiet ) stderr_required = ( not quiet ) pretty_to_stderr = ( stderr_required and ( fmt == \"pretty\" or ( fmt is None and sys . stderr . isatty ( ) ) ) ) should_inject_pretty_renderer = ( pretty_to_stderr and not isinstance ( log_renderer , structlog . dev . ConsoleRenderer ) ) if should_inject_pretty_renderer : stderr_required = False processors . append ( StderrConsoleRenderer ( ) ) processors . append ( log_renderer ) processors . extend ( [ wrap_hook ( h ) for h in post_hooks ] ) streams = [ ] if stderr_required : streams . append ( sys . stderr ) if fpath is not None : streams . append ( open ( fpath , \\'a\\' ) ) assert len ( streams ) != 0 , \"cannot configure logger for 0 streams\" stream = streams [ 0 ] if len ( streams ) == 1 else Stream ( * streams ) atexit . register ( stream . close ) structlog . configure ( processors = processors , context_class = dict , logger_factory = LevelLoggerFactory ( stream , level = level ) , wrapper_class = BoundLevelLogger , cache_logger_on_first_use = True , ) stdlib_root_log = logging . getLogger ( ) stdlib_root_log . addHandler ( StdlibStructlogHandler ( ) ) stdlib_root_log . setLevel ( level ) _GLOBAL_LOG_CONFIGURED = True'], ['Instead of using a processor adding basic information like caller filename etc here .', 'def _add_base_info ( self , event_dict ) : f = sys . _getframe ( ) level_method_frame = f . f_back caller_frame = level_method_frame . f_back return event_dict'], ['Propagate a method call to the wrapped logger .', \"def _proxy_to_logger ( self , method_name , event , * event_args , ** event_kw ) : if isinstance ( event , bytes ) : event = event . decode ( 'utf-8' ) if event_args : event_kw [ 'positional_args' ] = event_args return super ( BoundLevelLogger , self ) . _proxy_to_logger ( method_name , event = event , ** event_kw )\"], ['Given four points of a rectangle translate the rectangle to the specified x and y coordinates and optionally change the width .', 'def translate ( rect , x , y , width = 1 ) : return ( ( rect [ 0 ] [ 0 ] + x , rect [ 0 ] [ 1 ] + y ) , ( rect [ 1 ] [ 0 ] + x , rect [ 1 ] [ 1 ] + y ) , ( rect [ 2 ] [ 0 ] + x + width , rect [ 2 ] [ 1 ] + y ) , ( rect [ 3 ] [ 0 ] + x + width , rect [ 3 ] [ 1 ] + y ) )'], ['remove problem characters from string', \"def remove_bad ( string ) : remove = [ ':' , ',' , '(' , ')' , ' ' , '|' , ';' , '\\\\'' ] for c in remove : string = string . replace ( c , '_' ) return string\"], ['make copy of sequences with short identifier', \"def get_ids ( a ) : a_id = '%s.id.fa' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) a_id_lookup = '%s.id.lookup' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) if check ( a_id ) is True : return a_id , a_id_lookup a_id_f = open ( a_id , 'w' ) a_id_lookup_f = open ( a_id_lookup , 'w' ) ids = [ ] for seq in parse_fasta ( open ( a ) ) : id = id_generator ( ) while id in ids : id = id_generator ( ) ids . append ( id ) header = seq [ 0 ] . split ( '>' ) [ 1 ] name = remove_bad ( header ) seq [ 0 ] = '>%s %s' % ( id , header ) print ( '\\\\n' . join ( seq ) , file = a_id_f ) print ( '%s\\\\t%s\\\\t%s' % ( id , name , header ) , file = a_id_lookup_f ) return a_id , a_id_lookup\"], ['convert fasta to phylip because RAxML is ridiculous', 'def convert2phylip ( convert ) : out = \\'%s.phy\\' % ( convert . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if check ( out ) is False : convert = open ( convert , \\'rU\\' ) out_f = open ( out , \\'w\\' ) alignments = AlignIO . parse ( convert , \"fasta\" ) AlignIO . write ( alignments , out , \"phylip\" ) return out'], ['run IQ - Tree', 'def run_iqtree ( phy , model , threads , cluster , node ) : if threads > 24 : ppn = 24 else : ppn = threads tree = \\'%s.treefile\\' % ( phy ) if check ( tree ) is False : if model is False : model = \\'TEST\\' dir = os . getcwd ( ) command = \\'iqtree-omp -s %s -m %s -nt %s -quiet\\' % ( phy , model , threads ) if cluster is False : p = Popen ( command , shell = True ) else : if node is False : node = \\'1\\' qsub = \\'qsub -l nodes=%s:ppn=%s -m e -N iqtree\\' % ( node , ppn ) command = \\'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree\\' % ( dir , phy , command , dir ) re_call = \\'cd %s; %s --no-fast --iq\\' % ( dir . rsplit ( \\'/\\' , 1 ) [ 0 ] , \\' \\' . join ( sys . argv ) ) p = Popen ( \\'echo \"%s;%s\" | %s\\' % ( command , re_call , qsub ) , shell = True ) p . communicate ( ) return tree'], ['get the names for sequences in the raxml tree', \"def fix_tree ( tree , a_id_lookup , out ) : if check ( out ) is False and check ( tree ) is True : tree = open ( tree ) . read ( ) for line in open ( a_id_lookup ) : id , name , header = line . strip ( ) . split ( '\\\\t' ) tree = tree . replace ( id + ':' , name + ':' ) out_f = open ( out , 'w' ) print ( tree . strip ( ) , file = out_f ) return out\"], ['Creates a new Nydus cluster from the given settings .', \"def create_cluster ( settings ) : settings = copy . deepcopy ( settings ) backend = settings . pop ( 'engine' , settings . pop ( 'backend' , None ) ) if isinstance ( backend , basestring ) : Conn = import_string ( backend ) elif backend : Conn = backend else : raise KeyError ( 'backend' ) cluster = settings . pop ( 'cluster' , None ) if not cluster : Cluster = Conn . get_cluster ( ) elif isinstance ( cluster , basestring ) : Cluster = import_string ( cluster ) else : Cluster = cluster router = settings . pop ( 'router' , None ) if not router : Router = BaseRouter elif isinstance ( router , basestring ) : Router = import_string ( router ) else : Router = router return Cluster ( router = Router , backend = Conn , ** settings )\"], ['Gets the translation of a specific field for a specific language code .', \"def _get_translation ( self , field , code ) : if not code in self . _translation_cache : translations = self . translations . select_related ( ) logger . debug ( u'Matched with field %s for language %s. Attempting lookup.' , field , code ) try : translation_obj = translations . get ( language_code = code ) except ObjectDoesNotExist : translation_obj = None self . _translation_cache [ code ] = translation_obj logger . debug ( u'Translation not found in cache.' ) else : logger . debug ( u'Translation found in cache.' ) translation_obj = self . _translation_cache . get ( code ) if not translation_obj : raise ObjectDoesNotExist field_value = getattr ( translation_obj , field ) logger . debug ( u'Found translation object %s, returning value %s.' , translation_obj , field_value ) return field_value\"], ['Wrapper to allow for easy unicode representation of an object by the specified property . If this wrapper is not able to find the right translation of the specified property it will return the default value instead .', \"def unicode_wrapper ( self , property , default = ugettext ( 'Untitled' ) ) : try : value = getattr ( self , property ) except ValueError : logger . warn ( u'ValueError rendering unicode for %s object.' , self . _meta . object_name ) value = None if not value : value = default return value\"], ['remove insertion columns from aligned fasta file', \"def strip_inserts ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 1 ] = '' . join ( [ b for b in seq [ 1 ] if b == '-' or b . isupper ( ) ] ) yield seq\"], ['Transform a string s graphemes into the mappings given in a different column in the orthography profile .', 'def transform ( self , word , column = Profile . GRAPHEME_COL , error = errors . replace ) : assert self . op , \\'method can only be called with orthography profile.\\' if column != Profile . GRAPHEME_COL and column not in self . op . column_labels : raise ValueError ( \"Column {0} not found in profile.\" . format ( column ) ) word = self . op . tree . parse ( word , error ) if column == Profile . GRAPHEME_COL : return word out = [ ] for token in word : try : target = self . op . graphemes [ token ] [ column ] except KeyError : target = self . _errors [ \\'replace\\' ] ( token ) if target is not None : if isinstance ( target , ( tuple , list ) ) : out . extend ( target ) else : out . append ( target ) return out'], ['Function to tokenize input string and return output of str with ortho rules applied .', 'def rules ( self , word ) : return self . _rules . apply ( word ) if self . _rules else word'], ['Given a string that is space - delimited on Unicode grapheme clusters group Unicode modifier letters with their preceding base characters deal with tie bars etc .', 'def combine_modifiers ( self , graphemes ) : result = [ ] temp = \"\" count = len ( graphemes ) for grapheme in reversed ( graphemes ) : count -= 1 if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Lm\" and not ord ( grapheme ) in [ 712 , 716 ] : temp = grapheme + temp if count == 0 : result [ - 1 ] = temp + result [ - 1 ] continue if len ( grapheme ) == 1 and ord ( grapheme ) in [ 712 , 716 ] : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Sk\" : if len ( result ) == 0 : result . append ( grapheme ) temp = \"\" continue else : if unicodedata . category ( result [ - 1 ] [ 0 ] ) == \"Sk\" : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue result . append ( grapheme + temp ) temp = \"\" segments = result [ : : - 1 ] i = 0 r = [ ] while i < len ( segments ) : if ord ( segments [ i ] [ - 1 ] ) in [ 865 , 860 ] : r . append ( segments [ i ] + segments [ i + 1 ] ) i += 2 else : r . append ( segments [ i ] ) i += 1 return r'], ['parse catalytic RNAs to gff format', \"def parse_catalytic ( insertion , gff ) : offset = insertion [ 'offset' ] GeneStrand = insertion [ 'strand' ] if type ( insertion [ 'intron' ] ) is not str : return gff for intron in parse_fasta ( insertion [ 'intron' ] . split ( '|' ) ) : ID , annot , strand , pos = intron [ 0 ] . split ( '>' ) [ 1 ] . split ( ) Start , End = [ int ( i ) for i in pos . split ( '-' ) ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Rfam' ) gff [ 'feature' ] . append ( 'Catalytic RNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse ORF to gff format', \"def parse_orf ( insertion , gff ) : offset = insertion [ 'offset' ] if type ( insertion [ 'orf' ] ) is not str : return gff for orf in parse_fasta ( insertion [ 'orf' ] . split ( '|' ) ) : ID = orf [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End , strand = [ int ( i ) for i in orf [ 0 ] . split ( ' # ' ) [ 1 : 4 ] ] if strand == 1 : strand = '+' else : strand = '-' GeneStrand = insertion [ 'strand' ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 annot = orf [ 0 ] . split ( ) [ 1 ] if annot == 'n/a' : annot = 'unknown' gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Prodigal and Pfam' ) gff [ 'feature' ] . append ( 'CDS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse insertion to gff format', \"def parse_insertion ( insertion , gff ) : offset = insertion [ 'offset' ] for ins in parse_fasta ( insertion [ 'insertion sequence' ] . split ( '|' ) ) : strand = insertion [ 'strand' ] ID = ins [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End = [ int ( i ) for i in ins [ 0 ] . split ( 'gene-pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] Start , End = abs ( Start + offset ) , abs ( End + offset ) if strand == '-' : Start , End = End , Start gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( insertion [ 'source' ] ) gff [ 'feature' ] . append ( 'IVS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s' % ( ID ) ) return gff\"], ['parse rRNA to gff format', \"def parse_rRNA ( insertion , seq , gff ) : offset = insertion [ 'offset' ] strand = insertion [ 'strand' ] for rRNA in parse_masked ( seq , 0 ) [ 0 ] : rRNA = '' . join ( rRNA ) Start = seq [ 1 ] . find ( rRNA ) + 1 End = Start + len ( rRNA ) - 1 if strand == '-' : Start , End = End - 2 , Start - 2 pos = ( abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 ) Start , End = min ( pos ) , max ( pos ) source = insertion [ 'source' ] annot = '%s rRNA' % ( source . split ( 'from' , 1 ) [ 0 ] ) gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'rRNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'Name=%s' % ( annot ) ) return gff\"], ['convert iTable to gff file', \"def iTable2GFF ( iTable , fa , contig = False ) : columns = [ '#seqname' , 'source' , 'feature' , 'start' , 'end' , 'score' , 'strand' , 'frame' , 'attribute' ] gff = { c : [ ] for c in columns } for insertion in iTable . iterrows ( ) : insertion = insertion [ 1 ] if insertion [ 'ID' ] not in fa : continue strand = insertion [ 'sequence' ] . split ( 'strand=' , 1 ) [ 1 ] . split ( ) [ 0 ] if contig is True : gene = [ int ( i ) for i in insertion [ 'sequence' ] . split ( 'pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] if strand == '-' : offset = - 1 * ( gene [ 1 ] ) else : offset = gene [ 0 ] else : strand = '+' gene = [ 1 , int ( insertion [ 'sequence' ] . split ( 'total-len=' , 1 ) [ 1 ] . split ( ) [ 0 ] ) ] offset = gene [ 0 ] insertion [ 'strand' ] = strand insertion [ 'offset' ] = offset source = insertion [ 'sequence' ] . split ( '::model' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ - 1 ] insertion [ 'source' ] = source geneAnnot = '%s rRNA gene' % ( source . split ( 'from' , 1 ) [ 0 ] ) geneNum = insertion [ 'sequence' ] . split ( 'seq=' , 1 ) [ 1 ] . split ( ) [ 0 ] gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'Gene' ) gff [ 'start' ] . append ( gene [ 0 ] ) gff [ 'end' ] . append ( gene [ 1 ] ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( geneNum , geneAnnot ) ) gff = parse_rRNA ( insertion , fa [ insertion [ 'ID' ] ] , gff ) gff = parse_insertion ( insertion , gff ) gff = parse_orf ( insertion , gff ) gff = parse_catalytic ( insertion , gff ) return pd . DataFrame ( gff ) [ columns ] . drop_duplicates ( )\"], ['Given an abundance table group the counts by every taxonomic level .', \"def summarize_taxa ( biom ) : tamtcounts = defaultdict ( int ) tot_seqs = 0.0 for row , col , amt in biom [ 'data' ] : tot_seqs += amt rtax = biom [ 'rows' ] [ row ] [ 'metadata' ] [ 'taxonomy' ] for i , t in enumerate ( rtax ) : t = t . strip ( ) if i == len ( rtax ) - 1 and len ( t ) > 3 and len ( rtax [ - 1 ] ) > 3 : t = 's__' + rtax [ i - 1 ] . strip ( ) . split ( '_' ) [ - 1 ] + '_' + t . split ( '_' ) [ - 1 ] tamtcounts [ t ] += amt lvlData = { lvl : levelData ( tamtcounts , tot_seqs , lvl ) for lvl in [ 'k' , 'p' , 'c' , 'o' , 'f' , 'g' , 's' ] } return tot_seqs , lvlData\"], ['Returns the path to the custom image set for this game or None if no image is set', 'def custom_image ( self , user ) : for ext in self . valid_custom_image_extensions ( ) : image_location = self . _custom_image_path ( user , ext ) if os . path . isfile ( image_location ) : return image_location return None'], ['Sets a custom image for the game . image_path should refer to an image file on disk', 'def set_image ( self , user , image_path ) : _ , ext = os . path . splitext ( image_path ) shutil . copy ( image_path , self . _custom_image_path ( user , ext ) )'], ['get a list of mapped reads', \"def sam_list ( sam ) : list = [ ] for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : list . append ( id ) return set ( list )\"], ['get a list of mapped reads require that both pairs are mapped in the sam file in order to remove the reads', \"def sam_list_paired ( sam ) : list = [ ] pair = [ '1' , '2' ] prev = '' for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : read = id . rsplit ( '/' ) [ 0 ] if read == prev : list . append ( read ) prev = read return set ( list )\"], ['require that both pairs are mapped in the sam file in order to remove the reads', \"def filter_paired ( list ) : pairs = { } filtered = [ ] for id in list : read = id . rsplit ( '/' ) [ 0 ] if read not in pairs : pairs [ read ] = [ ] pairs [ read ] . append ( id ) for read in pairs : ids = pairs [ read ] if len ( ids ) == 2 : filtered . extend ( ids ) return set ( filtered )\"], ['print fastq from sam', \"def sam2fastq ( line ) : fastq = [ ] fastq . append ( '@%s' % line [ 0 ] ) fastq . append ( line [ 9 ] ) fastq . append ( '+%s' % line [ 0 ] ) fastq . append ( line [ 10 ] ) return fastq\"], ['- check to see if the read maps with < = threshold number of mismatches - mm_option = one or both depending on whether or not one or both reads in a pair need to pass the mismatch threshold - pair can be False if read does not have a pair - make sure alignment score is not 0 which would indicate that the read was not aligned to the reference', \"def check_mismatches ( read , pair , mismatches , mm_option , req_map ) : if pair is False : mm = count_mismatches ( read ) if mm is False : return False if mismatches is False : return True if mm <= mismatches : return True r_mm = count_mismatches ( read ) p_mm = count_mismatches ( pair ) if r_mm is False and p_mm is False : return False if mismatches is False : return True if req_map is True : if r_mm is False or p_mm is False : return False if mm_option == 'one' : if ( r_mm is not False and r_mm <= mismatches ) or ( p_mm is not False and p_mm <= mismatches ) : return True if mm_option == 'both' : if r_mm is False : if p_mm <= mismatches : return True elif p_mm is False : if r_mm <= mismatches : return True elif ( r_mm is not False and r_mm <= mismatches ) and ( p_mm is not False and p_mm <= mismatches ) : return True return False\"], ['determine whether or not reads map to specific region of scaffold', 'def check_region ( read , pair , region ) : if region is False : return True for mapping in read , pair : if mapping is False : continue start , length = int ( mapping [ 3 ] ) , len ( mapping [ 9 ] ) r = [ start , start + length - 1 ] if get_overlap ( r , region ) > 0 : return True return False'], ['Returns a Steam object representing the current Steam installation on the users computer . If the user doesn t have Steam installed returns None .', \"def get_steam ( ) : helper = lambda udd : Steam ( udd ) if os . path . exists ( udd ) else None plat = platform . system ( ) if plat == 'Darwin' : return helper ( paths . default_osx_userdata_path ( ) ) if plat == 'Linux' : return helper ( paths . default_linux_userdata_path ( ) ) if plat == 'Windows' : possible_dir = winutils . find_userdata_directory ( ) return helper ( possible_dir ) if possible_dir is not None else None return None\"], ['normalize from zero to one for row or table', \"def zero_to_one ( table , option ) : if option == 'table' : m = min ( min ( table ) ) ma = max ( max ( table ) ) t = [ ] for row in table : t_row = [ ] if option != 'table' : m , ma = min ( row ) , max ( row ) for i in row : if ma == m : t_row . append ( 0 ) else : t_row . append ( ( i - m ) / ( ma - m ) ) t . append ( t_row ) return t\"], ['calculate percent of total', \"def pertotal ( table , option ) : if option == 'table' : total = sum ( [ i for line in table for i in line ] ) t = [ ] for row in table : t_row = [ ] if option != 'table' : total = sum ( row ) for i in row : if total == 0 : t_row . append ( 0 ) else : t_row . append ( i / total * 100 ) t . append ( t_row ) return t\"], ['scale table based on the column with the largest sum', 'def scale ( table ) : t = [ ] columns = [ [ ] for i in table [ 0 ] ] for row in table : for i , v in enumerate ( row ) : columns [ i ] . append ( v ) sums = [ float ( sum ( i ) ) for i in columns ] scale_to = float ( max ( sums ) ) scale_factor = [ scale_to / i for i in sums if i != 0 ] for row in table : t . append ( [ a * b for a , b in zip ( row , scale_factor ) ] ) return t'], ['fit to normal distribution', \"def norm ( table ) : print ( '# norm dist is broken' , file = sys . stderr ) exit ( ) from matplotlib . pyplot import hist as hist t = [ ] for i in table : t . append ( np . ndarray . tolist ( hist ( i , bins = len ( i ) , normed = True ) [ 0 ] ) ) return t\"], ['log transform each value in table', 'def log_trans ( table ) : t = [ ] all = [ item for sublist in table for item in sublist ] if min ( all ) == 0 : scale = min ( [ i for i in all if i != 0 ] ) * 10e-10 else : scale = 0 for i in table : t . append ( np . ndarray . tolist ( np . log10 ( [ j + scale for j in i ] ) ) ) return t'], ['box - cox transform table', 'def box_cox ( table ) : from scipy . stats import boxcox as bc t = [ ] for i in table : if min ( i ) == 0 : scale = min ( [ j for j in i if j != 0 ] ) * 10e-10 else : scale = 0 t . append ( np . ndarray . tolist ( bc ( np . array ( [ j + scale for j in i ] ) ) [ 0 ] ) ) return t'], ['inverse hyperbolic sine transformation', 'def inh ( table ) : t = [ ] for i in table : t . append ( np . ndarray . tolist ( np . arcsinh ( i ) ) ) return t'], ['from SparCC - randomly draw from the corresponding posterior Dirichlet distribution with a uniform prior', 'def diri ( table ) : t = [ ] for i in table : a = [ j + 1 for j in i ] t . append ( np . ndarray . tolist ( np . random . mtrand . dirichlet ( a ) ) ) return t'], ['Given a list of sample IDs generate unique n - base barcodes for each . Note that only 4^n unique barcodes are possible .', \"def generate_barcodes ( nIds , codeLen = 12 ) : def next_code ( b , c , i ) : return c [ : i ] + b + ( c [ i + 1 : ] if i < - 1 else '' ) def rand_base ( ) : return random . choice ( [ 'A' , 'T' , 'C' , 'G' ] ) def rand_seq ( n ) : return '' . join ( [ rand_base ( ) for _ in range ( n ) ] ) hpf = re . compile ( 'aaaa|cccc|gggg|tttt' , re . IGNORECASE ) while True : codes = [ rand_seq ( codeLen ) ] if ( hpf . search ( codes [ 0 ] ) is None ) : break idx = 0 while len ( codes ) < nIds : idx -= 1 if idx < - codeLen : idx = - 1 codes . append ( rand_seq ( codeLen ) ) else : nc = next_code ( rand_base ( ) , codes [ - 1 ] , idx ) if hpf . search ( nc ) is None : codes . append ( nc ) codes = list ( set ( codes ) ) return codes\"], ['Given a sample ID and a mapping modify a Sanger FASTA file to include the barcode and primer in the sequence data and change the description line as needed .', \"def scrobble_data_dir ( dataDir , sampleMap , outF , qualF = None , idopt = None , utf16 = False ) : seqcount = 0 outfiles = [ osp . split ( outF . name ) [ 1 ] ] if qualF : outfiles . append ( osp . split ( qualF . name ) [ 1 ] ) for item in os . listdir ( dataDir ) : if item in outfiles or not osp . isfile ( os . path . join ( dataDir , item ) ) : continue if osp . splitext ( item ) [ 1 ] in file_types [ 'fasta' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'fasta' ) for record in records : if isinstance ( idopt , tuple ) : sep , field = idopt sampleID = record . id . split ( sep ) [ field - 1 ] else : sampleID = osp . splitext ( item ) [ 0 ] record . seq = ( sampleMap [ sampleID ] . barcode + sampleMap [ sampleID ] . primer + record . seq ) SeqIO . write ( record , outF , 'fasta' ) seqcount += 1 fh . close ( ) elif qualF and osp . splitext ( item ) [ 1 ] in file_types [ 'qual' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'qual' ) for record in records : mi = sampleMap [ sampleMap . keys ( ) [ 0 ] ] quals = [ 40 for _ in range ( len ( mi . barcode ) + len ( mi . primer ) ) ] record . letter_annotations [ 'phred_quality' ] [ 0 : 0 ] = quals SeqIO . write ( record , qualF , 'qual' ) fh . close ( ) return seqcount\"], ['Uses the built - in argparse module to handle command - line options for the program .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Convert Sanger-sequencing \\\\                                     derived data files for use with the \\\\                                     metagenomics analysis program QIIME, by \\\\                                     extracting Sample ID information, adding\\\\                                     barcodes and primers to the sequence \\\\                                     data, and outputting a mapping file and\\\\                                     single FASTA-formatted sequence file \\\\                                     formed by concatenating all input data.\" ) parser . add_argument ( \\'-i\\' , \\'--input_dir\\' , required = True , help = \"The directory containing sequence data files. \\\\                              Assumes all data files are placed in this \\\\                              directory. For files organized within folders by\\\\                              sample, use -s in addition.\" ) parser . add_argument ( \\'-m\\' , \\'--map_file\\' , default = \\'map.txt\\' , help = \"QIIME-formatted mapping file linking Sample IDs \\\\                              with barcodes and primers.\" ) parser . add_argument ( \\'-o\\' , \\'--output\\' , default = \\'output.fasta\\' , metavar = \\'OUTPUT_FILE\\' , help = \"Single file containing all sequence data found \\\\                              in input_dir, FASTA-formatted with barcode and \\\\                              primer preprended to sequence. If the -q option \\\\                              is passed, any quality data will also be output \\\\                              to a single file of the same name with a .qual \\\\                              extension.\" ) parser . add_argument ( \\'-b\\' , \\'--barcode_length\\' , type = int , default = 12 , help = \"Length of the generated barcode sequences. \\\\                              Default is 12 (QIIME default), minimum is 8.\" ) parser . add_argument ( \\'-q\\' , \\'--qual\\' , action = \\'store_true\\' , default = False , help = \"Instruct the program to look for quality \\\\                              input files\" ) parser . add_argument ( \\'-u\\' , \\'--utf16\\' , action = \\'store_true\\' , default = False , help = \"UTF-16 encoded input files\" ) parser . add_argument ( \\'-t\\' , \\'--treatment\\' , help = \"Inserts an additional column into the mapping \\\\                              file specifying some treatment or other variable\\\\                              that separates the current set of sequences \\\\                              from any other set of seqeunces. For example:\\\\                              -t DiseaseState=healthy\" ) sidGroup = parser . add_mutually_exclusive_group ( required = True ) sidGroup . add_argument ( \\'-d\\' , \\'--identifier_pattern\\' , action = ValidateIDPattern , nargs = 2 , metavar = ( \\'SEPARATOR\\' , \\'FIELD_NUMBER\\' ) , help = \"Indicates how to extract the Sample ID from \\\\                               the description line. Specify two things: \\\\                               1. Field separator, 2. Field number of Sample \\\\                               ID (1 or greater). If the separator is a space \\\\                               or tab, use \\\\s or \\\\\\\\t respectively. \\\\                               Example: >ka-SampleID-2091, use -i - 2, \\\\                               indicating - is the separator and the Sample ID\\\\                               is field #2.\" ) sidGroup . add_argument ( \\'-f\\' , \\'--filename_sample_id\\' , action = \\'store_true\\' , default = False , help = \\'Specify that the program should\\\\                          the name of each fasta file as the Sample ID for use\\\\                          in the mapping file. This is meant to be used when \\\\                          all sequence data for a sample is stored in a single\\\\                          file.\\' ) return parser . parse_args ( )'], ['Applies the arcsine square root transform to the given BIOM - format table', 'def arcsin_sqrt ( biom_tbl ) : arcsint = lambda data , id_ , md : np . arcsin ( np . sqrt ( data ) ) tbl_relabd = relative_abd ( biom_tbl ) tbl_asin = tbl_relabd . transform ( arcsint , inplace = False ) return tbl_asin'], ['parse sam file and check mapping quality', \"def parse_sam ( sam , qual ) : for line in sam : if line . startswith ( '@' ) : continue line = line . strip ( ) . split ( ) if int ( line [ 4 ] ) == 0 or int ( line [ 4 ] ) < qual : continue yield line\"], ['reverse completement stats', \"def rc_stats ( stats ) : rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } rcs = [ ] for pos in reversed ( stats ) : rc = { } rc [ 'reference frequencey' ] = pos [ 'reference frequency' ] rc [ 'consensus frequencey' ] = pos [ 'consensus frequency' ] rc [ 'In' ] = pos [ 'In' ] rc [ 'Del' ] = pos [ 'Del' ] rc [ 'ref' ] = rc_nucs [ pos [ 'ref' ] ] rc [ 'consensus' ] = ( rc_nucs [ pos [ 'consensus' ] [ 0 ] ] , pos [ 'consensus' ] [ 1 ] ) for base , stat in list ( pos . items ( ) ) : if base in rc_nucs : rc [ rc_nucs [ base ] ] = stat rcs . append ( rc ) return rcs\"], ['parse codon nucleotide positions in range start - > end wrt strand', 'def parse_codons ( ref , start , end , strand ) : codon = [ ] c = cycle ( [ 1 , 2 , 3 ] ) ref = ref [ start - 1 : end ] if strand == - 1 : ref = rc_stats ( ref ) for pos in ref : n = next ( c ) codon . append ( pos ) if n == 3 : yield codon codon = [ ]'], ['calculate coverage for positions in range start - > end', 'def calc_coverage ( ref , start , end , length , nucs ) : ref = ref [ start - 1 : end ] bases = 0 for pos in ref : for base , count in list ( pos . items ( ) ) : if base in nucs : bases += count return float ( bases ) / float ( length )'], ['parse gbk file', \"def parse_gbk ( gbks ) : for gbk in gbks : for record in SeqIO . parse ( open ( gbk ) , 'genbank' ) : for feature in record . features : if feature . type == 'gene' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : continue if feature . type == 'CDS' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : pass start = int ( feature . location . start ) + int ( feature . qualifiers [ 'codon_start' ] [ 0 ] ) end , strand = int ( feature . location . end ) , feature . location . strand if strand is None : strand = 1 else : strand = - 1 contig = record . id yield contig , [ locus , [ start , end , strand ] , feature . qualifiers ]\"], ['parse gene call information from Prodigal fasta output', \"def parse_fasta_annotations ( fastas , annot_tables , trans_table ) : if annot_tables is not False : annots = { } for table in annot_tables : for cds in open ( table ) : ID , start , end , strand = cds . strip ( ) . split ( ) annots [ ID ] = [ start , end , int ( strand ) ] for fasta in fastas : for seq in parse_fasta ( fasta ) : if ( '# ;gc_cont' not in seq [ 0 ] and '# ID=' not in seq [ 0 ] ) and annot_tables is False : print ( '# specify fasta from Prodigal or annotations table (-t)' , file = sys . stderr ) exit ( ) if 'ID=' in seq [ 0 ] : ID = seq [ 0 ] . rsplit ( 'ID=' , 1 ) [ 1 ] . split ( ';' , 1 ) [ 0 ] contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_%s' % ( ID ) , 1 ) [ 0 ] else : contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_' , 1 ) [ 0 ] locus = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] if ( '# ;gc_cont' in seq [ 0 ] or '# ID=' in seq [ 0 ] ) : info = seq [ 0 ] . split ( ' # ' ) start , end , strand = int ( info [ 1 ] ) , int ( info [ 2 ] ) , info [ 3 ] if strand == '1' : strand = 1 else : strand = - 1 product = [ '' . join ( info [ 4 ] . split ( ) [ 1 : ] ) ] else : start , end , strand = annots [ locus ] product = seq [ 0 ] . split ( ' ' , 1 ) [ 1 ] info = { 'transl_table' : [ trans_table ] , 'translation' : [ seq [ 1 ] ] , 'product' : product } yield contig , [ locus , [ start , end , strand ] , info ]\"], ['parse annotations in either gbk or Prodigal fasta format', 'def parse_annotations ( annots , fmt , annot_tables , trans_table ) : annotations = { } if fmt is False : for contig , feature in parse_gbk ( annots ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) else : for contig , feature in parse_fasta_annotations ( annots , annot_tables , trans_table ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) return annotations'], ['convert codon to amino acid', \"def codon2aa ( codon , trans_table ) : return Seq ( '' . join ( codon ) , IUPAC . ambiguous_dna ) . translate ( table = trans_table ) [ 0 ]\"], ['find consensus base based on nucleotide frequencies', \"def find_consensus ( bases ) : nucs = [ 'A' , 'T' , 'G' , 'C' , 'N' ] total = sum ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) try : top = max ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) except : bases [ 'consensus' ] = ( 'N' , 'n/a' ) bases [ 'consensus frequency' ] = 'n/a' bases [ 'reference frequency' ] = 'n/a' return bases top = [ ( nuc , bases [ nuc ] ) for nuc in bases if bases [ nuc ] == top ] if top [ 0 ] [ 1 ] == 0 : bases [ 'consensus' ] = ( 'n/a' , 0 ) else : bases [ 'consensus' ] = random . choice ( top ) if total == 0 : c_freq = 'n/a' ref_freq = 'n/a' else : c_freq = float ( bases [ 'consensus' ] [ 1 ] ) / float ( total ) if bases [ 'ref' ] not in bases : ref_freq = 0 else : ref_freq = float ( bases [ bases [ 'ref' ] ] ) / float ( total ) bases [ 'consensus frequency' ] = c_freq bases [ 'reference frequency' ] = ref_freq return bases\"], ['print consensensus sequences for each genome and sample', \"def print_consensus ( genomes ) : cons = { } for genome , contigs in list ( genomes . items ( ) ) : cons [ genome ] = { } for contig , samples in list ( contigs . items ( ) ) : for sample , stats in list ( samples . items ( ) ) : if sample not in cons [ genome ] : cons [ genome ] [ sample ] = { } seq = cons [ genome ] [ sample ] [ contig ] = [ ] for pos , ps in enumerate ( stats [ 'bp_stats' ] , 1 ) : ref , consensus = ps [ 'ref' ] , ps [ 'consensus' ] [ 0 ] if consensus == 'n/a' : consensus = ref . lower ( ) seq . append ( consensus ) for genome , samples in cons . items ( ) : for sample , contigs in samples . items ( ) : fn = '%s.%s.consensus.fa' % ( genome , sample ) f = open ( fn , 'w' ) for contig , seq in contigs . items ( ) : print ( '>%s' % ( contig ) , file = f ) print ( '' . join ( seq ) , file = f ) f . close ( ) return cons\"], ['calculate genome coverage from scaffold coverage table', \"def parse_cov ( cov_table , scaffold2genome ) : size = { } mapped = { } for line in open ( cov_table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : samples = line [ 1 : ] samples = [ i . rsplit ( '/' , 1 ) [ - 1 ] . split ( '.' , 1 ) [ 0 ] for i in samples ] continue scaffold , length = line [ 0 ] . split ( ': ' ) length = float ( length ) covs = [ float ( i ) for i in line [ 1 : ] ] bases = [ c * length for c in covs ] if scaffold not in scaffold2genome : continue genome = scaffold2genome [ scaffold ] if genome not in size : size [ genome ] = 0 mapped [ genome ] = { sample : 0 for sample in samples } size [ genome ] += length for sample , count in zip ( samples , bases ) : mapped [ genome ] [ sample ] += count coverage = { 'genome' : [ ] , 'genome size (bp)' : [ ] , 'sample' : [ ] , 'coverage' : [ ] } for genome , length in size . items ( ) : for sample in samples : cov = mapped [ genome ] [ sample ] / length coverage [ 'genome' ] . append ( genome ) coverage [ 'genome size (bp)' ] . append ( length ) coverage [ 'sample' ] . append ( sample ) coverage [ 'coverage' ] . append ( cov ) return pd . DataFrame ( coverage )\"], ['calculate genome coverage from scaffold coverage', 'def genome_coverage ( covs , s2b ) : COV = [ ] for cov in covs : COV . append ( parse_cov ( cov , s2b ) ) return pd . concat ( COV )'], ['convert s2b files to dictionary', \"def parse_s2bs ( s2bs ) : s2b = { } for s in s2bs : for line in open ( s ) : line = line . strip ( ) . split ( '\\\\t' ) s , b = line [ 0 ] , line [ 1 ] s2b [ s ] = b return s2b\"], ['convert fastas to s2b dictionary', \"def fa2s2b ( fastas ) : s2b = { } for fa in fastas : for seq in parse_fasta ( fa ) : s = seq [ 0 ] . split ( '>' , 1 ) [ 1 ] . split ( ) [ 0 ] s2b [ s ] = fa . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 1 ) [ 0 ] return s2b\"], ['Filters out sequences with too much ambiguity as defined by the method parameters .', \"def filter_ambiguity ( records , percent = 0.5 ) : seqs = [ ] count = 0 for record in records : if record . seq . count ( 'N' ) / float ( len ( record ) ) < percent : seqs . append ( record ) count += 1 return seqs , count\"], ['Search package .', 'def package_existent ( name ) : try : response = requests . get ( PYPI_URL . format ( name ) ) if response . ok : msg = ( \\'[error] \"{0}\" is registered already in PyPI.\\\\n\\' \\'\\\\tSpecify another package name.\\' ) . format ( name ) raise Conflict ( msg ) except ( socket . gaierror , Timeout , ConnectionError , HTTPError ) as exc : raise BackendFailure ( exc )'], ['add index to id to make it unique wrt ids', \"def append_index_id ( id , ids ) : index = 1 mod = '%s_%s' % ( id , index ) while mod in ids : index += 1 mod = '%s_%s' % ( id , index ) ids . append ( mod ) return mod , ids\"], ['de - replicate fastas based on sequence names', \"def de_rep ( fastas , append_index , return_original = False ) : ids = [ ] for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) id = header [ 0 ] if id not in ids : ids . append ( id ) if return_original is True : yield [ header , seq ] else : yield seq elif append_index == True : new , ids = append_index_id ( id , ids ) if return_original is True : yield [ header , [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ] ] else : yield [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ]\"], ['Request data associated with postcode .', \"def get ( postcode ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) url = '%s/postcode/%s.json' % ( END_POINT , postcode ) return _get_json_resp ( url )\"], ['Request all postcode data within distance miles of postcode .', \"def get_from_postcode ( postcode , distance ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) return _get_from ( distance , 'postcode=%s' % postcode )\"], ['Checks if latitude and longitude correct', 'def _check_point ( self , lat , lng ) : if abs ( lat ) > 90 or abs ( lng ) > 180 : msg = \"Illegal lat and/or lng, (%s, %s) provided.\" % ( lat , lng ) raise IllegalPointException ( msg )'], ['Checks for cached responses before requesting from web - service', 'def _lookup ( self , skip_cache , fun , * args , ** kwargs ) : if args not in self . cache or skip_cache : self . cache [ args ] = fun ( * args , ** kwargs ) return self . cache [ args ]'], ['Calls postcodes . get_nearest but checks correctness of lat and long and by default utilises a local cache .', 'def get_nearest ( self , lat , lng , skip_cache = False ) : lat , lng = float ( lat ) , float ( lng ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_nearest , lat , lng )'], ['Calls postcodes . get_from_postcode but checks correctness of distance and by default utilises a local cache .', 'def get_from_postcode ( self , postcode , distance , skip_cache = False ) : distance = float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) postcode = postcode . lower ( ) . replace ( \\' \\' , \\'\\' ) return self . _lookup ( skip_cache , get_from_postcode , postcode , float ( distance ) )'], ['Calls postcodes . get_from_geo but checks the correctness of all arguments and by default utilises a local cache .', 'def get_from_geo ( self , lat , lng , distance , skip_cache = False ) : lat , lng , distance = float ( lat ) , float ( lng ) , float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_from_geo , lat , lng , distance )'], ['get coordinates of insertions from insertion - masked sequence', 'def insertions_from_masked ( seq ) : insertions = [ ] prev = True for i , base in enumerate ( seq ) : if base . isupper ( ) and prev is True : insertions . append ( [ ] ) prev = False elif base . islower ( ) : insertions [ - 1 ] . append ( i ) prev = True return [ [ min ( i ) , max ( i ) ] for i in insertions if i != [ ] ]'], ['get insertion information from header', \"def seq_info ( names , id2names , insertions , sequences ) : seqs = { } for name in names : id = id2names [ name ] gene = name . split ( 'fromHMM::' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ 1 ] model = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( '=' , 1 ) [ 1 ] . split ( ) [ 0 ] i_gene_pos = insertions [ id ] i_model_pos = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( 'model-pos(ins-len)=' ) [ 1 ] . split ( ) [ 0 ] . split ( ';' ) i_info = [ ] for i , ins in enumerate ( i_gene_pos ) : model_pos = i_model_pos [ i ] . split ( '-' ) [ 1 ] . split ( '(' ) [ 0 ] length = i_model_pos [ i ] . split ( '(' ) [ 1 ] . split ( ')' ) [ 0 ] iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s' % ( id , ( i + 1 ) , ( i + 1 ) , ins [ 0 ] , ins [ 1 ] , model_pos ) iseq = sequences [ id ] [ 1 ] [ ins [ 0 ] : ( ins [ 1 ] + 1 ) ] iseq = [ iheader , iseq ] info = [ ins , model_pos , length , iseq , [ ] , [ ] ] i_info . append ( info ) seqs [ id ] = [ gene , model , i_info ] return seqs\"], ['make sure thresh % feature is contained within insertion', 'def check_overlap ( pos , ins , thresh ) : ins_pos = ins [ 0 ] ins_len = ins [ 2 ] ol = overlap ( ins_pos , pos ) feat_len = pos [ 1 ] - pos [ 0 ] + 1 if float ( ol ) / float ( feat_len ) >= thresh : return True return False'], ['length of largest insertion', 'def max_insertion ( seqs , gene , domain ) : seqs = [ i [ 2 ] for i in list ( seqs . values ( ) ) if i [ 2 ] != [ ] and i [ 0 ] == gene and i [ 1 ] == domain ] lengths = [ ] for seq in seqs : for ins in seq : lengths . append ( int ( ins [ 2 ] ) ) if lengths == [ ] : return 100 return max ( lengths )'], ['get length of model', \"def model_length ( gene , domain ) : if gene == '16S' : domain2max = { 'E_coli_K12' : int ( 1538 ) , 'bacteria' : int ( 1689 ) , 'archaea' : int ( 1563 ) , 'eukarya' : int ( 2652 ) } return domain2max [ domain ] elif gene == '23S' : domain2max = { 'E_coli_K12' : int ( 2903 ) , 'bacteria' : int ( 3146 ) , 'archaea' : int ( 3774 ) , 'eukarya' : int ( 9079 ) } return domain2max [ domain ] else : print ( sys . stderr , '# length unknown for gene: %s, domain: %s' % ( gene , domain ) ) exit ( )\"], ['setup unique marker for every orf annotation - change size if necessary', \"def setup_markers ( seqs ) : family2marker = { } markers = cycle ( [ '^' , 'p' , '*' , '+' , 'x' , 'd' , '|' , 'v' , '>' , '<' , '8' ] ) size = 60 families = [ ] for seq in list ( seqs . values ( ) ) : for insertion in seq [ 2 ] : for family in list ( insertion [ - 1 ] . values ( ) ) : if family not in families : families . append ( family ) for family in families : marker = next ( markers ) if marker == '^' : size = size * 0.5 family2marker [ family ] = [ marker , size ] return family2marker\"], ['plot insertions for each gene and domain', 'def plot_by_gene_and_domain ( name , seqs , tax , id2name ) : for gene in set ( [ seq [ 0 ] for seq in list ( seqs . values ( ) ) ] ) : for domain in set ( [ seq [ 1 ] for seq in list ( seqs . values ( ) ) ] ) : plot_insertions ( name , seqs , gene , domain , tax , id2name )'], ['get the description for each ORF', \"def get_descriptions ( fastas ) : id2desc = { } for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ' ' ) id = header [ 0 ] if len ( header ) > 1 : desc = ' ' . join ( header [ 1 : ] ) else : desc = 'n/a' length = float ( len ( [ i for i in seq [ 1 ] . strip ( ) if i != '*' ] ) ) id2desc [ id ] = [ fasta , desc , length ] return id2desc\"], ['optimize later? slow ... should combine with calculate_threshold module', \"def print_genome_matrix ( hits , fastas , id2desc , file_name ) : out = open ( file_name , 'w' ) fastas = sorted ( fastas ) print ( '## percent identity between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : average = '-' else : average = numpy . average ( [ hits [ fasta ] [ other ] [ i ] [ 3 ] for i in hits [ fasta ] [ other ] ] ) line . append ( str ( average ) ) print ( '\\\\t' . join ( line ) , file = out ) print ( '' , file = out ) print ( '## percent of orfs that are orthologous between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : percent = '-' else : orthologs = float ( len ( hits [ fasta ] [ other ] ) ) orfs = float ( len ( [ i for i in id2desc if id2desc [ i ] [ 0 ] == fasta ] ) ) percent = float ( orthologs / orfs ) * 100 line . append ( str ( percent ) ) print ( '\\\\t' . join ( line ) , file = out )\"], ['compare genome to self to get the best possible bit score for each ORF', \"def self_compare ( fastas , id2desc , algorithm ) : for fasta in fastas : blast = open ( search ( fasta , fasta , method = algorithm , alignment = 'local' ) ) for hit in best_blast ( blast , 1 ) : id , bit = hit [ 0 ] . split ( ) [ 0 ] , float ( hit [ - 1 ] ) id2desc [ id ] . append ( bit ) return id2desc\"], ['if thresholds are not specififed calculate based on the distribution of normalized bit scores', \"def calc_thresholds ( rbh , file_name , thresholds = [ False , False , False , False ] , stdevs = 2 ) : calc_threshold = thresholds [ - 1 ] norm_threshold = { } for pair in itertools . permutations ( [ i for i in rbh ] , 2 ) : if pair [ 0 ] not in norm_threshold : norm_threshold [ pair [ 0 ] ] = { } norm_threshold [ pair [ 0 ] ] [ pair [ 1 ] ] = { } out = open ( file_name , 'w' ) print ( '#### summary of rbh comparisons\\\\n' , file = out ) comparisons = [ ] for genome in rbh : for compare in rbh [ genome ] : pair = '' . join ( sorted ( [ genome , compare ] ) ) if pair in comparisons : continue comparisons . append ( pair ) scores = { 'percent identity' : [ ] , 'e-value' : [ ] , 'bit score' : [ ] , 'normalized bit score' : [ ] , 'alignment length fraction' : [ ] } print ( '### blast between %s and %s\\\\n' % ( genome , compare ) , file = out ) for id in rbh [ genome ] [ compare ] : pident , length_fraction , e , bit , norm_bit = rbh [ genome ] [ compare ] [ id ] [ 3 : ] scores [ 'percent identity' ] . append ( pident ) scores [ 'alignment length fraction' ] . append ( length_fraction ) scores [ 'e-value' ] . append ( e ) scores [ 'bit score' ] . append ( bit ) scores [ 'normalized bit score' ] . append ( norm_bit ) if calc_threshold is True : norms = scores [ 'normalized bit score' ] average = numpy . average ( norms ) std = numpy . std ( norms ) normal_thresh = average - ( std * stdevs ) print ( '## average normalized bit score: %s' % average , file = out ) print ( '## standard deviation of normalized bit scores: %s' % std , file = out ) print ( '## normalized bit score threshold set to: %s\\\\n' % ( normal_thresh ) , file = out ) norm_threshold [ genome ] [ compare ] , norm_threshold [ compare ] [ genome ] = normal_thresh , normal_thresh for score in scores : print ( '## %s' % ( score ) , file = out ) if len ( scores [ score ] ) > 0 : print ( '## average: %s' % numpy . average ( scores [ score ] ) , file = out ) print ( '' , file = out ) out . close ( ) if calc_threshold is True : return thresholds [ 0 : - 1 ] + [ norm_threshold ] else : return thresholds\"], ['make and split a rbh network', \"def neto ( fastas , algorithm = 'usearch' , e = 0.01 , bit = 40 , length = .65 , norm_bit = False ) : thresholds = [ e , bit , length , norm_bit ] id2desc = get_descriptions ( fastas ) id2desc = self_compare ( fastas , id2desc , algorithm ) hits = compare_genomes ( fastas , id2desc , algorithm ) calc_thresholds ( hits , file_name = 'fbh.scores.summary.txt' ) rbh_network ( id2desc , hits , file_name = 'fbh.network.edges.txt' ) hits , rbh = find_rbh ( hits , id2desc ) thresholds = calc_thresholds ( rbh , 'rbh.scores.summary.txt' , thresholds ) g = rbh_network ( id2desc , rbh , file_name = 'rbh.network.edges.txt' ) filtered_g , filtered_rbh = rbh_network ( id2desc , rbh , 'rbh.filtered.network.edges.txt' , thresholds ) calc_thresholds ( filtered_rbh , file_name = 'rbh.filtered.scores.summary.txt' ) print_summary ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.nodes.txt' ) print_network_matrix ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.matrix.txt' ) print_genome_matrix ( filtered_rbh , fastas , id2desc , file_name = 'rbh.filtered.network.genome_matrix.txt' ) split_g = split_network ( filtered_g , id2desc , file_name = 'rbh.filtered.split.network.edges.txt' ) print_summary ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.nodes.txt' ) print_network_matrix ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.matrix.txt' ) return split_g\"], ['Collapses multiple dimensions into a single raster_info complex struct', \"def _parse_raster_info ( self , prop = RASTER_INFO ) : raster_info = { } . fromkeys ( _iso_definitions [ prop ] , u'' ) raster_info [ 'dimensions' ] = get_default_for_complex_sub ( prop = prop , subprop = 'dimensions' , value = parse_property ( self . _xml_tree , None , self . _data_map , '_ri_num_dims' ) , xpath = self . _data_map [ '_ri_num_dims' ] ) xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] for dimension in parse_complex_list ( self . _xml_tree , xpath_root , xpath_map , RASTER_DIMS ) : dimension_type = dimension [ 'type' ] . lower ( ) if dimension_type == 'vertical' : raster_info [ 'vertical_count' ] = dimension [ 'size' ] elif dimension_type == 'column' : raster_info [ 'column_count' ] = dimension [ 'size' ] raster_info [ 'x_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) elif dimension_type == 'row' : raster_info [ 'row_count' ] = dimension [ 'size' ] raster_info [ 'y_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) return raster_info if any ( raster_info [ k ] for k in raster_info ) else { }\"], ['Derives multiple dimensions from a single raster_info complex struct', \"def _update_raster_info ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = update_props . pop ( 'values' ) xroot , xpath = None , self . _data_map [ '_ri_num_dims' ] raster_info = [ update_property ( tree_to_update , xroot , xpath , prop , values . get ( 'dimensions' , u'' ) ) ] xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] v_dimension = { } if values . get ( 'vertical_count' ) : v_dimension = v_dimension . fromkeys ( xpath_map , u'' ) v_dimension [ 'type' ] = 'vertical' v_dimension [ 'size' ] = values . get ( 'vertical_count' , u'' ) x_dimension = { } if values . get ( 'column_count' ) or values . get ( 'x_resolution' ) : x_dimension = x_dimension . fromkeys ( xpath_map , u'' ) x_dimension [ 'type' ] = 'column' x_dimension [ 'size' ] = values . get ( 'column_count' , u'' ) x_dimension [ 'value' ] = values . get ( 'x_resolution' , u'' ) y_dimension = { } if values . get ( 'row_count' ) or values . get ( 'y_resolution' ) : y_dimension = y_dimension . fromkeys ( xpath_map , u'' ) y_dimension [ 'type' ] = 'row' y_dimension [ 'size' ] = values . get ( 'row_count' , u'' ) y_dimension [ 'value' ] = values . get ( 'y_resolution' , u'' ) update_props [ 'prop' ] = RASTER_DIMS update_props [ 'values' ] = [ v_dimension , x_dimension , y_dimension ] raster_info += update_complex_list ( xpath_root = xpath_root , xpath_map = xpath_map , ** update_props ) return raster_info\"], ['Removes primitive type tags from an XPATH', 'def _trim_xpath ( self , xpath , prop ) : xroot = self . _get_xroot_for ( prop ) if xroot is None and isinstance ( xpath , string_types ) : xtags = xpath . split ( XPATH_DELIM ) if xtags [ - 1 ] in _iso_tag_primitives : xroot = XPATH_DELIM . join ( xtags [ : - 1 ] ) return xroot'], ['Generates the app id for a given shortcut . Steam uses app ids as a unique identifier for games but since shortcuts dont have a canonical serverside representation they need to be generated on the fly . The important part about this function is that it will generate the same app id as Steam does for a given shortcut', \"def shortcut_app_id ( shortcut ) : algorithm = Crc ( width = 32 , poly = 0x04C11DB7 , reflect_in = True , xor_in = 0xffffffff , reflect_out = True , xor_out = 0xffffffff ) crc_input = '' . join ( [ shortcut . exe , shortcut . name ] ) high_32 = algorithm . bit_by_bit ( crc_input ) | 0x80000000 full_64 = ( high_32 << 32 ) | 0x02000000 return str ( full_64 )\"], ['Execute git config .', \"def _config ( self ) : cfg_wr = self . repo . config_writer ( ) cfg_wr . add_section ( 'user' ) cfg_wr . set_value ( 'user' , 'name' , self . metadata . author ) cfg_wr . set_value ( 'user' , 'email' , self . metadata . email ) cfg_wr . release ( )\"], ['Execute git remote add .', \"def _remote_add ( self ) : self . repo . create_remote ( 'origin' , 'git@github.com:{username}/{repo}.git' . format ( username = self . metadata . username , repo = self . metadata . name ) )\"], ['Starts execution of the script', 'def start ( self ) : try : self . args . func ( ) except SystemExit as e : if e . code != 0 : raise except KeyboardInterrupt : self . log . warning ( \"exited via keyboard interrupt\" ) except : self . log . exception ( \"exited start function\" ) finally : self . _flush_metrics_q . put ( None , block = True ) self . _flush_metrics_q . put ( None , block = True , timeout = 1 ) self . log . debug ( \"exited_successfully\" )'], ['Define basic command - line arguments required by the script .', 'def define_baseargs ( self , parser ) : parser . add_argument ( \\'--name\\' , default = sys . argv [ 0 ] , help = \\'Name to identify this instance\\' ) parser . add_argument ( \\'--log-level\\' , default = None , help = \\'Logging level as picked from the logging module\\' ) parser . add_argument ( \\'--log-format\\' , default = None , choices = ( \"json\" , \"pretty\" , ) , help = ( \"Force the format of the logs. By default, if the \" \"command is from a terminal, print colorful logs. \" \"Otherwise print json.\" ) , ) parser . add_argument ( \\'--log-file\\' , default = None , help = \\'Writes logs to log file if specified, default: %(default)s\\' , ) parser . add_argument ( \\'--quiet\\' , default = False , action = \"store_true\" , help = \\'if true, does not print logs to stderr, default: %(default)s\\' , ) parser . add_argument ( \\'--metric-grouping-interval\\' , default = None , type = int , help = \\'To group metrics based on time interval ex:10 i.e;(10 sec)\\' , ) parser . add_argument ( \\'--debug\\' , default = False , action = \"store_true\" , help = \\'To run the code in debug mode\\' , )'], ['Basically turns payload that looks like \\\\\\\\ n to . In the calling function if this function returns no object is added for that payload .', \"def cleanup_payload ( self , payload ) : p = payload . replace ( '\\\\n' , '' ) p = p . rstrip ( ) p = p . lstrip ( ) return p\"], ['Ensures complex property types have the correct default values', \"def get_default_for ( prop , value ) : prop = prop . strip ( '_' ) val = reduce_value ( value ) if prop in _COMPLEX_LISTS : return wrap_value ( val ) elif prop in _COMPLEX_STRUCTS : return val or { } else : return u'' if val is None else val\"], ['Either update the tree the default way or call the custom updater', \"def update_property ( tree_to_update , xpath_root , xpaths , prop , values , supported = None ) : if supported and prop . startswith ( '_' ) and prop . strip ( '_' ) in supported : values = u'' else : values = get_default_for ( prop , values ) if not xpaths : return [ ] elif not isinstance ( xpaths , ParserProperty ) : return _update_property ( tree_to_update , xpath_root , xpaths , values ) else : return xpaths . set_prop ( tree_to_update = tree_to_update , prop = prop , values = values )\"], ['Default update operation for a single parser property . If xpaths contains one xpath then one element per value will be inserted at that location in the tree_to_update ; otherwise the number of values must match the number of xpaths .', \"def _update_property ( tree_to_update , xpath_root , xpaths , values ) : def update_element ( elem , idx , root , path , vals ) : has_root = bool ( root and len ( path ) > len ( root ) and path . startswith ( root ) ) path , attr = get_xpath_tuple ( path ) if attr : removed = [ get_element ( elem , path ) ] remove_element_attributes ( removed [ 0 ] , attr ) elif not has_root : removed = wrap_value ( remove_element ( elem , path ) ) else : path = get_xpath_branch ( root , path ) removed = [ ] if idx != 0 else [ remove_element ( e , path , True ) for e in get_elements ( elem , root ) ] if not vals : return removed items = [ ] for i , val in enumerate ( wrap_value ( vals ) ) : elem_to_update = elem if has_root : elem_to_update = insert_element ( elem , ( i + idx ) , root ) val = val . decode ( 'utf-8' ) if not isinstance ( val , string_types ) else val if not attr : items . append ( insert_element ( elem_to_update , i , path , val ) ) elif path : items . append ( insert_element ( elem_to_update , i , path , ** { attr : val } ) ) else : set_element_attributes ( elem_to_update , ** { attr : val } ) items . append ( elem_to_update ) return items xpaths = reduce_value ( xpaths ) values = filter_empty ( values ) if isinstance ( xpaths , string_types ) : return update_element ( tree_to_update , 0 , xpath_root , xpaths , values ) else : each = [ ] for index , xpath in enumerate ( xpaths ) : value = values [ index ] if values else None each . extend ( update_element ( tree_to_update , index , xpath_root , xpath , value ) ) return each\"], ['Default validation for single complex data structure', \"def validate_complex ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for complex_prop , complex_val in iteritems ( value ) : complex_key = '.' . join ( ( prop , complex_prop ) ) if complex_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) validate_type ( complex_key , complex_val , ( string_types , list ) )\"], ['Default validation for Attribute Details data structure', \"def validate_complex_list ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for idx , complex_struct in enumerate ( wrap_value ( value ) ) : cs_idx = prop + '[' + str ( idx ) + ']' validate_type ( cs_idx , complex_struct , dict ) for cs_prop , cs_val in iteritems ( complex_struct ) : cs_key = '.' . join ( ( cs_idx , cs_prop ) ) if cs_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) if not isinstance ( cs_val , list ) : validate_type ( cs_key , cs_val , ( string_types , list ) ) else : for list_idx , list_val in enumerate ( cs_val ) : list_prop = cs_key + '[' + str ( list_idx ) + ']' validate_type ( list_prop , list_val , string_types )\"], ['Default validation for Date Types data structure', \"def validate_dates ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) date_keys = set ( value ) if date_keys : if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys : if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = _complex_definitions [ DATES ] if xpath_map is None else xpath_map _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) date_type = value [ DATE_TYPE ] if date_type not in DATE_TYPES : _validation_error ( 'dates.type' , None , date_type , DATE_TYPES ) date_vals = value [ DATE_VALUES ] validate_type ( 'dates.values' , date_vals , list ) dates_len = len ( date_vals ) if date_type == DATE_TYPE_MISSING and dates_len != 0 : _validation_error ( 'len(dates.values)' , None , dates_len , 0 ) if date_type == DATE_TYPE_SINGLE and dates_len != 1 : _validation_error ( 'len(dates.values)' , None , dates_len , 1 ) if date_type == DATE_TYPE_RANGE and dates_len != 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 2 ) if date_type == DATE_TYPE_MULTIPLE and dates_len < 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 'at least two' ) for idx , date in enumerate ( date_vals ) : date_key = 'dates.value[' + str ( idx ) + ']' validate_type ( date_key , date , string_types )\"], ['Default validation for Process Steps data structure', \"def validate_process_steps ( prop , value ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) procstep_keys = set ( _complex_definitions [ prop ] ) for idx , procstep in enumerate ( wrap_value ( value ) ) : ps_idx = prop + '[' + str ( idx ) + ']' validate_type ( ps_idx , procstep , dict ) for ps_prop , ps_val in iteritems ( procstep ) : ps_key = '.' . join ( ( ps_idx , ps_prop ) ) if ps_prop not in procstep_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( procstep_keys ) ) ) ) if ps_prop != 'sources' : validate_type ( ps_key , ps_val , string_types ) else : validate_type ( ps_key , ps_val , ( string_types , list ) ) for src_idx , src_val in enumerate ( wrap_value ( ps_val ) ) : src_key = ps_key + '[' + str ( src_idx ) + ']' validate_type ( src_key , src_val , string_types )\"], ['Default validation for all types', 'def validate_type ( prop , value , expected ) : if value is not None and not isinstance ( value , expected ) : _validation_error ( prop , type ( value ) . __name__ , None , expected )'], ['Default validation for updated properties', \"def _validation_error ( prop , prop_type , prop_value , expected ) : if prop_type is None : attrib = 'value' assigned = prop_value else : attrib = 'type' assigned = prop_type raise ValidationError ( 'Invalid property {attrib} for {prop}:\\\\n\\\\t{attrib}: {assigned}\\\\n\\\\texpected: {expected}' , attrib = attrib , prop = prop , assigned = assigned , expected = expected , invalid = { prop : prop_value } if attrib == 'value' else { } )\"], ['Calls the getter with no arguments and returns its value', 'def get_prop ( self , prop ) : if self . _parser is None : raise ConfigurationError ( \\'Cannot call ParserProperty.\"get_prop\" with no parser configured\\' ) return self . _parser ( prop ) if prop else self . _parser ( )'], ['Returns a boolean representing whether these commands can be grouped together or not .', \"def can_group_commands ( command , next_command ) : multi_capable_commands = ( 'get' , 'set' , 'delete' ) if next_command is None : return False name = command . get_name ( ) if name not in multi_capable_commands : return False if name != next_command . get_name ( ) : return False if grouped_args_for_command ( command ) != grouped_args_for_command ( next_command ) : return False if command . get_kwargs ( ) != next_command . get_kwargs ( ) : return False return True\"], ['define ribosomal proteins and location of curated databases', \"def find_databases ( databases ) : proteins = [ 'L15' , 'L18' , 'L6' , 'S8' , 'L5' , 'L24' , 'L14' , 'S17' , 'L16' , 'S3' , 'L22' , 'S19' , 'L2' , 'L4' , 'L3' , 'S10' ] protein_databases = { 'L14' : 'rpL14_JGI_MDM.filtered.faa' , 'L15' : 'rpL15_JGI_MDM.filtered.faa' , 'L16' : 'rpL16_JGI_MDM.filtered.faa' , 'L18' : 'rpL18_JGI_MDM.filtered.faa' , 'L22' : 'rpL22_JGI_MDM.filtered.faa' , 'L24' : 'rpL24_JGI_MDM.filtered.faa' , 'L2' : 'rpL2_JGI_MDM.filtered.faa' , 'L3' : 'rpL3_JGI_MDM.filtered.faa' , 'L4' : 'rpL4_JGI_MDM.filtered.faa' , 'L5' : 'rpL5_JGI_MDM.filtered.faa' , 'L6' : 'rpL6_JGI_MDM.filtered.faa' , 'S10' : 'rpS10_JGI_MDM.filtered.faa' , 'S17' : 'rpS17_JGI_MDM.filtered.faa' , 'S19' : 'rpS19_JGI_MDM.filtered.faa' , 'S3' : 'rpS3_JGI_MDM.filtered.faa' , 'S8' : 'rpS8_JGI_MDM.filtered.faa' } protein_databases = { key : '%s/%s' % ( databases , database ) for key , database in list ( protein_databases . items ( ) ) } return proteins , protein_databases\"], ['which protein has the best hit the one to the right or to the left?', 'def find_next ( start , stop , i2hits ) : if start not in i2hits and stop in i2hits : index = stop elif stop not in i2hits and start in i2hits : index = start elif start not in i2hits and stop not in i2hits : index = choice ( [ start , stop ] ) i2hits [ index ] = [ [ False ] ] else : A , B = i2hits [ start ] [ 0 ] , i2hits [ stop ] [ 0 ] if B [ 10 ] <= A [ 10 ] : index = stop else : index = start if index == start : nstart = start - 1 nstop = stop else : nstop = stop + 1 nstart = start match = i2hits [ index ] [ 0 ] rp = match [ - 1 ] return index , nstart , nstop , rp , match'], ['determine which hits represent real ribosomal proteins identify each in syntenic block max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold', 'def find_ribosomal ( rps , scaffolds , s2rp , min_hits , max_hits_rp , max_errors ) : for scaffold , proteins in list ( s2rp . items ( ) ) : hits = { p : [ i for i in sorted ( hits , key = itemgetter ( 10 ) ) ] [ 0 : max_hits_rp ] for p , hits in list ( proteins . items ( ) ) if len ( hits ) > 0 } if len ( hits ) < min_hits : continue best = sorted ( [ hit [ 0 ] + [ p ] for p , hit in list ( hits . items ( ) ) ] , key = itemgetter ( 10 ) ) [ 0 ] block = find_block ( rps , scaffolds [ scaffold ] , hits , best , max_errors ) if ( len ( block ) - 1 ) >= min_hits : yield scaffold , block'], ['Parse the rep set file and remove all sequences not associated with unique OTUs .', 'def filter_rep_set ( inF , otuSet ) : seqs = [ ] for record in SeqIO . parse ( inF , \"fasta\" ) : if record . id in otuSet : seqs . append ( record ) return seqs'], ['Update the text for each element at the configured path if attribute matches', \"def _update_report_item ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = wrap_value ( update_props [ 'values' ] ) xroot = self . _get_xroot_for ( prop ) attr_key = 'type' attr_val = u'' if prop == 'attribute_accuracy' : attr_val = 'DQQuanAttAcc' elif prop == 'dataset_completeness' : attr_val = 'DQCompOm' for elem in get_elements ( tree_to_update , xroot ) : if get_element_attributes ( elem ) . get ( attr_key ) == attr_val : clear_element ( elem ) remove_empty_element ( tree_to_update , xroot ) attrs = { attr_key : attr_val } updated = [ ] for idx , value in enumerate ( values ) : elem = insert_element ( tree_to_update , idx , xroot , ** attrs ) updated . append ( insert_element ( elem , idx , 'measDesc' , value ) ) return updated\"], ['Clear the specified interrupt bit in the interrupt status register .', 'def _clear_interrupt ( self , intbit ) : int_status = self . _device . readU8 ( VCNL4010_INTSTAT ) int_status &= ~ intbit self . _device . write8 ( VCNL4010_INTSTAT , int_status )'], ['Swaps two nodes', 'def move ( self ) : a = random . randint ( 0 , len ( self . state ) - 1 ) b = random . randint ( 0 , len ( self . state ) - 1 ) self . state [ [ a , b ] ] = self . state [ [ b , a ] ]'], ['A bool - if the certificate should be self - signed .', 'def self_signed ( self , value ) : self . _self_signed = bool ( value ) if self . _self_signed : self . _issuer = None'], ['Grabs the first URL out of a asn1crypto . x509 . CRLDistributionPoints object', \"def _get_crl_url ( self , distribution_points ) : if distribution_points is None : return None for distribution_point in distribution_points : name = distribution_point [ 'distribution_point' ] if name . name == 'full_name' and name . chosen [ 0 ] . name == 'uniform_resource_identifier' : return name . chosen [ 0 ] . chosen . native return None\"], ['A bool - if the certificate should have the OCSP no check extension . Only applicable to certificates created for signing OCSP responses . Such certificates should normally be issued for a very short period of time since they are effectively whitelisted by clients .', 'def ocsp_no_check ( self , value ) : if value is None : self . _ocsp_no_check = None else : self . _ocsp_no_check = bool ( value )'], ['Removes empty line .', \"def emptylineless ( parser , token ) : nodelist = parser . parse ( ( 'endemptylineless' , ) ) parser . delete_first_token ( ) return EmptylinelessNode ( nodelist )\"], ['Do an HTTP PURGE of the given asset . The URL is run through urlparse and must point to the varnish instance not the varnishadm', \"def http_purge_url ( url ) : url = urlparse ( url ) connection = HTTPConnection ( url . hostname , url . port or 80 ) path = url . path or '/' connection . request ( 'PURGE' , '%s?%s' % ( path , url . query ) if url . query else path , '' , { 'Host' : '%s:%s' % ( url . hostname , url . port ) if url . port else url . hostname } ) response = connection . getresponse ( ) if response . status != 200 : logging . error ( 'Purge failed with status: %s' % response . status ) return response\"], ['Non - threaded batch command runner returning output results', \"def run ( addr , * commands , ** kwargs ) : results = [ ] handler = VarnishHandler ( addr , ** kwargs ) for cmd in commands : if isinstance ( cmd , tuple ) and len ( cmd ) > 1 : results . extend ( [ getattr ( handler , c [ 0 ] . replace ( '.' , '_' ) ) ( * c [ 1 : ] ) for c in cmd ] ) else : results . append ( getattr ( handler , cmd . replace ( '.' , '_' ) ) ( * commands [ 1 : ] ) ) break handler . close ( ) return results\"], ['add stylesheet files in HTML head', 'def add_stylesheets ( self , * css_files ) : for css_file in css_files : self . main_soup . style . append ( self . _text_file ( css_file ) )'], ['add javascripts files in HTML body', \"def add_javascripts ( self , * js_files ) : if self . main_soup . script is None : script_tag = self . main_soup . new_tag ( 'script' ) self . main_soup . body . append ( script_tag ) for js_file in js_files : self . main_soup . script . append ( self . _text_file ( js_file ) )\"], ['return the object in a file', \"def export ( self ) : with open ( self . export_url , 'w' , encoding = 'utf-8' ) as file : file . write ( self . build ( ) ) if self . open_browser : webbrowser . open_new_tab ( self . export_url )\"], ['convert Markdown text as html . return the html file as string', \"def build ( self ) : markdown_html = markdown . markdown ( self . markdown_text , extensions = [ TocExtension ( ) , 'fenced_code' , 'markdown_checklist.extension' , 'markdown.extensions.tables' ] ) markdown_soup = BeautifulSoup ( markdown_html , 'html.parser' ) if markdown_soup . find ( 'code' , attrs = { 'class' : 'mermaid' } ) : self . _add_mermaid_js ( ) for dot_tag in markdown_soup . find_all ( 'code' , attrs = { 'class' : 'dotgraph' } ) : grap_svg = self . _text_to_graphiz ( dot_tag . string ) graph_soup = BeautifulSoup ( grap_svg , 'html.parser' ) dot_tag . parent . replaceWith ( graph_soup ) self . main_soup . body . append ( markdown_soup ) return self . main_soup . prettify ( )\"], ['return the content of a file', \"def _text_file ( self , url ) : try : with open ( url , 'r' , encoding = 'utf-8' ) as file : return file . read ( ) except FileNotFoundError : print ( 'File `{}` not found' . format ( url ) ) sys . exit ( 0 )\"], ['create a graphviz graph from text', \"def _text_to_graphiz ( self , text ) : dot = Source ( text , format = 'svg' ) return dot . pipe ( ) . decode ( 'utf-8' )\"], ['add js libraries and css files of mermaid js_file', \"def _add_mermaid_js ( self ) : self . add_javascripts ( '{}/js/jquery-1.11.3.min.js' . format ( self . resources_path ) ) self . add_javascripts ( '{}/js/mermaid.min.js' . format ( self . resources_path ) ) self . add_stylesheets ( '{}/css/mermaid.css' . format ( self . resources_path ) ) self . main_soup . script . append ( 'mermaid.initialize({startOnLoad:true  });' )\"], ['Get a character set with individual members or ranges .', 'def getCharacterSet ( self ) : chars = u\\'\\' c = None cnt = 1 start = 0 while True : escaped_slash = False c = self . next ( ) if self . lookahead ( ) == u\\'-\\' and not c == u\\'\\\\\\\\\\' : f = c self . next ( ) c = self . next ( ) if not c or ( c in self . meta_chars ) : raise StringGenerator . SyntaxError ( u\"unexpected end of class range\" ) chars += self . getCharacterRange ( f , c ) elif c == u\\'\\\\\\\\\\' : if self . lookahead ( ) in self . meta_chars : c = self . next ( ) chars += c continue elif self . lookahead ( ) in self . string_code : c = self . next ( ) chars += self . string_code [ c ] elif c and c not in self . meta_chars : chars += c if c == u\\']\\' : if self . lookahead ( ) == u\\'{\\' : [ start , cnt ] = self . getQuantifier ( ) else : start = - 1 cnt = 1 break if c and c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped character in class definition: %s\" % c ) if not c : break return StringGenerator . CharacterSet ( chars , start , cnt )'], ['Get a sequence of non - special characters .', 'def getLiteral ( self ) : chars = u\\'\\' c = self . current ( ) while True : if c and c == u\"\\\\\\\\\" : c = self . next ( ) if c : chars += c continue elif not c or ( c in self . meta_chars ) : break else : chars += c if self . lookahead ( ) and self . lookahead ( ) in self . meta_chars : break c = self . next ( ) return StringGenerator . Literal ( chars )'], ['Get a sequence of nodes .', 'def getSequence ( self , level = 0 ) : seq = [ ] op = \\'\\' left_operand = None right_operand = None sequence_closed = False while True : c = self . next ( ) if not c : break if c and c not in self . meta_chars : seq . append ( self . getLiteral ( ) ) elif c and c == u\\'$\\' and self . lookahead ( ) == u\\'{\\' : seq . append ( self . getSource ( ) ) elif c == u\\'[\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getCharacterSet ( ) ) elif c == u\\'(\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getSequence ( level + 1 ) ) elif c == u\\')\\' and not self . last ( ) == u\\'\\\\\\\\\\' : if level == 0 : raise StringGenerator . SyntaxError ( u\"Extra closing parenthesis\" ) sequence_closed = True break elif c == u\\'|\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c elif c == u\\'&\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c else : if c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped special character: %s\" % c ) if op and not left_operand : if not seq or len ( seq ) < 1 : raise StringGenerator . SyntaxError ( u\"Operator: %s with no left operand\" % op ) left_operand = seq . pop ( ) elif op and len ( seq ) >= 1 and left_operand : right_operand = seq . pop ( ) if op == u\\'|\\' : seq . append ( StringGenerator . SequenceOR ( [ left_operand , right_operand ] ) ) elif op == u\\'&\\' : seq . append ( StringGenerator . SequenceAND ( [ left_operand , right_operand ] ) ) op = u\\'\\' left_operand = None right_operand = None if op : raise StringGenerator . SyntaxError ( u\"Operator: %s with no right operand\" % op ) if level > 0 and not sequence_closed : raise StringGenerator . SyntaxError ( u\"Missing closing parenthesis\" ) return StringGenerator . Sequence ( seq )'], ['Print the parse tree and then call render for an example .', 'def dump ( self , ** kwargs ) : import sys if not self . seq : self . seq = self . getSequence ( ) print ( \"StringGenerator version: %s\" % ( __version__ ) ) print ( \"Python version: %s\" % sys . version ) self . seq . dump ( ) return self . render ( ** kwargs )'], ['Return a list of generated strings .', 'def render_list ( self , cnt , unique = False , progress_callback = None , ** kwargs ) : rendered_list = [ ] i = 0 total_attempts = 0 while True : if i >= cnt : break if total_attempts > cnt * self . unique_attempts_factor : raise StringGenerator . UniquenessError ( u\"couldn\\'t satisfy uniqueness\" ) s = self . render ( ** kwargs ) if unique : if not s in rendered_list : rendered_list . append ( s ) i += 1 else : rendered_list . append ( s ) i += 1 total_attempts += 1 if progress_callback and callable ( progress_callback ) : progress_callback ( i , cnt ) return rendered_list'], ['Establish the connection . This is done automatically for you .', 'def connect ( self ) : self . conn = boto . connect_s3 ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL ) self . bucket = self . conn . get_bucket ( self . AWS_STORAGE_BUCKET_NAME ) self . k = Key ( self . bucket )'], ['Connect to Cloud Front . This is done automatically for you when needed .', 'def connect_cloudfront ( self ) : \"Connect to Cloud Front. This is done automatically for you when needed.\" self . conn_cloudfront = connect_cloudfront ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL )'], ['Create a folder on S3 .', 'def mkdir ( self , target_folder ) : self . printv ( \"Making directory: %s\" % target_folder ) self . k . key = re . sub ( r\"^/|/$\" , \"\" , target_folder ) + \"/\" self . k . set_contents_from_string ( \\'\\' ) self . k . close ( )'], ['Delete the path and anything under the path .', 'def rm ( self , path ) : list_of_files = list ( self . ls ( path ) ) if list_of_files : if len ( list_of_files ) == 1 : self . bucket . delete_key ( list_of_files [ 0 ] ) else : self . bucket . delete_keys ( list_of_files ) self . printv ( \"Deleted: %s\" % list_of_files ) else : logger . error ( \"There was nothing to remove under %s\" , path )'], ['Copy a file to s3 .', 'def __put_key ( self , local_file , target_file , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , source = \"filename\" ) : action_word = \"moving\" if del_after_upload else \"copying\" try : self . k . key = target_file if source == \"filename\" : self . k . set_contents_from_filename ( local_file , self . AWS_HEADERS ) elif source == \"fileobj\" : self . k . set_contents_from_file ( local_file , self . AWS_HEADERS ) elif source == \"string\" : self . k . set_contents_from_string ( local_file , self . AWS_HEADERS ) else : raise Exception ( \"%s is not implemented as a source.\" % source ) self . k . set_acl ( acl ) self . k . close ( ) self . printv ( \"%s %s to %s\" % ( action_word , local_file , target_file ) ) if del_after_upload and source == \"filename\" : try : os . remove ( local_file ) except : logger . error ( \"Unable to delete the file: \" , local_file , exc_info = True ) return True except : logger . error ( \"Error in writing to %s\" , target_file , exc_info = True ) return False'], ['Copy a file or folder from local to s3 .', 'def cp ( self , local_path , target_path , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , invalidate = False ) : result = None if overwrite : list_of_files = [ ] else : list_of_files = self . ls ( folder = target_path , begin_from_file = \"\" , num = - 1 , get_grants = False , all_grant_data = False ) if local_path . endswith ( \"/*\" ) : local_path = local_path [ : - 2 ] target_path = re . sub ( r\"^/|/$\" , \"\" , target_path ) else : local_base_name = os . path . basename ( local_path ) local_path = re . sub ( r\"/$\" , \"\" , local_path ) target_path = re . sub ( r\"^/\" , \"\" , target_path ) if not target_path . endswith ( local_base_name ) : target_path = os . path . join ( target_path , local_base_name ) if os . path . exists ( local_path ) : result = self . __find_files_and_copy ( local_path , target_path , acl , del_after_upload , overwrite , invalidate , list_of_files ) else : result = { \\'file_does_not_exist\\' : local_path } logger . error ( \"trying to upload to s3 but file doesn\\'t exist: %s\" % local_path ) return result'], ['Similar to Linux mv command .', \"def mv ( self , local_file , target_file , acl = 'public-read' , overwrite = True , invalidate = False ) : self . cp ( local_file , target_file , acl = acl , del_after_upload = True , overwrite = overwrite , invalidate = invalidate )\"], ['Deal with saving cropduster images to S3 . Cropduster is a Django library for resizing editorial images . S3utils was originally written to put cropduster images on S3 bucket .', 'def cp_cropduster_image ( self , the_image_path , del_after_upload = False , overwrite = False , invalidate = False ) : local_file = os . path . join ( settings . MEDIA_ROOT , the_image_path ) if os . path . exists ( local_file ) : the_image_crops_path = os . path . splitext ( the_image_path ) [ 0 ] the_image_crops_path_full_path = os . path . join ( settings . MEDIA_ROOT , the_image_crops_path ) self . cp ( local_path = local_file , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , ) self . cp ( local_path = the_image_crops_path_full_path + \"/*\" , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_crops_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , )'], ['sets permissions for a file on S3', \"def chmod ( self , target_file , acl = 'public-read' ) : self . k . key = target_file self . k . set_acl ( acl ) self . k . close ( )\"], ['Get the list of files and permissions from S3 .', 'def ll ( self , folder = \"\" , begin_from_file = \"\" , num = - 1 , all_grant_data = False ) : return self . ls ( folder = folder , begin_from_file = begin_from_file , num = num , get_grants = True , all_grant_data = all_grant_data )'], ['Get the path from a given url including the querystring .', 'def get_path ( url ) : url = urlsplit ( url ) path = url . path if url . query : path += \"?{}\" . format ( url . query ) return path'], ['Reads data from disk and generates CSV files .', \"def run ( self ) : if not os . path . exists ( self . output ) : try : os . mkdir ( self . output ) except : print 'failed to create output directory %s' % self . output if not os . path . isdir ( self . output ) : print 'invalid output directory %s' % self . output sys . exit ( 1 ) visitors = [ _CompaniesCSV ( self . output ) , _ActivitiesCSV ( self . output ) , _ActivitiesSeenCSV ( self . output ) , _QSACSV ( self . output ) , ] for path in glob . glob ( os . path . join ( self . input , '*.json' ) ) : with open ( path , 'r' ) as f : try : data = json . load ( f , encoding = 'utf-8' ) except ValueError : continue for visitor in visitors : visitor . visit ( data )\"], ['Process a list of simple string field definitions and assign their order based on prefix .', \"def process_fields ( self , fields ) : result = [ ] strip = '' . join ( self . PREFIX_MAP ) for field in fields : direction = self . PREFIX_MAP [ '' ] if field [ 0 ] in self . PREFIX_MAP : direction = self . PREFIX_MAP [ field [ 0 ] ] field = field . lstrip ( strip ) result . append ( ( field , direction ) ) return result\"], ['Firms search in rubric', \"def search_in_rubric ( self , ** kwargs ) : point = kwargs . pop ( 'point' , False ) if point : kwargs [ 'point' ] = '%s,%s' % point bound = kwargs . pop ( 'bound' , False ) if bound : kwargs [ 'bound[point1]' ] = bound [ 0 ] kwargs [ 'bound[point2]' ] = bound [ 1 ] filters = kwargs . pop ( 'filters' , False ) if filters : for k , v in filters . items ( ) : kwargs [ 'filters[%s]' % k ] = v return self . _search_in_rubric ( ** kwargs )\"], ['Refresh the list and the screen', 'def refresh ( self ) : self . _screen . force_update ( ) self . _screen . refresh ( ) self . _update ( 1 )'], ['Mark an action as started', 'def start ( self , activity , action ) : try : self . _start_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . refresh ( )'], ['Mark a task as completed', 'def stop ( self , activity , action ) : try : self . _remove_running_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . _mark_action_completed ( activity , action ) self . refresh ( )'], ['Move laggard tasks over', 'def finish ( self , status ) : retox_log . info ( \"Completing %s with status %s\" % ( self . name , status ) ) result = Screen . COLOUR_GREEN if not status else Screen . COLOUR_RED self . palette [ \\'title\\' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , result ) for item in list ( self . _task_view . options ) : self . _task_view . options . remove ( item ) self . _completed_view . options . append ( item ) self . refresh ( )'], ['Reset the frame between jobs', \"def reset ( self ) : self . palette [ 'title' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , Screen . COLOUR_BLUE ) self . _completed_view . options = [ ] self . _task_view . options = [ ] self . refresh ( )\"], ['Returns the available kwargs of the called class', 'def default_arguments ( cls ) : func = cls . __init__ args = func . __code__ . co_varnames defaults = func . __defaults__ index = - len ( defaults ) return { k : v for k , v in zip ( args [ index : ] , defaults ) }'], ['Recreate the class based in your args multiple uses', 'def recreate ( cls , * args , ** kwargs ) : cls . check_arguments ( kwargs ) first_is_callable = True if any ( args ) and callable ( args [ 0 ] ) else False signature = cls . default_arguments ( ) allowed_arguments = { k : v for k , v in kwargs . items ( ) if k in signature } if ( any ( allowed_arguments ) or any ( args ) ) and not first_is_callable : if any ( args ) and not first_is_callable : return cls ( args [ 0 ] , ** allowed_arguments ) elif any ( allowed_arguments ) : return cls ( ** allowed_arguments ) return cls . instances [ - 1 ] if any ( cls . instances ) else cls ( )'], ['Put warnings of arguments whose can t be handle by the class', 'def check_arguments ( cls , passed ) : defaults = list ( cls . default_arguments ( ) . keys ( ) ) template = ( \"Pass arg {argument:!r} in {cname:!r}, can be a typo? \" \"Supported key arguments: {defaults}\" ) fails = [ ] for arg in passed : if arg not in defaults : warn ( template . format ( argument = arg , cname = cls . __name__ , defaults = defaults ) ) fails . append ( arg ) return any ( fails )'], ['process the specified type then process its children', 'def process ( self , data , type , history ) : if type in history : return if type . enum ( ) : return history . append ( type ) resolved = type . resolve ( ) value = None if type . multi_occurrence ( ) : value = [ ] else : if len ( resolved ) > 0 : if resolved . mixed ( ) : value = Factory . property ( resolved . name ) md = value . __metadata__ md . sxtype = resolved else : value = Factory . object ( resolved . name ) md = value . __metadata__ md . sxtype = resolved md . ordering = self . ordering ( resolved ) setattr ( data , type . name , value ) if value is not None : data = value if not isinstance ( data , list ) : self . add_attributes ( data , resolved ) for child , ancestry in resolved . children ( ) : if self . skip_child ( child , ancestry ) : continue self . process ( data , child , history [ : ] )'], ['get whether or not to skip the specified child', 'def skip_child ( self , child , ancestry ) : if child . any ( ) : return True for x in ancestry : if x . choice ( ) : return True return False'], ['Checks whether knocks are enabled for the model given as argument', \"def active_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : return True return _thread_locals . knock_enabled . get ( obj . __class__ , True )\"], ['Context manager to suspend sending knocks for the given model', \"def pause_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : _thread_locals . knock_enabled = { } obj . __class__ . _disconnect ( ) _thread_locals . knock_enabled [ obj . __class__ ] = False yield _thread_locals . knock_enabled [ obj . __class__ ] = True obj . __class__ . _connect ( )\"], ['Loop over the report progress', 'def _loopreport ( self ) : while 1 : eventlet . sleep ( 0.2 ) ac2popenlist = { } for action in self . session . _actions : for popen in action . _popenlist : if popen . poll ( ) is None : lst = ac2popenlist . setdefault ( action . activity , [ ] ) lst . append ( popen ) if not action . _popenlist and action in self . _actionmayfinish : super ( RetoxReporter , self ) . logaction_finish ( action ) self . _actionmayfinish . remove ( action ) self . screen . draw_next_frame ( repeat = False )'], ['Send markdown email', \"def send ( email , subject = None , from_email = None , to_email = None , cc = None , bcc = None , reply_to = None , smtp = None ) : if is_string ( email ) : email = EmailContent ( email ) from_email = sanitize_email_address ( from_email or email . headers . get ( 'from' ) ) to_email = sanitize_email_address ( to_email or email . headers . get ( 'to' ) ) cc = sanitize_email_address ( cc or email . headers . get ( 'cc' ) ) bcc = sanitize_email_address ( bcc or email . headers . get ( 'bcc' ) ) reply_to = sanitize_email_address ( reply_to or email . headers . get ( 'reply-to' ) ) message_args = { 'html' : email . html , 'text' : email . text , 'subject' : ( subject or email . headers . get ( 'subject' , '' ) ) , 'mail_from' : from_email , 'mail_to' : to_email } if cc : message_args [ 'cc' ] = cc if bcc : message_args [ 'bcc' ] = bcc if reply_to : message_args [ 'headers' ] = { 'reply-to' : reply_to } message = emails . Message ( ** message_args ) for filename , data in email . inline_images : message . attach ( filename = filename , content_disposition = 'inline' , data = data ) message . send ( smtp = smtp )\"], ['Process timezone casting and conversion .', 'def _process_tz ( self , dt , naive , tz ) : def _tz ( t ) : if t in ( None , \\'naive\\' ) : return t if t == \\'local\\' : if __debug__ and not localtz : raise ValueError ( \"Requested conversion to local timezone, but `localtz` not installed.\" ) t = localtz if not isinstance ( t , tzinfo ) : if __debug__ and not localtz : raise ValueError ( \"The `pytz` package must be installed to look up timezone: \" + repr ( t ) ) t = get_tz ( t ) if not hasattr ( t , \\'normalize\\' ) and get_tz : t = get_tz ( t . tzname ( dt ) ) return t naive = _tz ( naive ) tz = _tz ( tz ) if not dt . tzinfo and naive : if hasattr ( naive , \\'localize\\' ) : dt = naive . localize ( dt ) else : dt = dt . replace ( tzinfo = naive ) if not tz : return dt if hasattr ( tz , \\'normalize\\' ) : dt = tz . normalize ( dt . astimezone ( tz ) ) elif tz == \\'naive\\' : dt = dt . replace ( tzinfo = None ) else : dt = dt . astimezone ( tz ) return dt'], ['Trigger assignment of default values .', 'def _prepare_defaults ( self ) : for name , field in self . __fields__ . items ( ) : if field . assign : getattr ( self , name )'], ['Convert data coming in from the MongoDB wire driver into a Document instance .', \"def from_mongo ( cls , doc ) : if doc is None : return None if isinstance ( doc , Document ) : return doc if cls . __type_store__ and cls . __type_store__ in doc : cls = load ( doc [ cls . __type_store__ ] , 'marrow.mongo.document' ) instance = cls ( _prepare_defaults = False ) instance . __data__ = doc instance . _prepare_defaults ( ) return instance\"], ['Retrieve and remove a value from the backing store optionally with a default .', 'def pop ( self , name , default = SENTINEL ) : if default is SENTINEL : return self . __data__ . pop ( name ) return self . __data__ . pop ( name , default )'], ['A basic operation operating on a single value .', 'def _op ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _op ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) if other is not None : other = f . transformer . foreign ( other , ( f , self . _document ) ) return Filter ( { self . _name : { operation : other } } )'], ['An iterative operation operating on multiple values . Consumes iterators to construct a concrete list at time of execution .', 'def _iop ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _iop ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) def _t ( o ) : for value in o : yield None if value is None else f . transformer . foreign ( value , ( f , self . _document ) ) other = other if len ( other ) > 1 else other [ 0 ] values = list ( _t ( other ) ) return Filter ( { self . _name : { operation : values } } )']]\n",
            "Read 300 sentence pairs\n",
            "[['Return either the full or truncated version of a QIIME - formatted taxonomy string .', 'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]'], ['Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .', 'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )'], ['Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .', 'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle'], ['Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .', 'def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table'], ['Parses the unifrac results file into a dictionary', 'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )'], ['Function to parse data from older version of unifrac file obtained from Qiime version 1 . 8 and earlier .', 'def parse_unifrac_v1_8 ( unifrac , file_data ) : for line in file_data : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ - 2 ] . split ( \"\\\\t\" ) [ 1 : ] ] unifrac [ \"varexp\" ] = [ float ( entry ) for entry in file_data [ - 1 ] . split ( \"\\\\t\" ) [ 1 : ] ] return unifrac'], ['Function to parse data from newer version of unifrac file obtained from Qiime version 1 . 9 and later .', 'def parse_unifrac_v1_9 ( unifrac , file_data ) : unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ 0 ] . split ( \"\\\\t\" ) ] unifrac [ \"varexp\" ] = [ float ( entry ) * 100 for entry in file_data [ 3 ] . split ( \"\\\\t\" ) ] for line in file_data [ 8 : ] : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] return unifrac'], ['Determine color - category mapping . If color_column was specified then map the category names to color values . Otherwise use the palettable colors to automatically generate a set of colors for the group values .', 'def color_mapping ( sample_map , header , group_column , color_column = None ) : group_colors = OrderedDict ( ) group_gather = gather_categories ( sample_map , header , [ group_column ] ) if color_column is not None : color_gather = gather_categories ( sample_map , header , [ color_column ] ) for group in group_gather : for color in color_gather : if group_gather [ group ] . sids . intersection ( color_gather [ color ] . sids ) : group_colors [ group ] = color else : bcolors = itertools . cycle ( Set3_12 . hex_colors ) for group in group_gather : group_colors [ group ] = bcolors . next ( ) return group_colors'], ['return reverse completment of read', \"def rev_c ( read ) : rc = [ ] rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } for base in read : rc . extend ( rc_nucs [ base . upper ( ) ] ) return rc [ : : - 1 ]\"], ['randomly shuffle genome', \"def shuffle_genome ( genome , cat , fraction = float ( 100 ) , plot = True , alpha = 0.1 , beta = 100000 , min_length = 1000 , max_length = 200000 ) : header = '>randomized_%s' % ( genome . name ) sequence = list ( '' . join ( [ i [ 1 ] for i in parse_fasta ( genome ) ] ) ) length = len ( sequence ) shuffled = [ ] while sequence is not False : s = int ( random . gammavariate ( alpha , beta ) ) if s <= min_length or s >= max_length : continue if len ( sequence ) < s : seq = sequence [ 0 : ] else : seq = sequence [ 0 : s ] sequence = sequence [ s : ] shuffled . append ( '' . join ( seq ) ) if sequence == [ ] : break random . shuffle ( shuffled ) if fraction == float ( 100 ) : subset = shuffled else : max_pieces = int ( length * fraction / 100 ) subset , total = [ ] , 0 for fragment in shuffled : length = len ( fragment ) if total + length <= max_pieces : subset . append ( fragment ) total += length else : diff = max_pieces - total subset . append ( fragment [ 0 : diff ] ) break if cat is True : yield [ header , '' . join ( subset ) ] else : for i , seq in enumerate ( subset ) : yield [ '%s fragment:%s' % ( header , i ) , seq ]\"], ['If the fit contains statistically insignificant parameters remove them . Returns a pruned fit where all parameters have p - values of the t - statistic below p_max', \"def _prune ( self , fit , p_max ) : def remove_from_model_desc ( x , model_desc ) : rhs_termlist = [ ] for t in model_desc . rhs_termlist : if not t . factors : rhs_termlist . append ( t ) elif not x == t . factors [ 0 ] . _varname : rhs_termlist . append ( t ) md = ModelDesc ( model_desc . lhs_termlist , rhs_termlist ) return md corrected_model_desc = ModelDesc ( fit . model . formula . lhs_termlist [ : ] , fit . model . formula . rhs_termlist [ : ] ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass while pars_to_prune : corrected_model_desc = remove_from_model_desc ( pars_to_prune [ 0 ] , corrected_model_desc ) fit = fm . ols ( corrected_model_desc , data = self . df ) . fit ( ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass return fit\"], ['Return the best fit based on rsquared', 'def find_best_rsquared ( list_of_fits ) : res = sorted ( list_of_fits , key = lambda x : x . rsquared ) return res [ - 1 ]'], ['Return a df with predictions and confidence interval', \"def _predict ( self , fit , df ) : df_res = df . copy ( ) if 'Intercept' in fit . model . exog_names : df_res [ 'Intercept' ] = 1.0 df_res [ 'predicted' ] = fit . predict ( df_res ) if not self . allow_negative_predictions : df_res . loc [ df_res [ 'predicted' ] < 0 , 'predicted' ] = 0 prstd , interval_l , interval_u = wls_prediction_std ( fit , df_res [ fit . model . exog_names ] , alpha = 1 - self . confint ) df_res [ 'interval_l' ] = interval_l df_res [ 'interval_u' ] = interval_u if 'Intercept' in df_res : df_res . drop ( labels = [ 'Intercept' ] , axis = 1 , inplace = True ) return df_res\"], ['Calculate the relative abundance of each OTUID in a Sample .', 'def relative_abundance ( biomf , sampleIDs = None ) : if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating relative abundances: The sampleIDs provided do\" \" not match the sampleIDs in biom file. Please double check the sampleIDs\" \" provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) norm_biomf = biomf . norm ( inplace = False ) return { sample : { otuID : norm_biomf . get_value_by_ids ( otuID , sample ) for otuID in otuIDs } for sample in sampleIDs }'], ['Calculate the mean OTU abundance percentage .', 'def mean_otu_pct_abundance ( ra , otuIDs ) : sids = ra . keys ( ) otumeans = defaultdict ( int ) for oid in otuIDs : otumeans [ oid ] = sum ( [ ra [ sid ] [ oid ] for sid in sids if oid in ra [ sid ] ] ) / len ( sids ) * 100 return otumeans'], ['Calculate the mean relative abundance percentage .', 'def MRA ( biomf , sampleIDs = None , transform = None ) : ra = relative_abundance ( biomf , sampleIDs ) if transform is not None : ra = { sample : { otuID : transform ( abd ) for otuID , abd in ra [ sample ] . items ( ) } for sample in ra . keys ( ) } otuIDs = biomf . ids ( axis = \"observation\" ) return mean_otu_pct_abundance ( ra , otuIDs )'], ['Calculate the total number of sequences in each OTU or SampleID .', 'def raw_abundance ( biomf , sampleIDs = None , sample_abd = True ) : results = defaultdict ( int ) if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating raw total abundances: The sampleIDs provided \" \"do not match the sampleIDs in biom file. Please double check the \" \"sampleIDs provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) for sampleID in sampleIDs : for otuID in otuIDs : abd = biomf . get_value_by_ids ( otuID , sampleID ) if sample_abd : results [ sampleID ] += abd else : results [ otuID ] += abd return results'], ['Function to transform the total abundance calculation for each sample ID to another format based on user given transformation function .', 'def transform_raw_abundance ( biomf , fn = math . log10 , sampleIDs = None , sample_abd = True ) : totals = raw_abundance ( biomf , sampleIDs , sample_abd ) return { sid : fn ( abd ) for sid , abd in totals . items ( ) }'], ['Compute the Mann - Whitney U test for unequal group sample sizes .', 'def print_MannWhitneyU ( div_calc ) : try : x = div_calc . values ( ) [ 0 ] . values ( ) y = div_calc . values ( ) [ 1 ] . values ( ) except : return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \" \"significance testing.\" T , p = stats . mannwhitneyu ( x , y ) print \"\\\\nMann-Whitney U test statistic:\" , T print \"Two-tailed p-value: {}\" . format ( 2 * p )'], ['Compute the Kruskal - Wallis H - test for independent samples . A typical rule is that each group must have at least 5 measurements .', 'def print_KruskalWallisH ( div_calc ) : calc = defaultdict ( list ) try : for k1 , v1 in div_calc . iteritems ( ) : for k2 , v2 in v1 . iteritems ( ) : calc [ k1 ] . append ( v2 ) except : return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \" \"significance testing.\" h , p = stats . kruskal ( * calc . values ( ) ) print \"\\\\nKruskal-Wallis H-test statistic for {} groups: {}\" . format ( str ( len ( div_calc ) ) , h ) print \"p-value: {}\" . format ( p )'], ['Parses the given options passed in at the command line .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Calculate the alpha diversity\\\\                                     of a set of samples using one or more \\\\                                     metrics and output a kernal density \\\\                                     estimator-smoothed histogram of the \\\\                                     results.\" ) parser . add_argument ( \"-m\" , \"--map_file\" , help = \"QIIME mapping file.\" ) parser . add_argument ( \"-i\" , \"--biom_fp\" , help = \"Path to the BIOM table\" ) parser . add_argument ( \"-c\" , \"--category\" , help = \"Specific category from the mapping file.\" ) parser . add_argument ( \"-d\" , \"--diversity\" , default = [ \"shannon\" ] , nargs = \"+\" , help = \"The alpha diversity metric. Default \\\\                             value is \\'shannon\\', which will calculate the Shannon\\\\                             entropy. Multiple metrics can be specified (space separated).\\\\                             The full list of metrics is available at:\\\\                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\\                             Beta diversity metrics will be supported in the future.\" ) parser . add_argument ( \"--x_label\" , default = [ None ] , nargs = \"+\" , help = \"The name of the diversity metric to be displayed on the\\\\                        plot as the X-axis label. If multiple metrics are specified,\\\\                        then multiple entries for the X-axis label should be given.\" ) parser . add_argument ( \"--color_by\" , help = \"A column name in the mapping file containing\\\\                              hexadecimal (#FF0000) color values that will\\\\                              be used to color the groups. Each sample ID must\\\\                              have a color entry.\" ) parser . add_argument ( \"--plot_title\" , default = \"\" , help = \"A descriptive title that will appear at the top \\\\                        of the output plot. Surround with quotes if there are\\\\                        spaces in the title.\" ) parser . add_argument ( \"-o\" , \"--output_dir\" , default = \".\" , help = \"The directory plots will be saved to.\" ) parser . add_argument ( \"--image_type\" , default = \"png\" , help = \"The type of image to save: png, svg, pdf, eps, etc...\" ) parser . add_argument ( \"--save_calculations\" , help = \"Path and name of text file to store the calculated \" \"diversity metrics.\" ) parser . add_argument ( \"--suppress_stats\" , action = \"store_true\" , help = \"Do not display \" \"significance testing results which are shown by default.\" ) parser . add_argument ( \"--show_available_metrics\" , action = \"store_true\" , help = \"Supply this parameter to see which alpha diversity metrics \" \" are available for usage. No calculations will be performed\" \" if this parameter is provided.\" ) return parser . parse_args ( )'], ['make blast db', \"def blastdb ( fasta , maxfile = 10000000 ) : db = fasta . rsplit ( '.' , 1 ) [ 0 ] type = check_type ( fasta ) if type == 'nucl' : type = [ 'nhr' , type ] else : type = [ 'phr' , type ] if os . path . exists ( '%s.%s' % ( db , type [ 0 ] ) ) is False and os . path . exists ( '%s.00.%s' % ( db , type [ 0 ] ) ) is False : print ( '# ... making blastdb for: %s' % ( fasta ) , file = sys . stderr ) os . system ( 'makeblastdb \\\\                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' % ( fasta , db , type [ 1 ] , maxfile ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['make usearch db', \"def usearchdb ( fasta , alignment = 'local' , usearch_loc = 'usearch' ) : if '.udb' in fasta : print ( '# ... database found: %s' % ( fasta ) , file = sys . stderr ) return fasta type = check_type ( fasta ) db = '%s.%s.udb' % ( fasta . rsplit ( '.' , 1 ) [ 0 ] , type ) if os . path . exists ( db ) is False : print ( '# ... making usearch db for: %s' % ( fasta ) , file = sys . stderr ) if alignment == 'local' : os . system ( '%s -makeudb_ublast %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) elif alignment == 'global' : os . system ( '%s -makeudb_usearch %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['Pretty print .', \"def _pp ( dict_data ) : for key , val in dict_data . items ( ) : print ( '{0:<11}: {1}' . format ( key , val ) )\"], ['Print licenses .', \"def print_licences ( params , metadata ) : if hasattr ( params , 'licenses' ) : if params . licenses : _pp ( metadata . licenses_desc ( ) ) sys . exit ( 0 )\"], ['Check repository existence .', 'def check_repository_existence ( params ) : repodir = os . path . join ( params . outdir , params . name ) if os . path . isdir ( repodir ) : raise Conflict ( \\'Package repository \"{0}\" has already exists.\\' . format ( repodir ) )'], ['Generate package repository .', 'def generate_package ( params ) : pkg_data = package . PackageData ( params ) pkg_tree = package . PackageTree ( pkg_data ) pkg_tree . generate ( ) pkg_tree . move ( ) VCS ( os . path . join ( pkg_tree . outdir , pkg_tree . name ) , pkg_tree . pkg_data )'], ['print single reads to stderr', \"def print_single ( line , rev ) : if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] fq = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] print ( '\\\\n' . join ( fq ) , file = sys . stderr )\"], ['convert sam to fastq', \"def sam2fastq ( sam , singles = False , force = False ) : L , R = None , None for line in sam : if line . startswith ( '@' ) is True : continue line = line . strip ( ) . split ( ) bit = [ True if i == '1' else False for i in bin ( int ( line [ 1 ] ) ) . split ( 'b' ) [ 1 ] [ : : - 1 ] ] while len ( bit ) < 8 : bit . append ( False ) pair , proper , na , nap , rev , mrev , left , right = bit if pair is False : if singles is True : print_single ( line , rev ) continue if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] if left is True : if L is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if L is not None : L = None continue L = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if R is not None : yield L yield R L , R = None , None if right is True : if R is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if R is not None : R = None continue R = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if L is not None : yield L yield R L , R = None , None\"], ['sort sam file', 'def sort_sam ( sam , sort ) : tempdir = \\'%s/\\' % ( os . path . abspath ( sam ) . rsplit ( \\'/\\' , 1 ) [ 0 ] ) if sort is True : mapping = \\'%s.sorted.sam\\' % ( sam . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if sam != \\'-\\' : if os . path . exists ( mapping ) is False : os . system ( \"\\\\                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\\                    \" % ( sbuffer , tempdir , mapping , sam ) ) else : mapping = \\'stdin-sam.sorted.sam\\' p = Popen ( \"sort -k1 --buffer-size=%sG -T %s -o %s\" % ( sbuffer , tempdir , mapping ) , stdin = sys . stdin , shell = True ) p . communicate ( ) mapping = open ( mapping ) else : if sam == \\'-\\' : mapping = sys . stdin else : mapping = open ( sam ) return mapping'], ['randomly subset sam file', \"def sub_sam ( sam , percent , sort = True , sbuffer = False ) : mapping = sort_sam ( sam , sort ) pool = [ 1 for i in range ( 0 , percent ) ] + [ 0 for i in range ( 0 , 100 - percent ) ] c = cycle ( [ 1 , 2 ] ) for line in mapping : line = line . strip ( ) . split ( ) if line [ 0 ] . startswith ( '@' ) : yield line continue if int ( line [ 1 ] ) <= 20 : if random . choice ( pool ) == 1 : yield line else : n = next ( c ) if n == 1 : prev = line if n == 2 and random . choice ( pool ) == 1 : yield prev yield line\"], ['convert fq to fa', \"def fq2fa ( fq ) : c = cycle ( [ 1 , 2 , 3 , 4 ] ) for line in fq : n = next ( c ) if n == 1 : seq = [ '>%s' % ( line . strip ( ) . split ( '@' , 1 ) [ 1 ] ) ] if n == 2 : seq . append ( line . strip ( ) ) yield seq\"], ['Converts the returned value of wrapped function to the type of the first arg or to the type specified by a kwarg key return_type s value .', \"def change_return_type ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) elif len ( args ) > 0 : return_type = type ( args [ 0 ] ) return return_type ( f ( * args , ** kwargs ) ) else : return f ( * args , ** kwargs ) return wrapper\"], ['Converts all args to set type via self . setify function .', 'def convert_args_to_sets ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : args = ( setify ( x ) for x in args ) return f ( * args , ** kwargs ) return wrapper'], ['Membuat objek - objek entri dari laman yang diambil .', \"def _init_entri ( self , laman ) : sup = BeautifulSoup ( laman . text , 'html.parser' ) estr = '' for label in sup . find ( 'hr' ) . next_siblings : if label . name == 'hr' : self . entri . append ( Entri ( estr ) ) break if label . name == 'h2' : if estr : self . entri . append ( Entri ( estr ) ) estr = '' estr += str ( label ) . strip ( )\"], ['Memproses kata dasar yang ada dalam nama entri .', \"def _init_kata_dasar ( self , dasar ) : for tiap in dasar : kata = tiap . find ( 'a' ) dasar_no = kata . find ( 'sup' ) kata = ambil_teks_dalam_label ( kata ) self . kata_dasar . append ( kata + ' [{}]' . format ( dasar_no . text . strip ( ) ) if dasar_no else kata )\"], ['Mengembalikan hasil serialisasi objek Entri ini .', 'def serialisasi ( self ) : return { \"nama\" : self . nama , \"nomor\" : self . nomor , \"kata_dasar\" : self . kata_dasar , \"pelafalan\" : self . pelafalan , \"bentuk_tidak_baku\" : self . bentuk_tidak_baku , \"varian\" : self . varian , \"makna\" : [ makna . serialisasi ( ) for makna in self . makna ] }'], ['Mengembalikan representasi string untuk semua makna entri ini .', 'def _makna ( self ) : if len ( self . makna ) > 1 : return \\'\\\\n\\' . join ( str ( i ) + \". \" + str ( makna ) for i , makna in enumerate ( self . makna , 1 ) ) return str ( self . makna [ 0 ] )'], ['Mengembalikan representasi string untuk nama entri ini .', 'def _nama ( self ) : hasil = self . nama if self . nomor : hasil += \" [{}]\" . format ( self . nomor ) if self . kata_dasar : hasil = \" » \". j oin( s elf. k ata_dasar)      » \" + h sil return hasil'], ['Mengembalikan representasi string untuk varian entri ini . Dapat digunakan untuk Varian maupun Bentuk tidak baku .', 'def _varian ( self , varian ) : if varian == self . bentuk_tidak_baku : nama = \"Bentuk tidak baku\" elif varian == self . varian : nama = \"Varian\" else : return \\'\\' return nama + \\': \\' + \\', \\' . join ( varian )'], ['Memproses kelas kata yang ada dalam makna .', \"def _init_kelas ( self , makna_label ) : kelas = makna_label . find ( color = 'red' ) lain = makna_label . find ( color = 'darkgreen' ) info = makna_label . find ( color = 'green' ) if kelas : kelas = kelas . find_all ( 'span' ) if lain : self . kelas = { lain . text . strip ( ) : lain [ 'title' ] . strip ( ) } self . submakna = lain . next_sibling . strip ( ) self . submakna += ' ' + makna_label . find ( color = 'grey' ) . text . strip ( ) else : self . kelas = { k . text . strip ( ) : k [ 'title' ] . strip ( ) for k in kelas } if kelas else { } self . info = info . text . strip ( ) if info else ''\"], ['Memproses contoh yang ada dalam makna .', \"def _init_contoh ( self , makna_label ) : indeks = makna_label . text . find ( ': ' ) if indeks != - 1 : contoh = makna_label . text [ indeks + 2 : ] . strip ( ) self . contoh = contoh . split ( '; ' ) else : self . contoh = [ ]\"], ['Mengembalikan hasil serialisasi objek Makna ini .', 'def serialisasi ( self ) : return { \"kelas\" : self . kelas , \"submakna\" : self . submakna , \"info\" : self . info , \"contoh\" : self . contoh }'], ['Build sphinx documentation .', 'def build_sphinx ( pkg_data , projectdir ) : try : version , _minor_version = pkg_data . version . rsplit ( \\'.\\' , 1 ) except ValueError : version = pkg_data . version args = \\' \\' . join ( ( \\'sphinx-quickstart\\' , \\'--sep\\' , \\'-q\\' , \\'-p \"{name}\"\\' , \\'-a \"{author}\"\\' , \\'-v \"{version}\"\\' , \\'-r \"{release}\"\\' , \\'-l en\\' , \\'--suffix=.rst\\' , \\'--master=index\\' , \\'--ext-autodoc\\' , \\'--ext-viewcode\\' , \\'--makefile\\' , \\'{projectdir}\\' ) ) . format ( name = pkg_data . name , author = pkg_data . author , version = version , release = pkg_data . version , projectdir = projectdir ) if subprocess . call ( shlex . split ( args ) ) == 0 : _touch_gitkeep ( projectdir )'], ['make bowtie db', \"def bowtiedb ( fa , keepDB ) : btdir = '%s/bt2' % ( os . getcwd ( ) ) if not os . path . exists ( btdir ) : os . mkdir ( btdir ) btdb = '%s/%s' % ( btdir , fa . rsplit ( '/' , 1 ) [ - 1 ] ) if keepDB is True : if os . path . exists ( '%s.1.bt2' % ( btdb ) ) : return btdb p = subprocess . Popen ( 'bowtie2-build -q %s %s' % ( fa , btdb ) , shell = True ) p . communicate ( ) return btdb\"], ['generate bowtie2 command', \"def bowtie ( sam , btd , f , r , u , opt , no_shrink , threads ) : bt2 = 'bowtie2 -x %s -p %s ' % ( btd , threads ) if f is not False : bt2 += '-1 %s -2 %s ' % ( f , r ) if u is not False : bt2 += '-U %s ' % ( u ) bt2 += opt if no_shrink is False : if f is False : bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' | shrinksam -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' > %s.sam' % ( sam ) return bt2\"], ['map all read sets against all fasta files', \"def crossmap ( fas , reads , options , no_shrink , keepDB , threads , cluster , nodes ) : if cluster is True : threads = '48' btc = [ ] for fa in fas : btd = bowtiedb ( fa , keepDB ) F , R , U = reads if F is not False : if U is False : u = False for i , f in enumerate ( F ) : r = R [ i ] if U is not False : u = U [ i ] sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , f . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) else : f = False r = False for u in U : sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , u . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) if cluster is False : for i in btc : p = subprocess . Popen ( i , shell = True ) p . communicate ( ) else : ID = '' . join ( random . choice ( [ str ( i ) for i in range ( 0 , 9 ) ] ) for _ in range ( 5 ) ) for node , commands in enumerate ( chunks ( btc , nodes ) , 1 ) : bs = open ( '%s/crossmap-qsub.%s.%s.sh' % ( os . getcwd ( ) , ID , node ) , 'w' ) print ( '\\\\n' . join ( commands ) , file = bs ) bs . close ( ) p = subprocess . Popen ( 'qsub -V -N crossmap %s' % ( bs . name ) , shell = True ) p . communicate ( )\"], ['Returns a connection object from the router given args .', \"def get_conn ( self , * args , ** kwargs ) : connections = self . __connections_for ( 'get_conn' , args = args , kwargs = kwargs ) if len ( connections ) is 1 : return connections [ 0 ] else : return connections\"], ['return the non - direct init if the direct algorithm has been selected .', 'def __get_nondirect_init ( self , init ) : crc = init for i in range ( self . Width ) : bit = crc & 0x01 if bit : crc ^= self . Poly crc >>= 1 if bit : crc |= self . MSB_Mask return crc & self . Mask'], ['reflect a data word i . e . reverts the bit order .', 'def reflect ( self , data , width ) : x = data & 0x01 for i in range ( width - 1 ) : data >>= 1 x = ( x << 1 ) | ( data & 0x01 ) return x'], ['Classic simple and slow CRC implementation . This function iterates bit by bit over the augmented input message and returns the calculated CRC value at the end .', 'def bit_by_bit ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] register = self . NonDirectInit for octet in in_data : if self . ReflectIn : octet = self . reflect ( octet , 8 ) for i in range ( 8 ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) | ( ( octet >> ( 7 - i ) ) & 0x01 ) if topbit : register ^= self . Poly for i in range ( self . Width ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) if topbit : register ^= self . Poly if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['This function generates the CRC table used for the table_driven CRC algorithm . The Python version cannot handle tables of an index width other than 8 . See the generated C code for tables with different sizes instead .', 'def gen_table ( self ) : table_length = 1 << self . TableIdxWidth tbl = [ 0 ] * table_length for i in range ( table_length ) : register = i if self . ReflectIn : register = self . reflect ( register , self . TableIdxWidth ) register = register << ( self . Width - self . TableIdxWidth + self . CrcShift ) for j in range ( self . TableIdxWidth ) : if register & ( self . MSB_Mask << self . CrcShift ) != 0 : register = ( register << 1 ) ^ ( self . Poly << self . CrcShift ) else : register = ( register << 1 ) if self . ReflectIn : register = self . reflect ( register >> self . CrcShift , self . Width ) << self . CrcShift tbl [ i ] = register & ( self . Mask << self . CrcShift ) return tbl'], ['The Standard table_driven CRC algorithm .', 'def table_driven ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] tbl = self . gen_table ( ) register = self . DirectInit << self . CrcShift if not self . ReflectIn : for octet in in_data : tblidx = ( ( register >> ( self . Width - self . TableIdxWidth + self . CrcShift ) ) ^ octet ) & 0xff register = ( ( register << ( self . TableIdxWidth - self . CrcShift ) ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = register >> self . CrcShift else : register = self . reflect ( register , self . Width + self . CrcShift ) << self . CrcShift for octet in in_data : tblidx = ( ( register >> self . CrcShift ) ^ octet ) & 0xff register = ( ( register >> self . TableIdxWidth ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = self . reflect ( register , self . Width + self . CrcShift ) & self . Mask if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['parse masked sequence into non - masked and masked regions', 'def parse_masked ( seq , min_len ) : nm , masked = [ ] , [ [ ] ] prev = None for base in seq [ 1 ] : if base . isupper ( ) : nm . append ( base ) if masked != [ [ ] ] and len ( masked [ - 1 ] ) < min_len : nm . extend ( masked [ - 1 ] ) del masked [ - 1 ] prev = False elif base . islower ( ) : if prev is False : masked . append ( [ ] ) masked [ - 1 ] . append ( base ) prev = True return nm , masked'], ['remove masked regions from fasta file as long as they are longer than min_len', \"def strip_masked ( fasta , min_len , print_masked ) : for seq in parse_fasta ( fasta ) : nm , masked = parse_masked ( seq , min_len ) nm = [ '%s removed_masked >=%s' % ( seq [ 0 ] , min_len ) , '' . join ( nm ) ] yield [ 0 , nm ] if print_masked is True : for i , m in enumerate ( [ i for i in masked if i != [ ] ] , 1 ) : m = [ '%s insertion:%s' % ( seq [ 0 ] , i ) , '' . join ( m ) ] yield [ 1 , m ]\"], ['Return arcsine transformed relative abundance from a BIOM format file .', 'def get_relative_abundance ( biomfile ) : biomf = biom . load_table ( biomfile ) norm_biomf = biomf . norm ( inplace = False ) rel_abd = { } for sid in norm_biomf . ids ( ) : rel_abd [ sid ] = { } for otuid in norm_biomf . ids ( \"observation\" ) : otuname = oc . otu_name ( norm_biomf . metadata ( otuid , axis = \"observation\" ) [ \"taxonomy\" ] ) otuname = \" \" . join ( otuname . split ( \"_\" ) ) abd = norm_biomf . get_value_by_ids ( otuid , sid ) rel_abd [ sid ] [ otuname ] = abd ast_rel_abd = bc . arcsine_sqrt_transform ( rel_abd ) return ast_rel_abd'], ['Find an OTU ID in a Newick - format tree . Return the starting position of the ID or None if not found .', 'def find_otu ( otuid , tree ) : for m in re . finditer ( otuid , tree ) : before , after = tree [ m . start ( ) - 1 ] , tree [ m . start ( ) + len ( otuid ) ] if before in [ \"(\" , \",\" , \")\" ] and after in [ \":\" , \";\" ] : return m . start ( ) return None'], ['Replace the OTU ids in the Newick phylogenetic tree format with truncated OTU names', 'def newick_replace_otuids ( tree , biomf ) : for val , id_ , md in biomf . iter ( axis = \"observation\" ) : otu_loc = find_otu ( id_ , tree ) if otu_loc is not None : tree = tree [ : otu_loc ] + oc . otu_name ( md [ \"taxonomy\" ] ) + tree [ otu_loc + len ( id_ ) : ] return tree'], ['return genome info for choosing representative', \"def genome_info ( genome , info ) : try : scg = info [ '#SCGs' ] dups = info [ '#SCG duplicates' ] length = info [ 'genome size (bp)' ] return [ scg - dups , length , genome ] except : return [ False , False , info [ 'genome size (bp)' ] , genome ]\"], ['choose represenative genome and print cluster information', \"def print_clusters ( fastas , info , ANI ) : header = [ '#cluster' , 'num. genomes' , 'rep.' , 'genome' , '#SCGs' , '#SCG duplicates' , 'genome size (bp)' , 'fragments' , 'list' ] yield header in_cluster = [ ] for cluster_num , cluster in enumerate ( connected_components ( ANI ) ) : cluster = sorted ( [ genome_info ( genome , info [ genome ] ) for genome in cluster ] , key = lambda x : x [ 0 : ] , reverse = True ) rep = cluster [ 0 ] [ - 1 ] cluster = [ i [ - 1 ] for i in cluster ] size = len ( cluster ) for genome in cluster : in_cluster . append ( genome ) try : stats = [ size , rep , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] except : stats = [ size , rep , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] if rep == genome : stats = [ '*%s' % ( cluster_num ) ] + stats else : stats = [ cluster_num ] + stats yield stats try : start = cluster_num + 1 except : start = 0 fastas = set ( [ i . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] for i in fastas ] ) for cluster_num , genome in enumerate ( fastas . difference ( set ( in_cluster ) ) , start ) : try : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] except : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] yield stats\"], ['convert ggKbase genome info tables to dictionary', \"def parse_ggKbase_tables ( tables , id_type ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'name' ) : header = line header [ 4 ] = 'genome size (bp)' header [ 12 ] = '#SCGs' header [ 13 ] = '#SCG duplicates' continue name , code , info = line [ 0 ] , line [ 1 ] , line info = [ to_int ( i ) for i in info ] if id_type is False : if 'UNK' in code or 'unknown' in code : code = name if ( name != code ) and ( name and code in g2info ) : print ( '# duplicate name or code in table(s)' , file = sys . stderr ) print ( '# %s and/or %s' % ( name , code ) , file = sys . stderr ) exit ( ) if name not in g2info : g2info [ name ] = { item : stat for item , stat in zip ( header , info ) } if code not in g2info : g2info [ code ] = { item : stat for item , stat in zip ( header , info ) } else : if id_type == 'name' : ID = name elif id_type == 'code' : ID = code else : print ( '# specify name or code column using -id' , file = sys . stderr ) exit ( ) ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['convert checkM genome info tables to dictionary', \"def parse_checkM_tables ( tables ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'Bin Id' ) : header = line header [ 8 ] = 'genome size (bp)' header [ 5 ] = '#SCGs' header [ 6 ] = '#SCG duplicates' continue ID , info = line [ 0 ] , line info = [ to_int ( i ) for i in info ] ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['get genome lengths', \"def genome_lengths ( fastas , info ) : if info is False : info = { } for genome in fastas : name = genome . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] if name in info : continue length = 0 fragments = 0 for seq in parse_fasta ( genome ) : length += len ( seq [ 1 ] ) fragments += 1 info [ name ] = { 'genome size (bp)' : length , '# contigs' : fragments } return info\"], ['Returns a list of db keys to route the given call to .', 'def get_dbs ( self , attr , args , kwargs , ** fkwargs ) : if not self . _ready : if not self . setup_router ( args = args , kwargs = kwargs , ** fkwargs ) : raise self . UnableToSetupRouter ( ) retval = self . _pre_routing ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) if retval is not None : args , kwargs = retval if not ( args or kwargs ) : return self . cluster . hosts . keys ( ) try : db_nums = self . _route ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) except Exception as e : self . _handle_exception ( e ) db_nums = [ ] return self . _post_routing ( attr = attr , db_nums = db_nums , args = args , kwargs = kwargs , ** fkwargs )'], ['Call method to perform any setup', 'def setup_router ( self , args , kwargs , ** fkwargs ) : self . _ready = self . _setup_router ( args = args , kwargs = kwargs , ** fkwargs ) return self . _ready'], ['Perform routing and return db_nums', 'def _route ( self , attr , args , kwargs , ** fkwargs ) : return self . cluster . hosts . keys ( )'], ['Iterates through all connections which were previously listed as unavailable and marks any that have expired their retry_timeout as being up .', 'def check_down_connections ( self ) : now = time . time ( ) for db_num , marked_down_at in self . _down_connections . items ( ) : if marked_down_at + self . retry_timeout <= now : self . mark_connection_up ( db_num )'], ['Marks all connections which were previously listed as unavailable as being up .', 'def flush_down_connections ( self ) : self . _get_db_attempts = 0 for db_num in self . _down_connections . keys ( ) : self . mark_connection_up ( db_num )'], ['Compute standby power', \"def standby ( df , resolution = '24h' , time_window = None ) : if df . empty : raise EmptyDataFrame ( ) df = pd . DataFrame ( df ) def parse_time ( t ) : if isinstance ( t , numbers . Number ) : return pd . Timestamp . utcfromtimestamp ( t ) . time ( ) else : return pd . Timestamp ( t ) . time ( ) if time_window is not None : t_start = parse_time ( time_window [ 0 ] ) t_end = parse_time ( time_window [ 1 ] ) if t_start > t_end : df = df [ ( df . index . time >= t_start ) | ( df . index . time < t_end ) ] else : df = df [ ( df . index . time >= t_start ) & ( df . index . time < t_end ) ] return df . resample ( resolution ) . min ( )\"], ['Compute the share of the standby power in the total consumption .', \"def share_of_standby ( df , resolution = '24h' , time_window = None ) : p_sb = standby ( df , resolution , time_window ) df = df . resample ( resolution ) . mean ( ) p_tot = df . sum ( ) p_standby = p_sb . sum ( ) share_standby = p_standby / p_tot res = share_standby . iloc [ 0 ] return res\"], ['Toggle counter for gas boilers', 'def count_peaks ( ts ) : on_toggles = ts . diff ( ) > 3000 shifted = np . logical_not ( on_toggles . shift ( 1 ) ) result = on_toggles & shifted count = result . sum ( ) return count'], ['Calculate the ratio of input vs . norm over a given interval .', 'def load_factor ( ts , resolution = None , norm = None ) : if norm is None : norm = ts . max ( ) if resolution is not None : ts = ts . resample ( rule = resolution ) . mean ( ) lf = ts / norm return lf'], ['get top hits after sorting by column number', 'def top_hits ( hits , num , column , reverse ) : hits . sort ( key = itemgetter ( column ) , reverse = reverse ) for hit in hits [ 0 : num ] : yield hit'], ['parse b6 output with sorting', \"def numBlast_sort ( blast , numHits , evalueT , bitT ) : header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header hmm = { h : [ ] for h in header } for line in blast : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( '\\\\t' ) line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if evalueT is not False and evalue > evalueT : continue if bitT is not False and bit < bitT : continue for i , h in zip ( line , header ) : hmm [ h ] . append ( i ) hmm = pd . DataFrame ( hmm ) for query , df in hmm . groupby ( by = [ '#query' ] ) : df = df . sort_values ( by = [ 'bitscore' ] , ascending = False ) for hit in df [ header ] . values [ 0 : numHits ] : yield hit\"], ['parse b6 output', \"def numBlast ( blast , numHits , evalueT = False , bitT = False , sort = False ) : if sort is True : for hit in numBlast_sort ( blast , numHits , evalueT , bitT ) : yield hit return header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header prev , hits = None , [ ] for line in blast : line = line . strip ( ) . split ( '\\\\t' ) ID = line [ 0 ] line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 11 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 11 , True ) : yield hit\"], ['parse hmm domain table output this version is faster but does not work unless the table is sorted', \"def numDomtblout ( domtblout , numHits , evalueT , bitT , sort ) : if sort is True : for hit in numDomtblout_sort ( domtblout , numHits , evalueT , bitT ) : yield hit return header = [ '#target name' , 'target accession' , 'tlen' , 'query name' , 'query accession' , 'qlen' , 'full E-value' , 'full score' , 'full bias' , 'domain #' , '# domains' , 'domain c-Evalue' , 'domain i-Evalue' , 'domain score' , 'domain bias' , 'hmm from' , 'hmm to' , 'seq from' , 'seq to' , 'env from' , 'env to' , 'acc' , 'target description' ] yield header prev , hits = None , [ ] for line in domtblout : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( ) desc = ' ' . join ( line [ 18 : ] ) line = line [ 0 : 18 ] line . append ( desc ) ID = line [ 0 ] + line [ 9 ] line [ 11 ] , line [ 13 ] = float ( line [ 11 ] ) , float ( line [ 13 ] ) evalue , bitscore = line [ 11 ] , line [ 13 ] line [ 11 ] , line [ 13 ] = evalue , bitscore if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 13 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 13 , True ) : yield hit\"], ['convert stockholm to fasta', \"def stock2fa ( stock ) : seqs = { } for line in stock : if line . startswith ( '#' ) is False and line . startswith ( ' ' ) is False and len ( line ) > 3 : id , seq = line . strip ( ) . split ( ) id = id . rsplit ( '/' , 1 ) [ 0 ] id = re . split ( '[0-9]\\\\|' , id , 1 ) [ - 1 ] if id not in seqs : seqs [ id ] = [ ] seqs [ id ] . append ( seq ) if line . startswith ( '//' ) : break return seqs\"], ['Return boolean time series following given week schedule .', \"def week_schedule ( index , on_time = None , off_time = None , off_days = None ) : if on_time is None : on_time = '9:00' if off_time is None : off_time = '17:00' if off_days is None : off_days = [ 'Sunday' , 'Monday' ] if not isinstance ( on_time , datetime . time ) : on_time = pd . to_datetime ( on_time , format = '%H:%M' ) . time ( ) if not isinstance ( off_time , datetime . time ) : off_time = pd . to_datetime ( off_time , format = '%H:%M' ) . time ( ) times = ( index . time >= on_time ) & ( index . time < off_time ) & ( ~ index . weekday_name . isin ( off_days ) ) return pd . Series ( times , index = index )\"], ['Draw a carpet plot of a pandas timeseries .', 'def carpet ( timeseries , ** kwargs ) : cmap = kwargs . pop ( \\'cmap\\' , cm . coolwarm ) norm = kwargs . pop ( \\'norm\\' , LogNorm ( ) ) interpolation = kwargs . pop ( \\'interpolation\\' , \\'nearest\\' ) cblabel = kwargs . pop ( \\'zlabel\\' , timeseries . name if timeseries . name else \\'\\' ) title = kwargs . pop ( \\'title\\' , \\'carpet plot: \\' + timeseries . name if timeseries . name else \\'\\' ) if timeseries . dropna ( ) . empty : print ( \\'skipped {} - no data\\' . format ( title ) ) return ts = timeseries . resample ( \\'15min\\' ) . interpolate ( ) vmin = max ( 0.1 , kwargs . pop ( \\'vmin\\' , ts [ ts > 0 ] . min ( ) ) ) vmax = max ( vmin , kwargs . pop ( \\'vmax\\' , ts . quantile ( .999 ) ) ) mpldatetimes = date2num ( ts . index . to_pydatetime ( ) ) ts . index = pd . MultiIndex . from_arrays ( [ np . floor ( mpldatetimes ) , 2 + mpldatetimes % 1 ] ) df = ts . unstack ( ) fig , ax = plt . subplots ( ) extent = [ df . columns [ 0 ] , df . columns [ - 1 ] , df . index [ - 1 ] + 0.5 , df . index [ 0 ] - 0.5 ] im = plt . imshow ( df , vmin = vmin , vmax = vmax , extent = extent , cmap = cmap , aspect = \\'auto\\' , norm = norm , interpolation = interpolation , ** kwargs ) ax . xaxis_date ( ) ax . xaxis . set_major_locator ( HourLocator ( interval = 2 ) ) ax . xaxis . set_major_formatter ( DateFormatter ( \\'%H:%M\\' ) ) ax . xaxis . grid ( True ) plt . xlabel ( \\'UTC Time\\' ) ax . yaxis_date ( ) dmin , dmax = ax . yaxis . get_data_interval ( ) number_of_days = ( num2date ( dmax ) - num2date ( dmin ) ) . days if abs ( number_of_days ) <= 35 : ax . yaxis . set_major_locator ( DayLocator ( ) ) else : ax . yaxis . set_major_locator ( AutoDateLocator ( ) ) ax . yaxis . set_major_formatter ( DateFormatter ( \"%a, %d %b %Y\" ) ) cbticks = np . logspace ( np . log10 ( vmin ) , np . log10 ( vmax ) , 11 , endpoint = True ) cb = plt . colorbar ( format = \\'%.0f\\' , ticks = cbticks ) cb . set_label ( cblabel ) plt . title ( title ) return im'], ['calculate percent identity', \"def calc_pident_ignore_gaps ( a , b ) : m = 0 mm = 0 for A , B in zip ( list ( a ) , list ( b ) ) : if A == '-' or A == '.' or B == '-' or B == '.' : continue if A == B : m += 1 else : mm += 1 try : return float ( float ( m ) / float ( ( m + mm ) ) ) * 100 except : return 0\"], ['skip column if either is a gap', \"def remove_gaps ( A , B ) : a_seq , b_seq = [ ] , [ ] for a , b in zip ( list ( A ) , list ( B ) ) : if a == '-' or a == '.' or b == '-' or b == '.' : continue a_seq . append ( a ) b_seq . append ( b ) return '' . join ( a_seq ) , '' . join ( b_seq )\"], ['compare pairs of sequences', \"def compare_seqs ( seqs ) : A , B , ignore_gaps = seqs a , b = A [ 1 ] , B [ 1 ] if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) if ignore_gaps is True : pident = calc_pident_ignore_gaps ( a , b ) else : pident = calc_pident ( a , b ) return A [ 0 ] , B [ 0 ] , pident\"], ['calculate Levenshtein ratio of sequences', \"def compare_seqs_leven ( seqs ) : A , B , ignore_gaps = seqs a , b = remove_gaps ( A [ 1 ] , B [ 1 ] ) if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) pident = lr ( a , b ) * 100 return A [ 0 ] , B [ 0 ] , pident\"], ['make pairwise sequence comparisons between aligned sequences', 'def pairwise_compare ( afa , leven , threads , print_list , ignore_gaps ) : seqs = { seq [ 0 ] : seq for seq in nr_fasta ( [ afa ] , append_index = True ) } num_seqs = len ( seqs ) pairs = ( ( i [ 0 ] , i [ 1 ] , ignore_gaps ) for i in itertools . combinations ( list ( seqs . values ( ) ) , 2 ) ) pool = multithread ( threads ) if leven is True : pident = pool . map ( compare_seqs_leven , pairs ) else : compare = pool . imap_unordered ( compare_seqs , pairs ) pident = [ i for i in tqdm ( compare , total = ( num_seqs * num_seqs ) / 2 ) ] pool . close ( ) pool . terminate ( ) pool . join ( ) return to_dictionary ( pident , print_list )'], ['print matrix of pidents to stdout', \"def print_pairwise ( pw , median = False ) : names = sorted ( set ( [ i for i in pw ] ) ) if len ( names ) != 0 : if '>' in names [ 0 ] : yield [ '#' ] + [ i . split ( '>' ) [ 1 ] for i in names if '>' in i ] else : yield [ '#' ] + names for a in names : if '>' in a : yield [ a . split ( '>' ) [ 1 ] ] + [ pw [ a ] [ b ] for b in names ] else : out = [ ] for b in names : if b in pw [ a ] : if median is False : out . append ( max ( pw [ a ] [ b ] ) ) else : out . append ( np . median ( pw [ a ] [ b ] ) ) else : out . append ( '-' ) yield [ a ] + out\"], ['print stats for comparisons', \"def print_comps ( comps ) : if comps == [ ] : print ( 'n/a' ) else : print ( '# min: %s, max: %s, mean: %s' % ( min ( comps ) , max ( comps ) , np . mean ( comps ) ) )\"], ['print min . pident within each clade and then matrix of between - clade max .', \"def compare_clades ( pw ) : names = sorted ( set ( [ i for i in pw ] ) ) for i in range ( 0 , 4 ) : wi , bt = { } , { } for a in names : for b in pw [ a ] : if ';' not in a or ';' not in b : continue pident = pw [ a ] [ b ] cA , cB = a . split ( ';' ) [ i ] , b . split ( ';' ) [ i ] if i == 0 and '_' in cA and '_' in cB : cA = cA . rsplit ( '_' , 1 ) [ 1 ] cB = cB . rsplit ( '_' , 1 ) [ 1 ] elif '>' in cA or '>' in cB : cA = cA . split ( '>' ) [ 1 ] cB = cB . split ( '>' ) [ 1 ] if cA == cB : if cA not in wi : wi [ cA ] = [ ] wi [ cA ] . append ( pident ) else : if cA not in bt : bt [ cA ] = { } if cB not in bt [ cA ] : bt [ cA ] [ cB ] = [ ] bt [ cA ] [ cB ] . append ( pident ) print ( '\\\\n# min. within' ) for clade , pidents in list ( wi . items ( ) ) : print ( '\\\\t' . join ( [ 'wi:%s' % str ( i ) , clade , str ( min ( pidents ) ) ] ) ) comps = [ ] print ( '\\\\n# max. between' ) for comp in print_pairwise ( bt ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps ) comps = [ ] print ( '\\\\n# median between' ) for comp in print_pairwise ( bt , median = True ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps )\"], ['convert matrix to dictionary of comparisons', \"def matrix2dictionary ( matrix ) : pw = { } for line in matrix : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : names = line [ 1 : ] continue a = line [ 0 ] for i , pident in enumerate ( line [ 1 : ] ) : b = names [ i ] if a not in pw : pw [ a ] = { } if b not in pw : pw [ b ] = { } if pident != '-' : pident = float ( pident ) pw [ a ] [ b ] = pident pw [ b ] [ a ] = pident return pw\"], ['Set argument parser option .', \"def setoption ( parser , metadata = None ) : parser . add_argument ( '-v' , action = 'version' , version = __version__ ) subparsers = parser . add_subparsers ( help = 'sub commands help' ) create_cmd = subparsers . add_parser ( 'create' ) create_cmd . add_argument ( 'name' , help = 'Specify Python package name.' ) create_cmd . add_argument ( '-d' , dest = 'description' , action = 'store' , help = 'Short description about your package.' ) create_cmd . add_argument ( '-a' , dest = 'author' , action = 'store' , required = True , help = 'Python package author name.' ) create_cmd . add_argument ( '-e' , dest = 'email' , action = 'store' , required = True , help = 'Python package author email address.' ) create_cmd . add_argument ( '-l' , dest = 'license' , choices = metadata . licenses ( ) . keys ( ) , default = 'GPLv3+' , help = 'Specify license. (default: %(default)s)' ) create_cmd . add_argument ( '-s' , dest = 'status' , choices = metadata . status ( ) . keys ( ) , default = 'Alpha' , help = ( 'Specify development status. ' '(default: %(default)s)' ) ) create_cmd . add_argument ( '--no-check' , action = 'store_true' , help = 'No checking package name in PyPI.' ) create_cmd . add_argument ( '--with-samples' , action = 'store_true' , help = 'Generate package with sample code.' ) group = create_cmd . add_mutually_exclusive_group ( required = True ) group . add_argument ( '-U' , dest = 'username' , action = 'store' , help = 'Specify GitHub username.' ) group . add_argument ( '-u' , dest = 'url' , action = 'store' , type = valid_url , help = 'Python package homepage url.' ) create_cmd . add_argument ( '-o' , dest = 'outdir' , action = 'store' , default = os . path . abspath ( os . path . curdir ) , help = 'Specify output directory. (default: $PWD)' ) list_cmd = subparsers . add_parser ( 'list' ) list_cmd . add_argument ( '-l' , dest = 'licenses' , action = 'store_true' , help = 'show license choices.' )\"], ['Parse argument options .', \"def parse_options ( metadata ) : parser = argparse . ArgumentParser ( description = '%(prog)s usage:' , prog = __prog__ ) setoption ( parser , metadata = metadata ) return parser\"], ['Execute main processes .', \"def main ( ) : try : pkg_version = Update ( ) if pkg_version . updatable ( ) : pkg_version . show_message ( ) metadata = control . retreive_metadata ( ) parser = parse_options ( metadata ) argvs = sys . argv if len ( argvs ) <= 1 : parser . print_help ( ) sys . exit ( 1 ) args = parser . parse_args ( ) control . print_licences ( args , metadata ) control . check_repository_existence ( args ) control . check_package_existence ( args ) control . generate_package ( args ) except ( RuntimeError , BackendFailure , Conflict ) as exc : sys . stderr . write ( '{0}\\\\n' . format ( exc ) ) sys . exit ( 1 )\"], ['Check key and set default vaule when it does not exists .', \"def _check_or_set_default_params ( self ) : if not hasattr ( self , 'date' ) : self . _set_param ( 'date' , datetime . utcnow ( ) . strftime ( '%Y-%m-%d' ) ) if not hasattr ( self , 'version' ) : self . _set_param ( 'version' , self . default_version ) if not hasattr ( self , 'description' ) or self . description is None : getattr ( self , '_set_param' ) ( 'description' , self . warning_message )\"], ['Move directory from working directory to output directory .', 'def move ( self ) : if not os . path . isdir ( self . outdir ) : os . makedirs ( self . outdir ) shutil . move ( self . tmpdir , os . path . join ( self . outdir , self . name ) )'], ['Initialize VCS repository .', 'def vcs_init ( self ) : VCS ( os . path . join ( self . outdir , self . name ) , self . pkg_data )'], ['Finds the location of the current Steam installation on Windows machines . Returns None for any non - Windows machines or for Windows machines where Steam is not installed .', 'def find_steam_location ( ) : if registry is None : return None key = registry . CreateKey ( registry . HKEY_CURRENT_USER , \"Software\\\\Valve\\\\Steam\" ) return registry . QueryValueEx ( key , \"SteamPath\" ) [ 0 ]'], ['Plot PCoA principal coordinates scaled by the relative abundances of otu_name .', 'def plot_PCoA ( cat_data , otu_name , unifrac , names , colors , xr , yr , outDir , save_as , plot_style ) : fig = plt . figure ( figsize = ( 14 , 8 ) ) ax = fig . add_subplot ( 111 ) for i , cat in enumerate ( cat_data ) : plt . scatter ( cat_data [ cat ] [ \"pc1\" ] , cat_data [ cat ] [ \"pc2\" ] , cat_data [ cat ] [ \"size\" ] , color = colors [ cat ] , alpha = 0.85 , marker = \"o\" , edgecolor = \"black\" , label = cat ) lgnd = plt . legend ( loc = \"best\" , scatterpoints = 3 , fontsize = 13 ) for i in range ( len ( colors . keys ( ) ) ) : lgnd . legendHandles [ i ] . _sizes = [ 80 ] plt . title ( \" \" . join ( otu_name . split ( \"_\" ) ) , style = \"italic\" ) plt . ylabel ( \"PC2 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 1 ] ) ) ) plt . xlabel ( \"PC1 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 0 ] ) ) ) plt . xlim ( round ( xr [ 0 ] * 1.5 , 1 ) , round ( xr [ 1 ] * 1.5 , 1 ) ) plt . ylim ( round ( yr [ 0 ] * 1.5 , 1 ) , round ( yr [ 1 ] * 1.5 , 1 ) ) if plot_style : gu . ggplot2_style ( ax ) fc = \"0.8\" else : fc = \"none\" fig . savefig ( os . path . join ( outDir , \"_\" . join ( otu_name . split ( ) ) ) + \".\" + save_as , facecolor = fc , edgecolor = \"none\" , format = save_as , bbox_inches = \"tight\" , pad_inches = 0.2 ) plt . close ( fig )'], ['Split up the column data in a biom table by mapping category value .', \"def split_by_category ( biom_cols , mapping , category_id ) : columns = defaultdict ( list ) for i , col in enumerate ( biom_cols ) : columns [ mapping [ col [ 'id' ] ] [ category_id ] ] . append ( ( i , col ) ) return columns\"], ['print line if starts with ...', \"def print_line ( l ) : print_lines = [ '# STOCKHOLM' , '#=GF' , '#=GS' , ' ' ] if len ( l . split ( ) ) == 0 : return True for start in print_lines : if l . startswith ( start ) : return True return False\"], ['convert stockholm to single line format', \"def stock2one ( stock ) : lines = { } for line in stock : line = line . strip ( ) if print_line ( line ) is True : yield line continue if line . startswith ( '//' ) : continue ID , seq = line . rsplit ( ' ' , 1 ) if ID not in lines : lines [ ID ] = '' else : seq = seq . strip ( ) lines [ ID ] += seq for ID , line in lines . items ( ) : yield '\\\\t' . join ( [ ID , line ] ) yield '\\\\n//'\"], ['Statics the methods . wut .', \"def math_func ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 0 : return_type = type ( args [ 0 ] ) if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) args = list ( ( setify ( x ) for x in args ) ) return return_type ( f ( * args , ** kwargs ) ) return wrapper\"], ['Show stats when pings are done', 'def dump_stats ( myStats ) : print ( \"\\\\n----%s PYTHON PING Statistics----\" % ( myStats . thisIP ) ) if myStats . pktsSent > 0 : myStats . fracLoss = ( myStats . pktsSent - myStats . pktsRcvd ) / myStats . pktsSent print ( ( \"%d packets transmitted, %d packets received, \" \"%0.1f%% packet loss\" ) % ( myStats . pktsSent , myStats . pktsRcvd , 100.0 * myStats . fracLoss ) ) if myStats . pktsRcvd > 0 : print ( \"round-trip (ms)  min/avg/max = %d/%0.1f/%d\" % ( myStats . minTime , myStats . totTime / myStats . pktsRcvd , myStats . maxTime ) ) print ( \"\" ) return'], ['bootstrap - py package updatable? .', 'def updatable ( self ) : if self . latest_version > self . current_version : updatable_version = self . latest_version else : updatable_version = False return updatable_version'], ['Show message updatable .', \"def show_message ( self ) : print ( 'current version: {current_version}\\\\n' 'latest version : {latest_version}' . format ( current_version = self . current_version , latest_version = self . latest_version ) )\"], ['Traverse the input otu - sequence file collect the non - unique OTU IDs and file the sequences associated with then under the unique OTU ID as defined by the input matrix .', 'def condense_otus ( otuF , nuniqueF ) : uniqueOTUs = set ( ) nuOTUs = { } for line in nuniqueF : line = line . split ( ) uOTU = line [ 0 ] for nuOTU in line [ 1 : ] : nuOTUs [ nuOTU ] = uOTU uniqueOTUs . add ( uOTU ) otuFilter = defaultdict ( list ) for line in otuF : line = line . split ( ) otuID , seqIDs = line [ 0 ] , line [ 1 : ] if otuID in uniqueOTUs : otuFilter [ otuID ] . extend ( seqIDs ) elif otuID in nuOTUs : otuFilter [ nuOTUs [ otuID ] ] . extend ( seqIDs ) return otuFilter'], ['determine if read overlaps with rna if so count bases', 'def rna_bases ( rna_cov , scaffold , bases , line ) : start = int ( line [ 3 ] ) stop = start + bases - 1 if scaffold not in rna_cov : return rna_cov for pos in rna_cov [ scaffold ] [ 2 ] : ol = get_overlap ( [ start , stop ] , pos ) rna_cov [ scaffold ] [ 0 ] += ol return rna_cov'], ['parse ggKbase scaffold - to - bin mapping - scaffolds - to - bins and bins - to - scaffolds', \"def parse_s2bins ( s2bins ) : s2b = { } b2s = { } for line in s2bins : line = line . strip ( ) . split ( ) s , b = line [ 0 ] , line [ 1 ] if 'UNK' in b : continue if len ( line ) > 2 : g = ' ' . join ( line [ 2 : ] ) else : g = 'n/a' b = '%s\\\\t%s' % ( b , g ) s2b [ s ] = b if b not in b2s : b2s [ b ] = [ ] b2s [ b ] . append ( s ) return s2b , b2s\"], ['remove any bins that don t have 16S', 'def filter_missing_rna ( s2bins , bins2s , rna_cov ) : for bin , scaffolds in list ( bins2s . items ( ) ) : c = 0 for s in scaffolds : if s in rna_cov : c += 1 if c == 0 : del bins2s [ bin ] for scaffold , bin in list ( s2bins . items ( ) ) : if bin not in bins2s : del s2bins [ scaffold ] return s2bins , bins2s'], ['calculate bin coverage', 'def calc_bin_cov ( scaffolds , cov ) : bases = sum ( [ cov [ i ] [ 0 ] for i in scaffolds if i in cov ] ) length = sum ( [ cov [ i ] [ 1 ] for i in scaffolds if i in cov ] ) if length == 0 : return 0 return float ( float ( bases ) / float ( length ) )'], ['Make sure there is at least a translation has been filled in . If a default language has been specified make sure that it exists amongst translations .', \"def clean ( self ) : super ( TranslationFormSet , self ) . clean ( ) if settings . HIDE_LANGUAGE : return if len ( self . forms ) > 0 : if settings . DEFAULT_LANGUAGE and not any ( self . errors ) : for form in self . forms : language_code = form . cleaned_data . get ( 'language_code' , None ) if language_code == settings . DEFAULT_LANGUAGE : return raise forms . ValidationError ( _ ( 'No translation provided for default language \\\\'%s\\\\'.' ) % settings . DEFAULT_LANGUAGE ) else : raise forms . ValidationError ( _ ( 'At least one translation should be provided.' ) )\"], ['If a default language has been set and is still available in self . available_languages return it and remove it from the list .', \"def _get_default_language ( self ) : assert hasattr ( self , 'available_languages' ) , 'No available languages have been generated.' assert len ( self . available_languages ) > 0 , 'No available languages to select from.' if ( settings . DEFAULT_LANGUAGE and settings . DEFAULT_LANGUAGE in self . available_languages ) or ( 'language_code' not in self . form . base_fields ) : self . available_languages . remove ( settings . DEFAULT_LANGUAGE ) return settings . DEFAULT_LANGUAGE else : return self . available_languages . pop ( 0 )\"], ['Construct the form overriding the initial value for language_code .', \"def _construct_form ( self , i , ** kwargs ) : if not settings . HIDE_LANGUAGE : self . _construct_available_languages ( ) form = super ( TranslationFormSet , self ) . _construct_form ( i , ** kwargs ) if settings . HIDE_LANGUAGE : form . instance . language_code = settings . DEFAULT_LANGUAGE else : language_code = form . instance . language_code if language_code : logger . debug ( u'Removing translation choice %s for instance %s' u' in form %d' , language_code , form . instance , i ) self . available_languages . remove ( language_code ) else : initial_language_code = self . _get_default_language ( ) logger . debug ( u'Preselecting language code %s for form %d' , initial_language_code , i ) form . initial [ 'language_code' ] = initial_language_code return form\"], ['merge separate fastq files', 'def fq_merge ( R1 , R2 ) : c = itertools . cycle ( [ 1 , 2 , 3 , 4 ] ) for r1 , r2 in zip ( R1 , R2 ) : n = next ( c ) if n == 1 : pair = [ [ ] , [ ] ] pair [ 0 ] . append ( r1 . strip ( ) ) pair [ 1 ] . append ( r2 . strip ( ) ) if n == 4 : yield pair'], ['Creates hash ring .', \"def _build_circle ( self ) : total_weight = 0 for node in self . _nodes : total_weight += self . _weights . get ( node , 1 ) for node in self . _nodes : weight = self . _weights . get ( node , 1 ) ks = math . floor ( ( 40 * len ( self . _nodes ) * weight ) / total_weight ) for i in xrange ( 0 , int ( ks ) ) : b_key = self . _md5_digest ( '%s-%s-salt' % ( node , i ) ) for l in xrange ( 0 , 4 ) : key = ( ( b_key [ 3 + l * 4 ] << 24 ) | ( b_key [ 2 + l * 4 ] << 16 ) | ( b_key [ 1 + l * 4 ] << 8 ) | b_key [ l * 4 ] ) self . _hashring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )\"], ['Return long integer for a given key that represent it place on the hash ring .', 'def _gen_key ( self , key ) : b_key = self . _md5_digest ( key ) return self . _hashi ( b_key , lambda x : x )'], ['Returns True if there exists a custom image for app_id .', 'def has_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) return any ( map ( os . path . exists , possible_paths ) )'], ['Returns the custom image associated with a given app . If there are multiple candidate images on disk one is chosen arbitrarily .', 'def get_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) existing_images = filter ( os . path . exists , possible_paths ) if len ( existing_images ) > 0 : return existing_images [ 0 ]'], ['Sets the custom image for app_id to be the image located at image_path . If there already exists a custom image for app_id it will be deleted . Returns True is setting the image was successful .', 'def set_custom_image ( user_context , app_id , image_path ) : if image_path is None : return False if not os . path . exists ( image_path ) : return False ( root , ext ) = os . path . splitext ( image_path ) if not is_valid_extension ( ext ) : return False if has_custom_image ( user_context , app_id ) : img = get_custom_image ( user_context , app_id ) assert ( img is not None ) os . remove ( img ) parent_dir = paths . custom_images_directory ( user_context ) new_path = os . path . join ( parent_dir , app_id + ext ) shutil . copyfile ( image_path , new_path ) return True'], ['Read an orthography profile from a metadata file or a default tab - separated profile file .', \"def from_file ( cls , fname , form = None ) : try : tg = TableGroup . from_file ( fname ) opfname = None except JSONDecodeError : tg = TableGroup . fromvalue ( cls . MD ) opfname = fname if len ( tg . tables ) != 1 : raise ValueError ( 'profile description must contain exactly one table' ) metadata = tg . common_props metadata . update ( fname = Path ( fname ) , form = form ) return cls ( * [ { k : None if ( k != cls . GRAPHEME_COL and v == cls . NULL ) else v for k , v in d . items ( ) } for d in tg . tables [ 0 ] . iterdicts ( fname = opfname ) ] , ** metadata )\"], ['Create a Profile instance from the Unicode graphemes found in text .', \"def from_text ( cls , text , mapping = 'mapping' ) : graphemes = Counter ( grapheme_pattern . findall ( text ) ) specs = [ OrderedDict ( [ ( cls . GRAPHEME_COL , grapheme ) , ( 'frequency' , frequency ) , ( mapping , grapheme ) ] ) for grapheme , frequency in graphemes . most_common ( ) ] return cls ( * specs )\"], ['split fasta file into separate fasta files based on list of scaffolds that belong to each separate file', \"def split_fasta ( f , id2f ) : opened = { } for seq in parse_fasta ( f ) : id = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] if id not in id2f : continue fasta = id2f [ id ] if fasta not in opened : opened [ fasta ] = '%s.fa' % fasta seq [ 1 ] += '\\\\n' with open ( opened [ fasta ] , 'a+' ) as f_out : f_out . write ( '\\\\n' . join ( seq ) )\"], ['Check whether pathname is a valid user data directory', 'def _is_user_directory ( self , pathname ) : fullpath = os . path . join ( self . userdata_location ( ) , pathname ) return os . path . isdir ( fullpath ) and pathname . isdigit ( )'], ['Returns an array of user ids for users on the filesystem', 'def local_users ( self ) : userdirs = filter ( self . _is_user_directory , os . listdir ( self . userdata_location ( ) ) ) return map ( lambda userdir : user . User ( self , int ( userdir ) ) , userdirs )'], ['Calculates degree days starting with a series of temperature equivalent values', \"def _calculate_degree_days ( temperature_equivalent , base_temperature , cooling = False ) : if cooling : ret = temperature_equivalent - base_temperature else : ret = base_temperature - temperature_equivalent ret [ ret < 0 ] = 0 prefix = 'CDD' if cooling else 'HDD' ret . name = '{}_{}' . format ( prefix , base_temperature ) return ret\"], ['Development status .', \"def status ( self ) : return { self . _acronym_status ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_status ) }\"], ['OSI Approved license .', \"def licenses ( self ) : return { self . _acronym_lic ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Remove prefix .', \"def licenses_desc ( self ) : return { self . _acronym_lic ( l ) : l . split ( self . prefix_lic ) [ 1 ] for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Convert license acronym .', \"def _acronym_lic ( self , license_statement ) : pat = re . compile ( r'\\\\(([\\\\w+\\\\W?\\\\s?]+)\\\\)' ) if pat . search ( license_statement ) : lic = pat . search ( license_statement ) . group ( 1 ) if lic . startswith ( 'CNRI' ) : acronym_licence = lic [ : 4 ] else : acronym_licence = lic . replace ( ' ' , '' ) else : acronym_licence = '' . join ( [ w [ 0 ] for w in license_statement . split ( self . prefix_lic ) [ 1 ] . split ( ) ] ) return acronym_licence\"], ['calc MD5 based on path', \"def calcMD5 ( path ) : if os . path . exists ( path ) is False : yield False else : command = [ 'md5sum' , path ] p = Popen ( command , stdout = PIPE ) for line in p . communicate ( ) [ 0 ] . splitlines ( ) : yield line . decode ( 'ascii' ) . strip ( ) . split ( ) [ 0 ] p . wait ( ) yield False\"], ['download files with wget', \"def wget ( ftp , f = False , exclude = False , name = False , md5 = False , tries = 10 ) : if f is False : f = ftp . rsplit ( '/' , 1 ) [ - 1 ] t = 0 while md5check ( f , ftp , md5 , exclude ) is not True : t += 1 if name is not False : print ( '# downloading:' , name , f ) if exclude is False : command = 'wget -q --random-wait %s' % ( ftp ) else : command = 'wget -q --random-wait -R %s %s' % ( exclude , ftp ) p = Popen ( command , shell = True ) p . communicate ( ) if t >= tries : print ( 'not downloaded:' , name , f ) return [ f , False ] return [ f , True ]\"], ['check that at least one of queries is in list l', \"def check ( line , queries ) : line = line . strip ( ) spLine = line . replace ( '.' , ' ' ) . split ( ) matches = set ( spLine ) . intersection ( queries ) if len ( matches ) > 0 : return matches , line . split ( '\\\\t' ) return matches , False\"], ['search entrez using specified database and accession', \"def entrez ( db , acc ) : c1 = [ 'esearch' , '-db' , db , '-query' , acc ] c2 = [ 'efetch' , '-db' , 'BioSample' , '-format' , 'docsum' ] p1 = Popen ( c1 , stdout = PIPE , stderr = PIPE ) p2 = Popen ( c2 , stdin = p1 . stdout , stdout = PIPE , stderr = PIPE ) return p2 . communicate ( )\"], ['attempt to use NCBI Entrez to get BioSample ID', \"def searchAccession ( acc ) : out , error = entrez ( 'genome' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'nucleotide' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'assembly' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) for error in error . splitlines ( ) : error = error . decode ( 'ascii' ) . strip ( ) if '500 Can' in error : return ( False , acc , 'no network' ) return ( False , acc , 'efetch failed' )\"], ['download genome info from NCBI', \"def getFTPs ( accessions , ftp , search , exclude , convert = False , threads = 1 , attempt = 1 , max_attempts = 2 ) : info = wget ( ftp ) [ 0 ] allMatches = [ ] for genome in open ( info , encoding = 'utf8' ) : genome = str ( genome ) matches , genomeInfo = check ( genome , accessions ) if genomeInfo is not False : f = genomeInfo [ 0 ] + search Gftp = genomeInfo [ 19 ] Gftp = Gftp + '/' + search allMatches . extend ( matches ) yield ( Gftp , f , exclude , matches ) newAccs = [ ] missing = accessions . difference ( set ( allMatches ) ) if convert is True : pool = Pool ( threads ) pool = pool . imap_unordered ( searchAccession , missing ) for newAcc in tqdm ( pool , total = len ( missing ) ) : status , accession , newAcc = newAcc if status is True : newAccs . append ( newAcc ) print ( 'not found:' , accession , '->' , newAcc ) else : for accession in missing : print ( 'not found:' , accession ) if len ( newAccs ) > 0 and attempt <= max_attempts : print ( 'convert accession attempt' , attempt ) attempt += 1 for hit in getFTPs ( set ( newAccs ) , ftp , search , exclude , convert , threads = 1 , attempt = attempt ) : yield hit\"], ['download genomes from NCBI', \"def download ( args ) : accessions , infoFTP = set ( args [ 'g' ] ) , args [ 'i' ] search , exclude = args [ 's' ] , args [ 'e' ] FTPs = getFTPs ( accessions , infoFTP , search , exclude , threads = args [ 't' ] , convert = args [ 'convert' ] ) if args [ 'test' ] is True : for genome in FTPs : print ( 'found:' , ';' . join ( genome [ - 1 ] ) , genome [ 0 ] ) return FTPs pool = Pool ( args [ 't' ] ) pool = pool . imap_unordered ( wgetGenome , FTPs ) files = [ ] for f in tqdm ( pool , total = len ( accessions ) ) : files . append ( f ) return files\"], ['remove pesky characters from fasta file header', 'def fix_fasta ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 0 ] = remove_char ( seq [ 0 ] ) if len ( seq [ 1 ] ) > 0 : yield seq'], ['Compute a DataFrame summary of a Stats object .', \"def _calc_frames ( stats ) : timings = [ ] callers = [ ] for key , values in iteritems ( stats . stats ) : timings . append ( pd . Series ( key + values [ : - 1 ] , index = timing_colnames , ) ) for caller_key , caller_values in iteritems ( values [ - 1 ] ) : callers . append ( pd . Series ( key + caller_key + caller_values , index = caller_columns , ) ) timings_df = pd . DataFrame ( timings ) callers_df = pd . DataFrame ( callers ) timings_df [ 'filename:funcname' ] = ( timings_df [ 'filename' ] + ':' + timings_df [ 'funcname' ] ) timings_df = timings_df . groupby ( 'filename:funcname' ) . sum ( ) return timings_df , callers_df\"], ['get unmapped reads', \"def unmapped ( sam , mates ) : for read in sam : if read . startswith ( '@' ) is True : continue read = read . strip ( ) . split ( ) if read [ 2 ] == '*' and read [ 6 ] == '*' : yield read elif mates is True : if read [ 2 ] == '*' or read [ 6 ] == '*' : yield read for i in read : if i == 'YT:Z:UP' : yield read\"], ['execute jobs in processes using N threads', 'def parallel ( processes , threads ) : pool = multithread ( threads ) pool . map ( run_process , processes ) pool . close ( ) pool . join ( )'], ['the final log processor that structlog requires to render .', 'def define_log_renderer ( fmt , fpath , quiet ) : if fmt : return structlog . processors . JSONRenderer ( ) if fpath is not None : return structlog . processors . JSONRenderer ( ) if sys . stderr . isatty ( ) and not quiet : return structlog . dev . ConsoleRenderer ( ) return structlog . processors . JSONRenderer ( )'], ['Add unique id type and hostname', \"def _structlog_default_keys_processor ( logger_class , log_method , event ) : global HOSTNAME if 'id' not in event : event [ 'id' ] = '%s_%s' % ( datetime . utcnow ( ) . strftime ( '%Y%m%dT%H%M%S' ) , uuid . uuid1 ( ) . hex ) if 'type' not in event : event [ 'type' ] = 'log' event [ 'host' ] = HOSTNAME return event\"], ['log processors that structlog executes before final rendering', 'def define_log_processors ( ) : return [ structlog . processors . TimeStamper ( fmt = \"iso\" ) , _structlog_default_keys_processor , structlog . stdlib . PositionalArgumentsFormatter ( ) , structlog . processors . StackInfoRenderer ( ) , structlog . processors . format_exc_info , ]'], ['configures a logger when required write to stderr or a file', 'def _configure_logger ( fmt , quiet , level , fpath , pre_hooks , post_hooks , metric_grouping_interval ) : level = getattr ( logging , level . upper ( ) ) global _GLOBAL_LOG_CONFIGURED if _GLOBAL_LOG_CONFIGURED : return def wrap_hook ( fn ) : @ wraps ( fn ) def processor ( logger , method_name , event_dict ) : fn ( event_dict ) return event_dict return processor processors = define_log_processors ( ) processors . extend ( [ wrap_hook ( h ) for h in pre_hooks ] ) if metric_grouping_interval : processors . append ( metrics_grouping_processor ) log_renderer = define_log_renderer ( fmt , fpath , quiet ) stderr_required = ( not quiet ) pretty_to_stderr = ( stderr_required and ( fmt == \"pretty\" or ( fmt is None and sys . stderr . isatty ( ) ) ) ) should_inject_pretty_renderer = ( pretty_to_stderr and not isinstance ( log_renderer , structlog . dev . ConsoleRenderer ) ) if should_inject_pretty_renderer : stderr_required = False processors . append ( StderrConsoleRenderer ( ) ) processors . append ( log_renderer ) processors . extend ( [ wrap_hook ( h ) for h in post_hooks ] ) streams = [ ] if stderr_required : streams . append ( sys . stderr ) if fpath is not None : streams . append ( open ( fpath , \\'a\\' ) ) assert len ( streams ) != 0 , \"cannot configure logger for 0 streams\" stream = streams [ 0 ] if len ( streams ) == 1 else Stream ( * streams ) atexit . register ( stream . close ) structlog . configure ( processors = processors , context_class = dict , logger_factory = LevelLoggerFactory ( stream , level = level ) , wrapper_class = BoundLevelLogger , cache_logger_on_first_use = True , ) stdlib_root_log = logging . getLogger ( ) stdlib_root_log . addHandler ( StdlibStructlogHandler ( ) ) stdlib_root_log . setLevel ( level ) _GLOBAL_LOG_CONFIGURED = True'], ['Instead of using a processor adding basic information like caller filename etc here .', 'def _add_base_info ( self , event_dict ) : f = sys . _getframe ( ) level_method_frame = f . f_back caller_frame = level_method_frame . f_back return event_dict'], ['Propagate a method call to the wrapped logger .', \"def _proxy_to_logger ( self , method_name , event , * event_args , ** event_kw ) : if isinstance ( event , bytes ) : event = event . decode ( 'utf-8' ) if event_args : event_kw [ 'positional_args' ] = event_args return super ( BoundLevelLogger , self ) . _proxy_to_logger ( method_name , event = event , ** event_kw )\"], ['Given four points of a rectangle translate the rectangle to the specified x and y coordinates and optionally change the width .', 'def translate ( rect , x , y , width = 1 ) : return ( ( rect [ 0 ] [ 0 ] + x , rect [ 0 ] [ 1 ] + y ) , ( rect [ 1 ] [ 0 ] + x , rect [ 1 ] [ 1 ] + y ) , ( rect [ 2 ] [ 0 ] + x + width , rect [ 2 ] [ 1 ] + y ) , ( rect [ 3 ] [ 0 ] + x + width , rect [ 3 ] [ 1 ] + y ) )'], ['remove problem characters from string', \"def remove_bad ( string ) : remove = [ ':' , ',' , '(' , ')' , ' ' , '|' , ';' , '\\\\'' ] for c in remove : string = string . replace ( c , '_' ) return string\"], ['make copy of sequences with short identifier', \"def get_ids ( a ) : a_id = '%s.id.fa' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) a_id_lookup = '%s.id.lookup' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) if check ( a_id ) is True : return a_id , a_id_lookup a_id_f = open ( a_id , 'w' ) a_id_lookup_f = open ( a_id_lookup , 'w' ) ids = [ ] for seq in parse_fasta ( open ( a ) ) : id = id_generator ( ) while id in ids : id = id_generator ( ) ids . append ( id ) header = seq [ 0 ] . split ( '>' ) [ 1 ] name = remove_bad ( header ) seq [ 0 ] = '>%s %s' % ( id , header ) print ( '\\\\n' . join ( seq ) , file = a_id_f ) print ( '%s\\\\t%s\\\\t%s' % ( id , name , header ) , file = a_id_lookup_f ) return a_id , a_id_lookup\"], ['convert fasta to phylip because RAxML is ridiculous', 'def convert2phylip ( convert ) : out = \\'%s.phy\\' % ( convert . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if check ( out ) is False : convert = open ( convert , \\'rU\\' ) out_f = open ( out , \\'w\\' ) alignments = AlignIO . parse ( convert , \"fasta\" ) AlignIO . write ( alignments , out , \"phylip\" ) return out'], ['run IQ - Tree', 'def run_iqtree ( phy , model , threads , cluster , node ) : if threads > 24 : ppn = 24 else : ppn = threads tree = \\'%s.treefile\\' % ( phy ) if check ( tree ) is False : if model is False : model = \\'TEST\\' dir = os . getcwd ( ) command = \\'iqtree-omp -s %s -m %s -nt %s -quiet\\' % ( phy , model , threads ) if cluster is False : p = Popen ( command , shell = True ) else : if node is False : node = \\'1\\' qsub = \\'qsub -l nodes=%s:ppn=%s -m e -N iqtree\\' % ( node , ppn ) command = \\'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree\\' % ( dir , phy , command , dir ) re_call = \\'cd %s; %s --no-fast --iq\\' % ( dir . rsplit ( \\'/\\' , 1 ) [ 0 ] , \\' \\' . join ( sys . argv ) ) p = Popen ( \\'echo \"%s;%s\" | %s\\' % ( command , re_call , qsub ) , shell = True ) p . communicate ( ) return tree'], ['get the names for sequences in the raxml tree', \"def fix_tree ( tree , a_id_lookup , out ) : if check ( out ) is False and check ( tree ) is True : tree = open ( tree ) . read ( ) for line in open ( a_id_lookup ) : id , name , header = line . strip ( ) . split ( '\\\\t' ) tree = tree . replace ( id + ':' , name + ':' ) out_f = open ( out , 'w' ) print ( tree . strip ( ) , file = out_f ) return out\"], ['Creates a new Nydus cluster from the given settings .', \"def create_cluster ( settings ) : settings = copy . deepcopy ( settings ) backend = settings . pop ( 'engine' , settings . pop ( 'backend' , None ) ) if isinstance ( backend , basestring ) : Conn = import_string ( backend ) elif backend : Conn = backend else : raise KeyError ( 'backend' ) cluster = settings . pop ( 'cluster' , None ) if not cluster : Cluster = Conn . get_cluster ( ) elif isinstance ( cluster , basestring ) : Cluster = import_string ( cluster ) else : Cluster = cluster router = settings . pop ( 'router' , None ) if not router : Router = BaseRouter elif isinstance ( router , basestring ) : Router = import_string ( router ) else : Router = router return Cluster ( router = Router , backend = Conn , ** settings )\"], ['Gets the translation of a specific field for a specific language code .', \"def _get_translation ( self , field , code ) : if not code in self . _translation_cache : translations = self . translations . select_related ( ) logger . debug ( u'Matched with field %s for language %s. Attempting lookup.' , field , code ) try : translation_obj = translations . get ( language_code = code ) except ObjectDoesNotExist : translation_obj = None self . _translation_cache [ code ] = translation_obj logger . debug ( u'Translation not found in cache.' ) else : logger . debug ( u'Translation found in cache.' ) translation_obj = self . _translation_cache . get ( code ) if not translation_obj : raise ObjectDoesNotExist field_value = getattr ( translation_obj , field ) logger . debug ( u'Found translation object %s, returning value %s.' , translation_obj , field_value ) return field_value\"], ['Wrapper to allow for easy unicode representation of an object by the specified property . If this wrapper is not able to find the right translation of the specified property it will return the default value instead .', \"def unicode_wrapper ( self , property , default = ugettext ( 'Untitled' ) ) : try : value = getattr ( self , property ) except ValueError : logger . warn ( u'ValueError rendering unicode for %s object.' , self . _meta . object_name ) value = None if not value : value = default return value\"], ['remove insertion columns from aligned fasta file', \"def strip_inserts ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 1 ] = '' . join ( [ b for b in seq [ 1 ] if b == '-' or b . isupper ( ) ] ) yield seq\"], ['Transform a string s graphemes into the mappings given in a different column in the orthography profile .', 'def transform ( self , word , column = Profile . GRAPHEME_COL , error = errors . replace ) : assert self . op , \\'method can only be called with orthography profile.\\' if column != Profile . GRAPHEME_COL and column not in self . op . column_labels : raise ValueError ( \"Column {0} not found in profile.\" . format ( column ) ) word = self . op . tree . parse ( word , error ) if column == Profile . GRAPHEME_COL : return word out = [ ] for token in word : try : target = self . op . graphemes [ token ] [ column ] except KeyError : target = self . _errors [ \\'replace\\' ] ( token ) if target is not None : if isinstance ( target , ( tuple , list ) ) : out . extend ( target ) else : out . append ( target ) return out'], ['Function to tokenize input string and return output of str with ortho rules applied .', 'def rules ( self , word ) : return self . _rules . apply ( word ) if self . _rules else word'], ['Given a string that is space - delimited on Unicode grapheme clusters group Unicode modifier letters with their preceding base characters deal with tie bars etc .', 'def combine_modifiers ( self , graphemes ) : result = [ ] temp = \"\" count = len ( graphemes ) for grapheme in reversed ( graphemes ) : count -= 1 if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Lm\" and not ord ( grapheme ) in [ 712 , 716 ] : temp = grapheme + temp if count == 0 : result [ - 1 ] = temp + result [ - 1 ] continue if len ( grapheme ) == 1 and ord ( grapheme ) in [ 712 , 716 ] : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Sk\" : if len ( result ) == 0 : result . append ( grapheme ) temp = \"\" continue else : if unicodedata . category ( result [ - 1 ] [ 0 ] ) == \"Sk\" : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue result . append ( grapheme + temp ) temp = \"\" segments = result [ : : - 1 ] i = 0 r = [ ] while i < len ( segments ) : if ord ( segments [ i ] [ - 1 ] ) in [ 865 , 860 ] : r . append ( segments [ i ] + segments [ i + 1 ] ) i += 2 else : r . append ( segments [ i ] ) i += 1 return r'], ['parse catalytic RNAs to gff format', \"def parse_catalytic ( insertion , gff ) : offset = insertion [ 'offset' ] GeneStrand = insertion [ 'strand' ] if type ( insertion [ 'intron' ] ) is not str : return gff for intron in parse_fasta ( insertion [ 'intron' ] . split ( '|' ) ) : ID , annot , strand , pos = intron [ 0 ] . split ( '>' ) [ 1 ] . split ( ) Start , End = [ int ( i ) for i in pos . split ( '-' ) ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Rfam' ) gff [ 'feature' ] . append ( 'Catalytic RNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse ORF to gff format', \"def parse_orf ( insertion , gff ) : offset = insertion [ 'offset' ] if type ( insertion [ 'orf' ] ) is not str : return gff for orf in parse_fasta ( insertion [ 'orf' ] . split ( '|' ) ) : ID = orf [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End , strand = [ int ( i ) for i in orf [ 0 ] . split ( ' # ' ) [ 1 : 4 ] ] if strand == 1 : strand = '+' else : strand = '-' GeneStrand = insertion [ 'strand' ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 annot = orf [ 0 ] . split ( ) [ 1 ] if annot == 'n/a' : annot = 'unknown' gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Prodigal and Pfam' ) gff [ 'feature' ] . append ( 'CDS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse insertion to gff format', \"def parse_insertion ( insertion , gff ) : offset = insertion [ 'offset' ] for ins in parse_fasta ( insertion [ 'insertion sequence' ] . split ( '|' ) ) : strand = insertion [ 'strand' ] ID = ins [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End = [ int ( i ) for i in ins [ 0 ] . split ( 'gene-pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] Start , End = abs ( Start + offset ) , abs ( End + offset ) if strand == '-' : Start , End = End , Start gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( insertion [ 'source' ] ) gff [ 'feature' ] . append ( 'IVS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s' % ( ID ) ) return gff\"], ['parse rRNA to gff format', \"def parse_rRNA ( insertion , seq , gff ) : offset = insertion [ 'offset' ] strand = insertion [ 'strand' ] for rRNA in parse_masked ( seq , 0 ) [ 0 ] : rRNA = '' . join ( rRNA ) Start = seq [ 1 ] . find ( rRNA ) + 1 End = Start + len ( rRNA ) - 1 if strand == '-' : Start , End = End - 2 , Start - 2 pos = ( abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 ) Start , End = min ( pos ) , max ( pos ) source = insertion [ 'source' ] annot = '%s rRNA' % ( source . split ( 'from' , 1 ) [ 0 ] ) gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'rRNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'Name=%s' % ( annot ) ) return gff\"], ['convert iTable to gff file', \"def iTable2GFF ( iTable , fa , contig = False ) : columns = [ '#seqname' , 'source' , 'feature' , 'start' , 'end' , 'score' , 'strand' , 'frame' , 'attribute' ] gff = { c : [ ] for c in columns } for insertion in iTable . iterrows ( ) : insertion = insertion [ 1 ] if insertion [ 'ID' ] not in fa : continue strand = insertion [ 'sequence' ] . split ( 'strand=' , 1 ) [ 1 ] . split ( ) [ 0 ] if contig is True : gene = [ int ( i ) for i in insertion [ 'sequence' ] . split ( 'pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] if strand == '-' : offset = - 1 * ( gene [ 1 ] ) else : offset = gene [ 0 ] else : strand = '+' gene = [ 1 , int ( insertion [ 'sequence' ] . split ( 'total-len=' , 1 ) [ 1 ] . split ( ) [ 0 ] ) ] offset = gene [ 0 ] insertion [ 'strand' ] = strand insertion [ 'offset' ] = offset source = insertion [ 'sequence' ] . split ( '::model' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ - 1 ] insertion [ 'source' ] = source geneAnnot = '%s rRNA gene' % ( source . split ( 'from' , 1 ) [ 0 ] ) geneNum = insertion [ 'sequence' ] . split ( 'seq=' , 1 ) [ 1 ] . split ( ) [ 0 ] gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'Gene' ) gff [ 'start' ] . append ( gene [ 0 ] ) gff [ 'end' ] . append ( gene [ 1 ] ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( geneNum , geneAnnot ) ) gff = parse_rRNA ( insertion , fa [ insertion [ 'ID' ] ] , gff ) gff = parse_insertion ( insertion , gff ) gff = parse_orf ( insertion , gff ) gff = parse_catalytic ( insertion , gff ) return pd . DataFrame ( gff ) [ columns ] . drop_duplicates ( )\"], ['Given an abundance table group the counts by every taxonomic level .', \"def summarize_taxa ( biom ) : tamtcounts = defaultdict ( int ) tot_seqs = 0.0 for row , col , amt in biom [ 'data' ] : tot_seqs += amt rtax = biom [ 'rows' ] [ row ] [ 'metadata' ] [ 'taxonomy' ] for i , t in enumerate ( rtax ) : t = t . strip ( ) if i == len ( rtax ) - 1 and len ( t ) > 3 and len ( rtax [ - 1 ] ) > 3 : t = 's__' + rtax [ i - 1 ] . strip ( ) . split ( '_' ) [ - 1 ] + '_' + t . split ( '_' ) [ - 1 ] tamtcounts [ t ] += amt lvlData = { lvl : levelData ( tamtcounts , tot_seqs , lvl ) for lvl in [ 'k' , 'p' , 'c' , 'o' , 'f' , 'g' , 's' ] } return tot_seqs , lvlData\"], ['Returns the path to the custom image set for this game or None if no image is set', 'def custom_image ( self , user ) : for ext in self . valid_custom_image_extensions ( ) : image_location = self . _custom_image_path ( user , ext ) if os . path . isfile ( image_location ) : return image_location return None'], ['Sets a custom image for the game . image_path should refer to an image file on disk', 'def set_image ( self , user , image_path ) : _ , ext = os . path . splitext ( image_path ) shutil . copy ( image_path , self . _custom_image_path ( user , ext ) )'], ['get a list of mapped reads', \"def sam_list ( sam ) : list = [ ] for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : list . append ( id ) return set ( list )\"], ['get a list of mapped reads require that both pairs are mapped in the sam file in order to remove the reads', \"def sam_list_paired ( sam ) : list = [ ] pair = [ '1' , '2' ] prev = '' for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : read = id . rsplit ( '/' ) [ 0 ] if read == prev : list . append ( read ) prev = read return set ( list )\"], ['require that both pairs are mapped in the sam file in order to remove the reads', \"def filter_paired ( list ) : pairs = { } filtered = [ ] for id in list : read = id . rsplit ( '/' ) [ 0 ] if read not in pairs : pairs [ read ] = [ ] pairs [ read ] . append ( id ) for read in pairs : ids = pairs [ read ] if len ( ids ) == 2 : filtered . extend ( ids ) return set ( filtered )\"], ['print fastq from sam', \"def sam2fastq ( line ) : fastq = [ ] fastq . append ( '@%s' % line [ 0 ] ) fastq . append ( line [ 9 ] ) fastq . append ( '+%s' % line [ 0 ] ) fastq . append ( line [ 10 ] ) return fastq\"], ['- check to see if the read maps with < = threshold number of mismatches - mm_option = one or both depending on whether or not one or both reads in a pair need to pass the mismatch threshold - pair can be False if read does not have a pair - make sure alignment score is not 0 which would indicate that the read was not aligned to the reference', \"def check_mismatches ( read , pair , mismatches , mm_option , req_map ) : if pair is False : mm = count_mismatches ( read ) if mm is False : return False if mismatches is False : return True if mm <= mismatches : return True r_mm = count_mismatches ( read ) p_mm = count_mismatches ( pair ) if r_mm is False and p_mm is False : return False if mismatches is False : return True if req_map is True : if r_mm is False or p_mm is False : return False if mm_option == 'one' : if ( r_mm is not False and r_mm <= mismatches ) or ( p_mm is not False and p_mm <= mismatches ) : return True if mm_option == 'both' : if r_mm is False : if p_mm <= mismatches : return True elif p_mm is False : if r_mm <= mismatches : return True elif ( r_mm is not False and r_mm <= mismatches ) and ( p_mm is not False and p_mm <= mismatches ) : return True return False\"], ['determine whether or not reads map to specific region of scaffold', 'def check_region ( read , pair , region ) : if region is False : return True for mapping in read , pair : if mapping is False : continue start , length = int ( mapping [ 3 ] ) , len ( mapping [ 9 ] ) r = [ start , start + length - 1 ] if get_overlap ( r , region ) > 0 : return True return False'], ['Returns a Steam object representing the current Steam installation on the users computer . If the user doesn t have Steam installed returns None .', \"def get_steam ( ) : helper = lambda udd : Steam ( udd ) if os . path . exists ( udd ) else None plat = platform . system ( ) if plat == 'Darwin' : return helper ( paths . default_osx_userdata_path ( ) ) if plat == 'Linux' : return helper ( paths . default_linux_userdata_path ( ) ) if plat == 'Windows' : possible_dir = winutils . find_userdata_directory ( ) return helper ( possible_dir ) if possible_dir is not None else None return None\"], ['normalize from zero to one for row or table', \"def zero_to_one ( table , option ) : if option == 'table' : m = min ( min ( table ) ) ma = max ( max ( table ) ) t = [ ] for row in table : t_row = [ ] if option != 'table' : m , ma = min ( row ) , max ( row ) for i in row : if ma == m : t_row . append ( 0 ) else : t_row . append ( ( i - m ) / ( ma - m ) ) t . append ( t_row ) return t\"], ['calculate percent of total', \"def pertotal ( table , option ) : if option == 'table' : total = sum ( [ i for line in table for i in line ] ) t = [ ] for row in table : t_row = [ ] if option != 'table' : total = sum ( row ) for i in row : if total == 0 : t_row . append ( 0 ) else : t_row . append ( i / total * 100 ) t . append ( t_row ) return t\"], ['scale table based on the column with the largest sum', 'def scale ( table ) : t = [ ] columns = [ [ ] for i in table [ 0 ] ] for row in table : for i , v in enumerate ( row ) : columns [ i ] . append ( v ) sums = [ float ( sum ( i ) ) for i in columns ] scale_to = float ( max ( sums ) ) scale_factor = [ scale_to / i for i in sums if i != 0 ] for row in table : t . append ( [ a * b for a , b in zip ( row , scale_factor ) ] ) return t'], ['fit to normal distribution', \"def norm ( table ) : print ( '# norm dist is broken' , file = sys . stderr ) exit ( ) from matplotlib . pyplot import hist as hist t = [ ] for i in table : t . append ( np . ndarray . tolist ( hist ( i , bins = len ( i ) , normed = True ) [ 0 ] ) ) return t\"], ['log transform each value in table', 'def log_trans ( table ) : t = [ ] all = [ item for sublist in table for item in sublist ] if min ( all ) == 0 : scale = min ( [ i for i in all if i != 0 ] ) * 10e-10 else : scale = 0 for i in table : t . append ( np . ndarray . tolist ( np . log10 ( [ j + scale for j in i ] ) ) ) return t'], ['box - cox transform table', 'def box_cox ( table ) : from scipy . stats import boxcox as bc t = [ ] for i in table : if min ( i ) == 0 : scale = min ( [ j for j in i if j != 0 ] ) * 10e-10 else : scale = 0 t . append ( np . ndarray . tolist ( bc ( np . array ( [ j + scale for j in i ] ) ) [ 0 ] ) ) return t'], ['inverse hyperbolic sine transformation', 'def inh ( table ) : t = [ ] for i in table : t . append ( np . ndarray . tolist ( np . arcsinh ( i ) ) ) return t'], ['from SparCC - randomly draw from the corresponding posterior Dirichlet distribution with a uniform prior', 'def diri ( table ) : t = [ ] for i in table : a = [ j + 1 for j in i ] t . append ( np . ndarray . tolist ( np . random . mtrand . dirichlet ( a ) ) ) return t'], ['Given a list of sample IDs generate unique n - base barcodes for each . Note that only 4^n unique barcodes are possible .', \"def generate_barcodes ( nIds , codeLen = 12 ) : def next_code ( b , c , i ) : return c [ : i ] + b + ( c [ i + 1 : ] if i < - 1 else '' ) def rand_base ( ) : return random . choice ( [ 'A' , 'T' , 'C' , 'G' ] ) def rand_seq ( n ) : return '' . join ( [ rand_base ( ) for _ in range ( n ) ] ) hpf = re . compile ( 'aaaa|cccc|gggg|tttt' , re . IGNORECASE ) while True : codes = [ rand_seq ( codeLen ) ] if ( hpf . search ( codes [ 0 ] ) is None ) : break idx = 0 while len ( codes ) < nIds : idx -= 1 if idx < - codeLen : idx = - 1 codes . append ( rand_seq ( codeLen ) ) else : nc = next_code ( rand_base ( ) , codes [ - 1 ] , idx ) if hpf . search ( nc ) is None : codes . append ( nc ) codes = list ( set ( codes ) ) return codes\"], ['Given a sample ID and a mapping modify a Sanger FASTA file to include the barcode and primer in the sequence data and change the description line as needed .', \"def scrobble_data_dir ( dataDir , sampleMap , outF , qualF = None , idopt = None , utf16 = False ) : seqcount = 0 outfiles = [ osp . split ( outF . name ) [ 1 ] ] if qualF : outfiles . append ( osp . split ( qualF . name ) [ 1 ] ) for item in os . listdir ( dataDir ) : if item in outfiles or not osp . isfile ( os . path . join ( dataDir , item ) ) : continue if osp . splitext ( item ) [ 1 ] in file_types [ 'fasta' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'fasta' ) for record in records : if isinstance ( idopt , tuple ) : sep , field = idopt sampleID = record . id . split ( sep ) [ field - 1 ] else : sampleID = osp . splitext ( item ) [ 0 ] record . seq = ( sampleMap [ sampleID ] . barcode + sampleMap [ sampleID ] . primer + record . seq ) SeqIO . write ( record , outF , 'fasta' ) seqcount += 1 fh . close ( ) elif qualF and osp . splitext ( item ) [ 1 ] in file_types [ 'qual' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'qual' ) for record in records : mi = sampleMap [ sampleMap . keys ( ) [ 0 ] ] quals = [ 40 for _ in range ( len ( mi . barcode ) + len ( mi . primer ) ) ] record . letter_annotations [ 'phred_quality' ] [ 0 : 0 ] = quals SeqIO . write ( record , qualF , 'qual' ) fh . close ( ) return seqcount\"], ['Applies the arcsine square root transform to the given BIOM - format table', 'def arcsin_sqrt ( biom_tbl ) : arcsint = lambda data , id_ , md : np . arcsin ( np . sqrt ( data ) ) tbl_relabd = relative_abd ( biom_tbl ) tbl_asin = tbl_relabd . transform ( arcsint , inplace = False ) return tbl_asin'], ['parse sam file and check mapping quality', \"def parse_sam ( sam , qual ) : for line in sam : if line . startswith ( '@' ) : continue line = line . strip ( ) . split ( ) if int ( line [ 4 ] ) == 0 or int ( line [ 4 ] ) < qual : continue yield line\"], ['reverse completement stats', \"def rc_stats ( stats ) : rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } rcs = [ ] for pos in reversed ( stats ) : rc = { } rc [ 'reference frequencey' ] = pos [ 'reference frequency' ] rc [ 'consensus frequencey' ] = pos [ 'consensus frequency' ] rc [ 'In' ] = pos [ 'In' ] rc [ 'Del' ] = pos [ 'Del' ] rc [ 'ref' ] = rc_nucs [ pos [ 'ref' ] ] rc [ 'consensus' ] = ( rc_nucs [ pos [ 'consensus' ] [ 0 ] ] , pos [ 'consensus' ] [ 1 ] ) for base , stat in list ( pos . items ( ) ) : if base in rc_nucs : rc [ rc_nucs [ base ] ] = stat rcs . append ( rc ) return rcs\"], ['parse codon nucleotide positions in range start - > end wrt strand', 'def parse_codons ( ref , start , end , strand ) : codon = [ ] c = cycle ( [ 1 , 2 , 3 ] ) ref = ref [ start - 1 : end ] if strand == - 1 : ref = rc_stats ( ref ) for pos in ref : n = next ( c ) codon . append ( pos ) if n == 3 : yield codon codon = [ ]'], ['calculate coverage for positions in range start - > end', 'def calc_coverage ( ref , start , end , length , nucs ) : ref = ref [ start - 1 : end ] bases = 0 for pos in ref : for base , count in list ( pos . items ( ) ) : if base in nucs : bases += count return float ( bases ) / float ( length )'], ['parse gbk file', \"def parse_gbk ( gbks ) : for gbk in gbks : for record in SeqIO . parse ( open ( gbk ) , 'genbank' ) : for feature in record . features : if feature . type == 'gene' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : continue if feature . type == 'CDS' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : pass start = int ( feature . location . start ) + int ( feature . qualifiers [ 'codon_start' ] [ 0 ] ) end , strand = int ( feature . location . end ) , feature . location . strand if strand is None : strand = 1 else : strand = - 1 contig = record . id yield contig , [ locus , [ start , end , strand ] , feature . qualifiers ]\"], ['parse gene call information from Prodigal fasta output', \"def parse_fasta_annotations ( fastas , annot_tables , trans_table ) : if annot_tables is not False : annots = { } for table in annot_tables : for cds in open ( table ) : ID , start , end , strand = cds . strip ( ) . split ( ) annots [ ID ] = [ start , end , int ( strand ) ] for fasta in fastas : for seq in parse_fasta ( fasta ) : if ( '# ;gc_cont' not in seq [ 0 ] and '# ID=' not in seq [ 0 ] ) and annot_tables is False : print ( '# specify fasta from Prodigal or annotations table (-t)' , file = sys . stderr ) exit ( ) if 'ID=' in seq [ 0 ] : ID = seq [ 0 ] . rsplit ( 'ID=' , 1 ) [ 1 ] . split ( ';' , 1 ) [ 0 ] contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_%s' % ( ID ) , 1 ) [ 0 ] else : contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_' , 1 ) [ 0 ] locus = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] if ( '# ;gc_cont' in seq [ 0 ] or '# ID=' in seq [ 0 ] ) : info = seq [ 0 ] . split ( ' # ' ) start , end , strand = int ( info [ 1 ] ) , int ( info [ 2 ] ) , info [ 3 ] if strand == '1' : strand = 1 else : strand = - 1 product = [ '' . join ( info [ 4 ] . split ( ) [ 1 : ] ) ] else : start , end , strand = annots [ locus ] product = seq [ 0 ] . split ( ' ' , 1 ) [ 1 ] info = { 'transl_table' : [ trans_table ] , 'translation' : [ seq [ 1 ] ] , 'product' : product } yield contig , [ locus , [ start , end , strand ] , info ]\"], ['parse annotations in either gbk or Prodigal fasta format', 'def parse_annotations ( annots , fmt , annot_tables , trans_table ) : annotations = { } if fmt is False : for contig , feature in parse_gbk ( annots ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) else : for contig , feature in parse_fasta_annotations ( annots , annot_tables , trans_table ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) return annotations'], ['convert codon to amino acid', \"def codon2aa ( codon , trans_table ) : return Seq ( '' . join ( codon ) , IUPAC . ambiguous_dna ) . translate ( table = trans_table ) [ 0 ]\"], ['find consensus base based on nucleotide frequencies', \"def find_consensus ( bases ) : nucs = [ 'A' , 'T' , 'G' , 'C' , 'N' ] total = sum ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) try : top = max ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) except : bases [ 'consensus' ] = ( 'N' , 'n/a' ) bases [ 'consensus frequency' ] = 'n/a' bases [ 'reference frequency' ] = 'n/a' return bases top = [ ( nuc , bases [ nuc ] ) for nuc in bases if bases [ nuc ] == top ] if top [ 0 ] [ 1 ] == 0 : bases [ 'consensus' ] = ( 'n/a' , 0 ) else : bases [ 'consensus' ] = random . choice ( top ) if total == 0 : c_freq = 'n/a' ref_freq = 'n/a' else : c_freq = float ( bases [ 'consensus' ] [ 1 ] ) / float ( total ) if bases [ 'ref' ] not in bases : ref_freq = 0 else : ref_freq = float ( bases [ bases [ 'ref' ] ] ) / float ( total ) bases [ 'consensus frequency' ] = c_freq bases [ 'reference frequency' ] = ref_freq return bases\"], ['print consensensus sequences for each genome and sample', \"def print_consensus ( genomes ) : cons = { } for genome , contigs in list ( genomes . items ( ) ) : cons [ genome ] = { } for contig , samples in list ( contigs . items ( ) ) : for sample , stats in list ( samples . items ( ) ) : if sample not in cons [ genome ] : cons [ genome ] [ sample ] = { } seq = cons [ genome ] [ sample ] [ contig ] = [ ] for pos , ps in enumerate ( stats [ 'bp_stats' ] , 1 ) : ref , consensus = ps [ 'ref' ] , ps [ 'consensus' ] [ 0 ] if consensus == 'n/a' : consensus = ref . lower ( ) seq . append ( consensus ) for genome , samples in cons . items ( ) : for sample , contigs in samples . items ( ) : fn = '%s.%s.consensus.fa' % ( genome , sample ) f = open ( fn , 'w' ) for contig , seq in contigs . items ( ) : print ( '>%s' % ( contig ) , file = f ) print ( '' . join ( seq ) , file = f ) f . close ( ) return cons\"], ['calculate genome coverage from scaffold coverage table', \"def parse_cov ( cov_table , scaffold2genome ) : size = { } mapped = { } for line in open ( cov_table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : samples = line [ 1 : ] samples = [ i . rsplit ( '/' , 1 ) [ - 1 ] . split ( '.' , 1 ) [ 0 ] for i in samples ] continue scaffold , length = line [ 0 ] . split ( ': ' ) length = float ( length ) covs = [ float ( i ) for i in line [ 1 : ] ] bases = [ c * length for c in covs ] if scaffold not in scaffold2genome : continue genome = scaffold2genome [ scaffold ] if genome not in size : size [ genome ] = 0 mapped [ genome ] = { sample : 0 for sample in samples } size [ genome ] += length for sample , count in zip ( samples , bases ) : mapped [ genome ] [ sample ] += count coverage = { 'genome' : [ ] , 'genome size (bp)' : [ ] , 'sample' : [ ] , 'coverage' : [ ] } for genome , length in size . items ( ) : for sample in samples : cov = mapped [ genome ] [ sample ] / length coverage [ 'genome' ] . append ( genome ) coverage [ 'genome size (bp)' ] . append ( length ) coverage [ 'sample' ] . append ( sample ) coverage [ 'coverage' ] . append ( cov ) return pd . DataFrame ( coverage )\"], ['calculate genome coverage from scaffold coverage', 'def genome_coverage ( covs , s2b ) : COV = [ ] for cov in covs : COV . append ( parse_cov ( cov , s2b ) ) return pd . concat ( COV )'], ['convert s2b files to dictionary', \"def parse_s2bs ( s2bs ) : s2b = { } for s in s2bs : for line in open ( s ) : line = line . strip ( ) . split ( '\\\\t' ) s , b = line [ 0 ] , line [ 1 ] s2b [ s ] = b return s2b\"], ['convert fastas to s2b dictionary', \"def fa2s2b ( fastas ) : s2b = { } for fa in fastas : for seq in parse_fasta ( fa ) : s = seq [ 0 ] . split ( '>' , 1 ) [ 1 ] . split ( ) [ 0 ] s2b [ s ] = fa . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 1 ) [ 0 ] return s2b\"], ['Filters out sequences with too much ambiguity as defined by the method parameters .', \"def filter_ambiguity ( records , percent = 0.5 ) : seqs = [ ] count = 0 for record in records : if record . seq . count ( 'N' ) / float ( len ( record ) ) < percent : seqs . append ( record ) count += 1 return seqs , count\"], ['Search package .', 'def package_existent ( name ) : try : response = requests . get ( PYPI_URL . format ( name ) ) if response . ok : msg = ( \\'[error] \"{0}\" is registered already in PyPI.\\\\n\\' \\'\\\\tSpecify another package name.\\' ) . format ( name ) raise Conflict ( msg ) except ( socket . gaierror , Timeout , ConnectionError , HTTPError ) as exc : raise BackendFailure ( exc )'], ['add index to id to make it unique wrt ids', \"def append_index_id ( id , ids ) : index = 1 mod = '%s_%s' % ( id , index ) while mod in ids : index += 1 mod = '%s_%s' % ( id , index ) ids . append ( mod ) return mod , ids\"], ['de - replicate fastas based on sequence names', \"def de_rep ( fastas , append_index , return_original = False ) : ids = [ ] for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) id = header [ 0 ] if id not in ids : ids . append ( id ) if return_original is True : yield [ header , seq ] else : yield seq elif append_index == True : new , ids = append_index_id ( id , ids ) if return_original is True : yield [ header , [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ] ] else : yield [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ]\"], ['Request data associated with postcode .', \"def get ( postcode ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) url = '%s/postcode/%s.json' % ( END_POINT , postcode ) return _get_json_resp ( url )\"], ['Request all postcode data within distance miles of postcode .', \"def get_from_postcode ( postcode , distance ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) return _get_from ( distance , 'postcode=%s' % postcode )\"], ['Checks if latitude and longitude correct', 'def _check_point ( self , lat , lng ) : if abs ( lat ) > 90 or abs ( lng ) > 180 : msg = \"Illegal lat and/or lng, (%s, %s) provided.\" % ( lat , lng ) raise IllegalPointException ( msg )'], ['Checks for cached responses before requesting from web - service', 'def _lookup ( self , skip_cache , fun , * args , ** kwargs ) : if args not in self . cache or skip_cache : self . cache [ args ] = fun ( * args , ** kwargs ) return self . cache [ args ]'], ['Calls postcodes . get_nearest but checks correctness of lat and long and by default utilises a local cache .', 'def get_nearest ( self , lat , lng , skip_cache = False ) : lat , lng = float ( lat ) , float ( lng ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_nearest , lat , lng )'], ['Calls postcodes . get_from_postcode but checks correctness of distance and by default utilises a local cache .', 'def get_from_postcode ( self , postcode , distance , skip_cache = False ) : distance = float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) postcode = postcode . lower ( ) . replace ( \\' \\' , \\'\\' ) return self . _lookup ( skip_cache , get_from_postcode , postcode , float ( distance ) )'], ['Calls postcodes . get_from_geo but checks the correctness of all arguments and by default utilises a local cache .', 'def get_from_geo ( self , lat , lng , distance , skip_cache = False ) : lat , lng , distance = float ( lat ) , float ( lng ) , float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_from_geo , lat , lng , distance )'], ['get coordinates of insertions from insertion - masked sequence', 'def insertions_from_masked ( seq ) : insertions = [ ] prev = True for i , base in enumerate ( seq ) : if base . isupper ( ) and prev is True : insertions . append ( [ ] ) prev = False elif base . islower ( ) : insertions [ - 1 ] . append ( i ) prev = True return [ [ min ( i ) , max ( i ) ] for i in insertions if i != [ ] ]'], ['get insertion information from header', \"def seq_info ( names , id2names , insertions , sequences ) : seqs = { } for name in names : id = id2names [ name ] gene = name . split ( 'fromHMM::' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ 1 ] model = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( '=' , 1 ) [ 1 ] . split ( ) [ 0 ] i_gene_pos = insertions [ id ] i_model_pos = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( 'model-pos(ins-len)=' ) [ 1 ] . split ( ) [ 0 ] . split ( ';' ) i_info = [ ] for i , ins in enumerate ( i_gene_pos ) : model_pos = i_model_pos [ i ] . split ( '-' ) [ 1 ] . split ( '(' ) [ 0 ] length = i_model_pos [ i ] . split ( '(' ) [ 1 ] . split ( ')' ) [ 0 ] iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s' % ( id , ( i + 1 ) , ( i + 1 ) , ins [ 0 ] , ins [ 1 ] , model_pos ) iseq = sequences [ id ] [ 1 ] [ ins [ 0 ] : ( ins [ 1 ] + 1 ) ] iseq = [ iheader , iseq ] info = [ ins , model_pos , length , iseq , [ ] , [ ] ] i_info . append ( info ) seqs [ id ] = [ gene , model , i_info ] return seqs\"], ['make sure thresh % feature is contained within insertion', 'def check_overlap ( pos , ins , thresh ) : ins_pos = ins [ 0 ] ins_len = ins [ 2 ] ol = overlap ( ins_pos , pos ) feat_len = pos [ 1 ] - pos [ 0 ] + 1 if float ( ol ) / float ( feat_len ) >= thresh : return True return False'], ['length of largest insertion', 'def max_insertion ( seqs , gene , domain ) : seqs = [ i [ 2 ] for i in list ( seqs . values ( ) ) if i [ 2 ] != [ ] and i [ 0 ] == gene and i [ 1 ] == domain ] lengths = [ ] for seq in seqs : for ins in seq : lengths . append ( int ( ins [ 2 ] ) ) if lengths == [ ] : return 100 return max ( lengths )'], ['get length of model', \"def model_length ( gene , domain ) : if gene == '16S' : domain2max = { 'E_coli_K12' : int ( 1538 ) , 'bacteria' : int ( 1689 ) , 'archaea' : int ( 1563 ) , 'eukarya' : int ( 2652 ) } return domain2max [ domain ] elif gene == '23S' : domain2max = { 'E_coli_K12' : int ( 2903 ) , 'bacteria' : int ( 3146 ) , 'archaea' : int ( 3774 ) , 'eukarya' : int ( 9079 ) } return domain2max [ domain ] else : print ( sys . stderr , '# length unknown for gene: %s, domain: %s' % ( gene , domain ) ) exit ( )\"], ['setup unique marker for every orf annotation - change size if necessary', \"def setup_markers ( seqs ) : family2marker = { } markers = cycle ( [ '^' , 'p' , '*' , '+' , 'x' , 'd' , '|' , 'v' , '>' , '<' , '8' ] ) size = 60 families = [ ] for seq in list ( seqs . values ( ) ) : for insertion in seq [ 2 ] : for family in list ( insertion [ - 1 ] . values ( ) ) : if family not in families : families . append ( family ) for family in families : marker = next ( markers ) if marker == '^' : size = size * 0.5 family2marker [ family ] = [ marker , size ] return family2marker\"], ['plot insertions for each gene and domain', 'def plot_by_gene_and_domain ( name , seqs , tax , id2name ) : for gene in set ( [ seq [ 0 ] for seq in list ( seqs . values ( ) ) ] ) : for domain in set ( [ seq [ 1 ] for seq in list ( seqs . values ( ) ) ] ) : plot_insertions ( name , seqs , gene , domain , tax , id2name )'], ['get the description for each ORF', \"def get_descriptions ( fastas ) : id2desc = { } for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ' ' ) id = header [ 0 ] if len ( header ) > 1 : desc = ' ' . join ( header [ 1 : ] ) else : desc = 'n/a' length = float ( len ( [ i for i in seq [ 1 ] . strip ( ) if i != '*' ] ) ) id2desc [ id ] = [ fasta , desc , length ] return id2desc\"], ['optimize later? slow ... should combine with calculate_threshold module', \"def print_genome_matrix ( hits , fastas , id2desc , file_name ) : out = open ( file_name , 'w' ) fastas = sorted ( fastas ) print ( '## percent identity between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : average = '-' else : average = numpy . average ( [ hits [ fasta ] [ other ] [ i ] [ 3 ] for i in hits [ fasta ] [ other ] ] ) line . append ( str ( average ) ) print ( '\\\\t' . join ( line ) , file = out ) print ( '' , file = out ) print ( '## percent of orfs that are orthologous between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : percent = '-' else : orthologs = float ( len ( hits [ fasta ] [ other ] ) ) orfs = float ( len ( [ i for i in id2desc if id2desc [ i ] [ 0 ] == fasta ] ) ) percent = float ( orthologs / orfs ) * 100 line . append ( str ( percent ) ) print ( '\\\\t' . join ( line ) , file = out )\"], ['compare genome to self to get the best possible bit score for each ORF', \"def self_compare ( fastas , id2desc , algorithm ) : for fasta in fastas : blast = open ( search ( fasta , fasta , method = algorithm , alignment = 'local' ) ) for hit in best_blast ( blast , 1 ) : id , bit = hit [ 0 ] . split ( ) [ 0 ] , float ( hit [ - 1 ] ) id2desc [ id ] . append ( bit ) return id2desc\"], ['if thresholds are not specififed calculate based on the distribution of normalized bit scores', \"def calc_thresholds ( rbh , file_name , thresholds = [ False , False , False , False ] , stdevs = 2 ) : calc_threshold = thresholds [ - 1 ] norm_threshold = { } for pair in itertools . permutations ( [ i for i in rbh ] , 2 ) : if pair [ 0 ] not in norm_threshold : norm_threshold [ pair [ 0 ] ] = { } norm_threshold [ pair [ 0 ] ] [ pair [ 1 ] ] = { } out = open ( file_name , 'w' ) print ( '#### summary of rbh comparisons\\\\n' , file = out ) comparisons = [ ] for genome in rbh : for compare in rbh [ genome ] : pair = '' . join ( sorted ( [ genome , compare ] ) ) if pair in comparisons : continue comparisons . append ( pair ) scores = { 'percent identity' : [ ] , 'e-value' : [ ] , 'bit score' : [ ] , 'normalized bit score' : [ ] , 'alignment length fraction' : [ ] } print ( '### blast between %s and %s\\\\n' % ( genome , compare ) , file = out ) for id in rbh [ genome ] [ compare ] : pident , length_fraction , e , bit , norm_bit = rbh [ genome ] [ compare ] [ id ] [ 3 : ] scores [ 'percent identity' ] . append ( pident ) scores [ 'alignment length fraction' ] . append ( length_fraction ) scores [ 'e-value' ] . append ( e ) scores [ 'bit score' ] . append ( bit ) scores [ 'normalized bit score' ] . append ( norm_bit ) if calc_threshold is True : norms = scores [ 'normalized bit score' ] average = numpy . average ( norms ) std = numpy . std ( norms ) normal_thresh = average - ( std * stdevs ) print ( '## average normalized bit score: %s' % average , file = out ) print ( '## standard deviation of normalized bit scores: %s' % std , file = out ) print ( '## normalized bit score threshold set to: %s\\\\n' % ( normal_thresh ) , file = out ) norm_threshold [ genome ] [ compare ] , norm_threshold [ compare ] [ genome ] = normal_thresh , normal_thresh for score in scores : print ( '## %s' % ( score ) , file = out ) if len ( scores [ score ] ) > 0 : print ( '## average: %s' % numpy . average ( scores [ score ] ) , file = out ) print ( '' , file = out ) out . close ( ) if calc_threshold is True : return thresholds [ 0 : - 1 ] + [ norm_threshold ] else : return thresholds\"], ['make and split a rbh network', \"def neto ( fastas , algorithm = 'usearch' , e = 0.01 , bit = 40 , length = .65 , norm_bit = False ) : thresholds = [ e , bit , length , norm_bit ] id2desc = get_descriptions ( fastas ) id2desc = self_compare ( fastas , id2desc , algorithm ) hits = compare_genomes ( fastas , id2desc , algorithm ) calc_thresholds ( hits , file_name = 'fbh.scores.summary.txt' ) rbh_network ( id2desc , hits , file_name = 'fbh.network.edges.txt' ) hits , rbh = find_rbh ( hits , id2desc ) thresholds = calc_thresholds ( rbh , 'rbh.scores.summary.txt' , thresholds ) g = rbh_network ( id2desc , rbh , file_name = 'rbh.network.edges.txt' ) filtered_g , filtered_rbh = rbh_network ( id2desc , rbh , 'rbh.filtered.network.edges.txt' , thresholds ) calc_thresholds ( filtered_rbh , file_name = 'rbh.filtered.scores.summary.txt' ) print_summary ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.nodes.txt' ) print_network_matrix ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.matrix.txt' ) print_genome_matrix ( filtered_rbh , fastas , id2desc , file_name = 'rbh.filtered.network.genome_matrix.txt' ) split_g = split_network ( filtered_g , id2desc , file_name = 'rbh.filtered.split.network.edges.txt' ) print_summary ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.nodes.txt' ) print_network_matrix ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.matrix.txt' ) return split_g\"], ['Collapses multiple dimensions into a single raster_info complex struct', \"def _parse_raster_info ( self , prop = RASTER_INFO ) : raster_info = { } . fromkeys ( _iso_definitions [ prop ] , u'' ) raster_info [ 'dimensions' ] = get_default_for_complex_sub ( prop = prop , subprop = 'dimensions' , value = parse_property ( self . _xml_tree , None , self . _data_map , '_ri_num_dims' ) , xpath = self . _data_map [ '_ri_num_dims' ] ) xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] for dimension in parse_complex_list ( self . _xml_tree , xpath_root , xpath_map , RASTER_DIMS ) : dimension_type = dimension [ 'type' ] . lower ( ) if dimension_type == 'vertical' : raster_info [ 'vertical_count' ] = dimension [ 'size' ] elif dimension_type == 'column' : raster_info [ 'column_count' ] = dimension [ 'size' ] raster_info [ 'x_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) elif dimension_type == 'row' : raster_info [ 'row_count' ] = dimension [ 'size' ] raster_info [ 'y_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) return raster_info if any ( raster_info [ k ] for k in raster_info ) else { }\"], ['Derives multiple dimensions from a single raster_info complex struct', \"def _update_raster_info ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = update_props . pop ( 'values' ) xroot , xpath = None , self . _data_map [ '_ri_num_dims' ] raster_info = [ update_property ( tree_to_update , xroot , xpath , prop , values . get ( 'dimensions' , u'' ) ) ] xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] v_dimension = { } if values . get ( 'vertical_count' ) : v_dimension = v_dimension . fromkeys ( xpath_map , u'' ) v_dimension [ 'type' ] = 'vertical' v_dimension [ 'size' ] = values . get ( 'vertical_count' , u'' ) x_dimension = { } if values . get ( 'column_count' ) or values . get ( 'x_resolution' ) : x_dimension = x_dimension . fromkeys ( xpath_map , u'' ) x_dimension [ 'type' ] = 'column' x_dimension [ 'size' ] = values . get ( 'column_count' , u'' ) x_dimension [ 'value' ] = values . get ( 'x_resolution' , u'' ) y_dimension = { } if values . get ( 'row_count' ) or values . get ( 'y_resolution' ) : y_dimension = y_dimension . fromkeys ( xpath_map , u'' ) y_dimension [ 'type' ] = 'row' y_dimension [ 'size' ] = values . get ( 'row_count' , u'' ) y_dimension [ 'value' ] = values . get ( 'y_resolution' , u'' ) update_props [ 'prop' ] = RASTER_DIMS update_props [ 'values' ] = [ v_dimension , x_dimension , y_dimension ] raster_info += update_complex_list ( xpath_root = xpath_root , xpath_map = xpath_map , ** update_props ) return raster_info\"], ['Removes primitive type tags from an XPATH', 'def _trim_xpath ( self , xpath , prop ) : xroot = self . _get_xroot_for ( prop ) if xroot is None and isinstance ( xpath , string_types ) : xtags = xpath . split ( XPATH_DELIM ) if xtags [ - 1 ] in _iso_tag_primitives : xroot = XPATH_DELIM . join ( xtags [ : - 1 ] ) return xroot'], ['Generates the app id for a given shortcut . Steam uses app ids as a unique identifier for games but since shortcuts dont have a canonical serverside representation they need to be generated on the fly . The important part about this function is that it will generate the same app id as Steam does for a given shortcut', \"def shortcut_app_id ( shortcut ) : algorithm = Crc ( width = 32 , poly = 0x04C11DB7 , reflect_in = True , xor_in = 0xffffffff , reflect_out = True , xor_out = 0xffffffff ) crc_input = '' . join ( [ shortcut . exe , shortcut . name ] ) high_32 = algorithm . bit_by_bit ( crc_input ) | 0x80000000 full_64 = ( high_32 << 32 ) | 0x02000000 return str ( full_64 )\"], ['Execute git config .', \"def _config ( self ) : cfg_wr = self . repo . config_writer ( ) cfg_wr . add_section ( 'user' ) cfg_wr . set_value ( 'user' , 'name' , self . metadata . author ) cfg_wr . set_value ( 'user' , 'email' , self . metadata . email ) cfg_wr . release ( )\"], ['Execute git remote add .', \"def _remote_add ( self ) : self . repo . create_remote ( 'origin' , 'git@github.com:{username}/{repo}.git' . format ( username = self . metadata . username , repo = self . metadata . name ) )\"], ['Starts execution of the script', 'def start ( self ) : try : self . args . func ( ) except SystemExit as e : if e . code != 0 : raise except KeyboardInterrupt : self . log . warning ( \"exited via keyboard interrupt\" ) except : self . log . exception ( \"exited start function\" ) finally : self . _flush_metrics_q . put ( None , block = True ) self . _flush_metrics_q . put ( None , block = True , timeout = 1 ) self . log . debug ( \"exited_successfully\" )'], ['Define basic command - line arguments required by the script .', 'def define_baseargs ( self , parser ) : parser . add_argument ( \\'--name\\' , default = sys . argv [ 0 ] , help = \\'Name to identify this instance\\' ) parser . add_argument ( \\'--log-level\\' , default = None , help = \\'Logging level as picked from the logging module\\' ) parser . add_argument ( \\'--log-format\\' , default = None , choices = ( \"json\" , \"pretty\" , ) , help = ( \"Force the format of the logs. By default, if the \" \"command is from a terminal, print colorful logs. \" \"Otherwise print json.\" ) , ) parser . add_argument ( \\'--log-file\\' , default = None , help = \\'Writes logs to log file if specified, default: %(default)s\\' , ) parser . add_argument ( \\'--quiet\\' , default = False , action = \"store_true\" , help = \\'if true, does not print logs to stderr, default: %(default)s\\' , ) parser . add_argument ( \\'--metric-grouping-interval\\' , default = None , type = int , help = \\'To group metrics based on time interval ex:10 i.e;(10 sec)\\' , ) parser . add_argument ( \\'--debug\\' , default = False , action = \"store_true\" , help = \\'To run the code in debug mode\\' , )'], ['Basically turns payload that looks like \\\\\\\\ n to . In the calling function if this function returns no object is added for that payload .', \"def cleanup_payload ( self , payload ) : p = payload . replace ( '\\\\n' , '' ) p = p . rstrip ( ) p = p . lstrip ( ) return p\"], ['Ensures complex property types have the correct default values', \"def get_default_for ( prop , value ) : prop = prop . strip ( '_' ) val = reduce_value ( value ) if prop in _COMPLEX_LISTS : return wrap_value ( val ) elif prop in _COMPLEX_STRUCTS : return val or { } else : return u'' if val is None else val\"], ['Either update the tree the default way or call the custom updater', \"def update_property ( tree_to_update , xpath_root , xpaths , prop , values , supported = None ) : if supported and prop . startswith ( '_' ) and prop . strip ( '_' ) in supported : values = u'' else : values = get_default_for ( prop , values ) if not xpaths : return [ ] elif not isinstance ( xpaths , ParserProperty ) : return _update_property ( tree_to_update , xpath_root , xpaths , values ) else : return xpaths . set_prop ( tree_to_update = tree_to_update , prop = prop , values = values )\"], ['Default update operation for a single parser property . If xpaths contains one xpath then one element per value will be inserted at that location in the tree_to_update ; otherwise the number of values must match the number of xpaths .', \"def _update_property ( tree_to_update , xpath_root , xpaths , values ) : def update_element ( elem , idx , root , path , vals ) : has_root = bool ( root and len ( path ) > len ( root ) and path . startswith ( root ) ) path , attr = get_xpath_tuple ( path ) if attr : removed = [ get_element ( elem , path ) ] remove_element_attributes ( removed [ 0 ] , attr ) elif not has_root : removed = wrap_value ( remove_element ( elem , path ) ) else : path = get_xpath_branch ( root , path ) removed = [ ] if idx != 0 else [ remove_element ( e , path , True ) for e in get_elements ( elem , root ) ] if not vals : return removed items = [ ] for i , val in enumerate ( wrap_value ( vals ) ) : elem_to_update = elem if has_root : elem_to_update = insert_element ( elem , ( i + idx ) , root ) val = val . decode ( 'utf-8' ) if not isinstance ( val , string_types ) else val if not attr : items . append ( insert_element ( elem_to_update , i , path , val ) ) elif path : items . append ( insert_element ( elem_to_update , i , path , ** { attr : val } ) ) else : set_element_attributes ( elem_to_update , ** { attr : val } ) items . append ( elem_to_update ) return items xpaths = reduce_value ( xpaths ) values = filter_empty ( values ) if isinstance ( xpaths , string_types ) : return update_element ( tree_to_update , 0 , xpath_root , xpaths , values ) else : each = [ ] for index , xpath in enumerate ( xpaths ) : value = values [ index ] if values else None each . extend ( update_element ( tree_to_update , index , xpath_root , xpath , value ) ) return each\"], ['Default validation for single complex data structure', \"def validate_complex ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for complex_prop , complex_val in iteritems ( value ) : complex_key = '.' . join ( ( prop , complex_prop ) ) if complex_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) validate_type ( complex_key , complex_val , ( string_types , list ) )\"], ['Default validation for Attribute Details data structure', \"def validate_complex_list ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for idx , complex_struct in enumerate ( wrap_value ( value ) ) : cs_idx = prop + '[' + str ( idx ) + ']' validate_type ( cs_idx , complex_struct , dict ) for cs_prop , cs_val in iteritems ( complex_struct ) : cs_key = '.' . join ( ( cs_idx , cs_prop ) ) if cs_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) if not isinstance ( cs_val , list ) : validate_type ( cs_key , cs_val , ( string_types , list ) ) else : for list_idx , list_val in enumerate ( cs_val ) : list_prop = cs_key + '[' + str ( list_idx ) + ']' validate_type ( list_prop , list_val , string_types )\"], ['Default validation for Date Types data structure', \"def validate_dates ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) date_keys = set ( value ) if date_keys : if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys : if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = _complex_definitions [ DATES ] if xpath_map is None else xpath_map _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) date_type = value [ DATE_TYPE ] if date_type not in DATE_TYPES : _validation_error ( 'dates.type' , None , date_type , DATE_TYPES ) date_vals = value [ DATE_VALUES ] validate_type ( 'dates.values' , date_vals , list ) dates_len = len ( date_vals ) if date_type == DATE_TYPE_MISSING and dates_len != 0 : _validation_error ( 'len(dates.values)' , None , dates_len , 0 ) if date_type == DATE_TYPE_SINGLE and dates_len != 1 : _validation_error ( 'len(dates.values)' , None , dates_len , 1 ) if date_type == DATE_TYPE_RANGE and dates_len != 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 2 ) if date_type == DATE_TYPE_MULTIPLE and dates_len < 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 'at least two' ) for idx , date in enumerate ( date_vals ) : date_key = 'dates.value[' + str ( idx ) + ']' validate_type ( date_key , date , string_types )\"], ['Default validation for Process Steps data structure', \"def validate_process_steps ( prop , value ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) procstep_keys = set ( _complex_definitions [ prop ] ) for idx , procstep in enumerate ( wrap_value ( value ) ) : ps_idx = prop + '[' + str ( idx ) + ']' validate_type ( ps_idx , procstep , dict ) for ps_prop , ps_val in iteritems ( procstep ) : ps_key = '.' . join ( ( ps_idx , ps_prop ) ) if ps_prop not in procstep_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( procstep_keys ) ) ) ) if ps_prop != 'sources' : validate_type ( ps_key , ps_val , string_types ) else : validate_type ( ps_key , ps_val , ( string_types , list ) ) for src_idx , src_val in enumerate ( wrap_value ( ps_val ) ) : src_key = ps_key + '[' + str ( src_idx ) + ']' validate_type ( src_key , src_val , string_types )\"], ['Default validation for all types', 'def validate_type ( prop , value , expected ) : if value is not None and not isinstance ( value , expected ) : _validation_error ( prop , type ( value ) . __name__ , None , expected )'], ['Default validation for updated properties', \"def _validation_error ( prop , prop_type , prop_value , expected ) : if prop_type is None : attrib = 'value' assigned = prop_value else : attrib = 'type' assigned = prop_type raise ValidationError ( 'Invalid property {attrib} for {prop}:\\\\n\\\\t{attrib}: {assigned}\\\\n\\\\texpected: {expected}' , attrib = attrib , prop = prop , assigned = assigned , expected = expected , invalid = { prop : prop_value } if attrib == 'value' else { } )\"], ['Calls the getter with no arguments and returns its value', 'def get_prop ( self , prop ) : if self . _parser is None : raise ConfigurationError ( \\'Cannot call ParserProperty.\"get_prop\" with no parser configured\\' ) return self . _parser ( prop ) if prop else self . _parser ( )'], ['Returns a boolean representing whether these commands can be grouped together or not .', \"def can_group_commands ( command , next_command ) : multi_capable_commands = ( 'get' , 'set' , 'delete' ) if next_command is None : return False name = command . get_name ( ) if name not in multi_capable_commands : return False if name != next_command . get_name ( ) : return False if grouped_args_for_command ( command ) != grouped_args_for_command ( next_command ) : return False if command . get_kwargs ( ) != next_command . get_kwargs ( ) : return False return True\"], ['define ribosomal proteins and location of curated databases', \"def find_databases ( databases ) : proteins = [ 'L15' , 'L18' , 'L6' , 'S8' , 'L5' , 'L24' , 'L14' , 'S17' , 'L16' , 'S3' , 'L22' , 'S19' , 'L2' , 'L4' , 'L3' , 'S10' ] protein_databases = { 'L14' : 'rpL14_JGI_MDM.filtered.faa' , 'L15' : 'rpL15_JGI_MDM.filtered.faa' , 'L16' : 'rpL16_JGI_MDM.filtered.faa' , 'L18' : 'rpL18_JGI_MDM.filtered.faa' , 'L22' : 'rpL22_JGI_MDM.filtered.faa' , 'L24' : 'rpL24_JGI_MDM.filtered.faa' , 'L2' : 'rpL2_JGI_MDM.filtered.faa' , 'L3' : 'rpL3_JGI_MDM.filtered.faa' , 'L4' : 'rpL4_JGI_MDM.filtered.faa' , 'L5' : 'rpL5_JGI_MDM.filtered.faa' , 'L6' : 'rpL6_JGI_MDM.filtered.faa' , 'S10' : 'rpS10_JGI_MDM.filtered.faa' , 'S17' : 'rpS17_JGI_MDM.filtered.faa' , 'S19' : 'rpS19_JGI_MDM.filtered.faa' , 'S3' : 'rpS3_JGI_MDM.filtered.faa' , 'S8' : 'rpS8_JGI_MDM.filtered.faa' } protein_databases = { key : '%s/%s' % ( databases , database ) for key , database in list ( protein_databases . items ( ) ) } return proteins , protein_databases\"], ['which protein has the best hit the one to the right or to the left?', 'def find_next ( start , stop , i2hits ) : if start not in i2hits and stop in i2hits : index = stop elif stop not in i2hits and start in i2hits : index = start elif start not in i2hits and stop not in i2hits : index = choice ( [ start , stop ] ) i2hits [ index ] = [ [ False ] ] else : A , B = i2hits [ start ] [ 0 ] , i2hits [ stop ] [ 0 ] if B [ 10 ] <= A [ 10 ] : index = stop else : index = start if index == start : nstart = start - 1 nstop = stop else : nstop = stop + 1 nstart = start match = i2hits [ index ] [ 0 ] rp = match [ - 1 ] return index , nstart , nstop , rp , match'], ['determine which hits represent real ribosomal proteins identify each in syntenic block max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold', 'def find_ribosomal ( rps , scaffolds , s2rp , min_hits , max_hits_rp , max_errors ) : for scaffold , proteins in list ( s2rp . items ( ) ) : hits = { p : [ i for i in sorted ( hits , key = itemgetter ( 10 ) ) ] [ 0 : max_hits_rp ] for p , hits in list ( proteins . items ( ) ) if len ( hits ) > 0 } if len ( hits ) < min_hits : continue best = sorted ( [ hit [ 0 ] + [ p ] for p , hit in list ( hits . items ( ) ) ] , key = itemgetter ( 10 ) ) [ 0 ] block = find_block ( rps , scaffolds [ scaffold ] , hits , best , max_errors ) if ( len ( block ) - 1 ) >= min_hits : yield scaffold , block'], ['Parse the rep set file and remove all sequences not associated with unique OTUs .', 'def filter_rep_set ( inF , otuSet ) : seqs = [ ] for record in SeqIO . parse ( inF , \"fasta\" ) : if record . id in otuSet : seqs . append ( record ) return seqs'], ['Update the text for each element at the configured path if attribute matches', \"def _update_report_item ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = wrap_value ( update_props [ 'values' ] ) xroot = self . _get_xroot_for ( prop ) attr_key = 'type' attr_val = u'' if prop == 'attribute_accuracy' : attr_val = 'DQQuanAttAcc' elif prop == 'dataset_completeness' : attr_val = 'DQCompOm' for elem in get_elements ( tree_to_update , xroot ) : if get_element_attributes ( elem ) . get ( attr_key ) == attr_val : clear_element ( elem ) remove_empty_element ( tree_to_update , xroot ) attrs = { attr_key : attr_val } updated = [ ] for idx , value in enumerate ( values ) : elem = insert_element ( tree_to_update , idx , xroot , ** attrs ) updated . append ( insert_element ( elem , idx , 'measDesc' , value ) ) return updated\"], ['Clear the specified interrupt bit in the interrupt status register .', 'def _clear_interrupt ( self , intbit ) : int_status = self . _device . readU8 ( VCNL4010_INTSTAT ) int_status &= ~ intbit self . _device . write8 ( VCNL4010_INTSTAT , int_status )'], ['Swaps two nodes', 'def move ( self ) : a = random . randint ( 0 , len ( self . state ) - 1 ) b = random . randint ( 0 , len ( self . state ) - 1 ) self . state [ [ a , b ] ] = self . state [ [ b , a ] ]'], ['A bool - if the certificate should be self - signed .', 'def self_signed ( self , value ) : self . _self_signed = bool ( value ) if self . _self_signed : self . _issuer = None'], ['Grabs the first URL out of a asn1crypto . x509 . CRLDistributionPoints object', \"def _get_crl_url ( self , distribution_points ) : if distribution_points is None : return None for distribution_point in distribution_points : name = distribution_point [ 'distribution_point' ] if name . name == 'full_name' and name . chosen [ 0 ] . name == 'uniform_resource_identifier' : return name . chosen [ 0 ] . chosen . native return None\"], ['A bool - if the certificate should have the OCSP no check extension . Only applicable to certificates created for signing OCSP responses . Such certificates should normally be issued for a very short period of time since they are effectively whitelisted by clients .', 'def ocsp_no_check ( self , value ) : if value is None : self . _ocsp_no_check = None else : self . _ocsp_no_check = bool ( value )'], ['Removes empty line .', \"def emptylineless ( parser , token ) : nodelist = parser . parse ( ( 'endemptylineless' , ) ) parser . delete_first_token ( ) return EmptylinelessNode ( nodelist )\"], ['Do an HTTP PURGE of the given asset . The URL is run through urlparse and must point to the varnish instance not the varnishadm', \"def http_purge_url ( url ) : url = urlparse ( url ) connection = HTTPConnection ( url . hostname , url . port or 80 ) path = url . path or '/' connection . request ( 'PURGE' , '%s?%s' % ( path , url . query ) if url . query else path , '' , { 'Host' : '%s:%s' % ( url . hostname , url . port ) if url . port else url . hostname } ) response = connection . getresponse ( ) if response . status != 200 : logging . error ( 'Purge failed with status: %s' % response . status ) return response\"], ['Non - threaded batch command runner returning output results', \"def run ( addr , * commands , ** kwargs ) : results = [ ] handler = VarnishHandler ( addr , ** kwargs ) for cmd in commands : if isinstance ( cmd , tuple ) and len ( cmd ) > 1 : results . extend ( [ getattr ( handler , c [ 0 ] . replace ( '.' , '_' ) ) ( * c [ 1 : ] ) for c in cmd ] ) else : results . append ( getattr ( handler , cmd . replace ( '.' , '_' ) ) ( * commands [ 1 : ] ) ) break handler . close ( ) return results\"], ['add stylesheet files in HTML head', 'def add_stylesheets ( self , * css_files ) : for css_file in css_files : self . main_soup . style . append ( self . _text_file ( css_file ) )'], ['add javascripts files in HTML body', \"def add_javascripts ( self , * js_files ) : if self . main_soup . script is None : script_tag = self . main_soup . new_tag ( 'script' ) self . main_soup . body . append ( script_tag ) for js_file in js_files : self . main_soup . script . append ( self . _text_file ( js_file ) )\"], ['return the object in a file', \"def export ( self ) : with open ( self . export_url , 'w' , encoding = 'utf-8' ) as file : file . write ( self . build ( ) ) if self . open_browser : webbrowser . open_new_tab ( self . export_url )\"], ['convert Markdown text as html . return the html file as string', \"def build ( self ) : markdown_html = markdown . markdown ( self . markdown_text , extensions = [ TocExtension ( ) , 'fenced_code' , 'markdown_checklist.extension' , 'markdown.extensions.tables' ] ) markdown_soup = BeautifulSoup ( markdown_html , 'html.parser' ) if markdown_soup . find ( 'code' , attrs = { 'class' : 'mermaid' } ) : self . _add_mermaid_js ( ) for dot_tag in markdown_soup . find_all ( 'code' , attrs = { 'class' : 'dotgraph' } ) : grap_svg = self . _text_to_graphiz ( dot_tag . string ) graph_soup = BeautifulSoup ( grap_svg , 'html.parser' ) dot_tag . parent . replaceWith ( graph_soup ) self . main_soup . body . append ( markdown_soup ) return self . main_soup . prettify ( )\"], ['return the content of a file', \"def _text_file ( self , url ) : try : with open ( url , 'r' , encoding = 'utf-8' ) as file : return file . read ( ) except FileNotFoundError : print ( 'File `{}` not found' . format ( url ) ) sys . exit ( 0 )\"], ['create a graphviz graph from text', \"def _text_to_graphiz ( self , text ) : dot = Source ( text , format = 'svg' ) return dot . pipe ( ) . decode ( 'utf-8' )\"], ['add js libraries and css files of mermaid js_file', \"def _add_mermaid_js ( self ) : self . add_javascripts ( '{}/js/jquery-1.11.3.min.js' . format ( self . resources_path ) ) self . add_javascripts ( '{}/js/mermaid.min.js' . format ( self . resources_path ) ) self . add_stylesheets ( '{}/css/mermaid.css' . format ( self . resources_path ) ) self . main_soup . script . append ( 'mermaid.initialize({startOnLoad:true  });' )\"], ['Get a character set with individual members or ranges .', 'def getCharacterSet ( self ) : chars = u\\'\\' c = None cnt = 1 start = 0 while True : escaped_slash = False c = self . next ( ) if self . lookahead ( ) == u\\'-\\' and not c == u\\'\\\\\\\\\\' : f = c self . next ( ) c = self . next ( ) if not c or ( c in self . meta_chars ) : raise StringGenerator . SyntaxError ( u\"unexpected end of class range\" ) chars += self . getCharacterRange ( f , c ) elif c == u\\'\\\\\\\\\\' : if self . lookahead ( ) in self . meta_chars : c = self . next ( ) chars += c continue elif self . lookahead ( ) in self . string_code : c = self . next ( ) chars += self . string_code [ c ] elif c and c not in self . meta_chars : chars += c if c == u\\']\\' : if self . lookahead ( ) == u\\'{\\' : [ start , cnt ] = self . getQuantifier ( ) else : start = - 1 cnt = 1 break if c and c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped character in class definition: %s\" % c ) if not c : break return StringGenerator . CharacterSet ( chars , start , cnt )'], ['Get a sequence of non - special characters .', 'def getLiteral ( self ) : chars = u\\'\\' c = self . current ( ) while True : if c and c == u\"\\\\\\\\\" : c = self . next ( ) if c : chars += c continue elif not c or ( c in self . meta_chars ) : break else : chars += c if self . lookahead ( ) and self . lookahead ( ) in self . meta_chars : break c = self . next ( ) return StringGenerator . Literal ( chars )'], ['Get a sequence of nodes .', 'def getSequence ( self , level = 0 ) : seq = [ ] op = \\'\\' left_operand = None right_operand = None sequence_closed = False while True : c = self . next ( ) if not c : break if c and c not in self . meta_chars : seq . append ( self . getLiteral ( ) ) elif c and c == u\\'$\\' and self . lookahead ( ) == u\\'{\\' : seq . append ( self . getSource ( ) ) elif c == u\\'[\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getCharacterSet ( ) ) elif c == u\\'(\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getSequence ( level + 1 ) ) elif c == u\\')\\' and not self . last ( ) == u\\'\\\\\\\\\\' : if level == 0 : raise StringGenerator . SyntaxError ( u\"Extra closing parenthesis\" ) sequence_closed = True break elif c == u\\'|\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c elif c == u\\'&\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c else : if c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped special character: %s\" % c ) if op and not left_operand : if not seq or len ( seq ) < 1 : raise StringGenerator . SyntaxError ( u\"Operator: %s with no left operand\" % op ) left_operand = seq . pop ( ) elif op and len ( seq ) >= 1 and left_operand : right_operand = seq . pop ( ) if op == u\\'|\\' : seq . append ( StringGenerator . SequenceOR ( [ left_operand , right_operand ] ) ) elif op == u\\'&\\' : seq . append ( StringGenerator . SequenceAND ( [ left_operand , right_operand ] ) ) op = u\\'\\' left_operand = None right_operand = None if op : raise StringGenerator . SyntaxError ( u\"Operator: %s with no right operand\" % op ) if level > 0 and not sequence_closed : raise StringGenerator . SyntaxError ( u\"Missing closing parenthesis\" ) return StringGenerator . Sequence ( seq )'], ['Print the parse tree and then call render for an example .', 'def dump ( self , ** kwargs ) : import sys if not self . seq : self . seq = self . getSequence ( ) print ( \"StringGenerator version: %s\" % ( __version__ ) ) print ( \"Python version: %s\" % sys . version ) self . seq . dump ( ) return self . render ( ** kwargs )'], ['Return a list of generated strings .', 'def render_list ( self , cnt , unique = False , progress_callback = None , ** kwargs ) : rendered_list = [ ] i = 0 total_attempts = 0 while True : if i >= cnt : break if total_attempts > cnt * self . unique_attempts_factor : raise StringGenerator . UniquenessError ( u\"couldn\\'t satisfy uniqueness\" ) s = self . render ( ** kwargs ) if unique : if not s in rendered_list : rendered_list . append ( s ) i += 1 else : rendered_list . append ( s ) i += 1 total_attempts += 1 if progress_callback and callable ( progress_callback ) : progress_callback ( i , cnt ) return rendered_list'], ['Establish the connection . This is done automatically for you .', 'def connect ( self ) : self . conn = boto . connect_s3 ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL ) self . bucket = self . conn . get_bucket ( self . AWS_STORAGE_BUCKET_NAME ) self . k = Key ( self . bucket )'], ['Connect to Cloud Front . This is done automatically for you when needed .', 'def connect_cloudfront ( self ) : \"Connect to Cloud Front. This is done automatically for you when needed.\" self . conn_cloudfront = connect_cloudfront ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL )'], ['Create a folder on S3 .', 'def mkdir ( self , target_folder ) : self . printv ( \"Making directory: %s\" % target_folder ) self . k . key = re . sub ( r\"^/|/$\" , \"\" , target_folder ) + \"/\" self . k . set_contents_from_string ( \\'\\' ) self . k . close ( )'], ['Delete the path and anything under the path .', 'def rm ( self , path ) : list_of_files = list ( self . ls ( path ) ) if list_of_files : if len ( list_of_files ) == 1 : self . bucket . delete_key ( list_of_files [ 0 ] ) else : self . bucket . delete_keys ( list_of_files ) self . printv ( \"Deleted: %s\" % list_of_files ) else : logger . error ( \"There was nothing to remove under %s\" , path )'], ['Copy a file to s3 .', 'def __put_key ( self , local_file , target_file , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , source = \"filename\" ) : action_word = \"moving\" if del_after_upload else \"copying\" try : self . k . key = target_file if source == \"filename\" : self . k . set_contents_from_filename ( local_file , self . AWS_HEADERS ) elif source == \"fileobj\" : self . k . set_contents_from_file ( local_file , self . AWS_HEADERS ) elif source == \"string\" : self . k . set_contents_from_string ( local_file , self . AWS_HEADERS ) else : raise Exception ( \"%s is not implemented as a source.\" % source ) self . k . set_acl ( acl ) self . k . close ( ) self . printv ( \"%s %s to %s\" % ( action_word , local_file , target_file ) ) if del_after_upload and source == \"filename\" : try : os . remove ( local_file ) except : logger . error ( \"Unable to delete the file: \" , local_file , exc_info = True ) return True except : logger . error ( \"Error in writing to %s\" , target_file , exc_info = True ) return False'], ['Copy a file or folder from local to s3 .', 'def cp ( self , local_path , target_path , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , invalidate = False ) : result = None if overwrite : list_of_files = [ ] else : list_of_files = self . ls ( folder = target_path , begin_from_file = \"\" , num = - 1 , get_grants = False , all_grant_data = False ) if local_path . endswith ( \"/*\" ) : local_path = local_path [ : - 2 ] target_path = re . sub ( r\"^/|/$\" , \"\" , target_path ) else : local_base_name = os . path . basename ( local_path ) local_path = re . sub ( r\"/$\" , \"\" , local_path ) target_path = re . sub ( r\"^/\" , \"\" , target_path ) if not target_path . endswith ( local_base_name ) : target_path = os . path . join ( target_path , local_base_name ) if os . path . exists ( local_path ) : result = self . __find_files_and_copy ( local_path , target_path , acl , del_after_upload , overwrite , invalidate , list_of_files ) else : result = { \\'file_does_not_exist\\' : local_path } logger . error ( \"trying to upload to s3 but file doesn\\'t exist: %s\" % local_path ) return result'], ['Similar to Linux mv command .', \"def mv ( self , local_file , target_file , acl = 'public-read' , overwrite = True , invalidate = False ) : self . cp ( local_file , target_file , acl = acl , del_after_upload = True , overwrite = overwrite , invalidate = invalidate )\"], ['Deal with saving cropduster images to S3 . Cropduster is a Django library for resizing editorial images . S3utils was originally written to put cropduster images on S3 bucket .', 'def cp_cropduster_image ( self , the_image_path , del_after_upload = False , overwrite = False , invalidate = False ) : local_file = os . path . join ( settings . MEDIA_ROOT , the_image_path ) if os . path . exists ( local_file ) : the_image_crops_path = os . path . splitext ( the_image_path ) [ 0 ] the_image_crops_path_full_path = os . path . join ( settings . MEDIA_ROOT , the_image_crops_path ) self . cp ( local_path = local_file , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , ) self . cp ( local_path = the_image_crops_path_full_path + \"/*\" , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_crops_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , )'], ['sets permissions for a file on S3', \"def chmod ( self , target_file , acl = 'public-read' ) : self . k . key = target_file self . k . set_acl ( acl ) self . k . close ( )\"], ['Get the list of files and permissions from S3 .', 'def ll ( self , folder = \"\" , begin_from_file = \"\" , num = - 1 , all_grant_data = False ) : return self . ls ( folder = folder , begin_from_file = begin_from_file , num = num , get_grants = True , all_grant_data = all_grant_data )'], ['Get the path from a given url including the querystring .', 'def get_path ( url ) : url = urlsplit ( url ) path = url . path if url . query : path += \"?{}\" . format ( url . query ) return path'], ['Reads data from disk and generates CSV files .', \"def run ( self ) : if not os . path . exists ( self . output ) : try : os . mkdir ( self . output ) except : print 'failed to create output directory %s' % self . output if not os . path . isdir ( self . output ) : print 'invalid output directory %s' % self . output sys . exit ( 1 ) visitors = [ _CompaniesCSV ( self . output ) , _ActivitiesCSV ( self . output ) , _ActivitiesSeenCSV ( self . output ) , _QSACSV ( self . output ) , ] for path in glob . glob ( os . path . join ( self . input , '*.json' ) ) : with open ( path , 'r' ) as f : try : data = json . load ( f , encoding = 'utf-8' ) except ValueError : continue for visitor in visitors : visitor . visit ( data )\"], ['Process a list of simple string field definitions and assign their order based on prefix .', \"def process_fields ( self , fields ) : result = [ ] strip = '' . join ( self . PREFIX_MAP ) for field in fields : direction = self . PREFIX_MAP [ '' ] if field [ 0 ] in self . PREFIX_MAP : direction = self . PREFIX_MAP [ field [ 0 ] ] field = field . lstrip ( strip ) result . append ( ( field , direction ) ) return result\"], ['Firms search in rubric', \"def search_in_rubric ( self , ** kwargs ) : point = kwargs . pop ( 'point' , False ) if point : kwargs [ 'point' ] = '%s,%s' % point bound = kwargs . pop ( 'bound' , False ) if bound : kwargs [ 'bound[point1]' ] = bound [ 0 ] kwargs [ 'bound[point2]' ] = bound [ 1 ] filters = kwargs . pop ( 'filters' , False ) if filters : for k , v in filters . items ( ) : kwargs [ 'filters[%s]' % k ] = v return self . _search_in_rubric ( ** kwargs )\"], ['Refresh the list and the screen', 'def refresh ( self ) : self . _screen . force_update ( ) self . _screen . refresh ( ) self . _update ( 1 )'], ['Mark an action as started', 'def start ( self , activity , action ) : try : self . _start_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . refresh ( )'], ['Mark a task as completed', 'def stop ( self , activity , action ) : try : self . _remove_running_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . _mark_action_completed ( activity , action ) self . refresh ( )'], ['Move laggard tasks over', 'def finish ( self , status ) : retox_log . info ( \"Completing %s with status %s\" % ( self . name , status ) ) result = Screen . COLOUR_GREEN if not status else Screen . COLOUR_RED self . palette [ \\'title\\' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , result ) for item in list ( self . _task_view . options ) : self . _task_view . options . remove ( item ) self . _completed_view . options . append ( item ) self . refresh ( )'], ['Reset the frame between jobs', \"def reset ( self ) : self . palette [ 'title' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , Screen . COLOUR_BLUE ) self . _completed_view . options = [ ] self . _task_view . options = [ ] self . refresh ( )\"], ['Returns the available kwargs of the called class', 'def default_arguments ( cls ) : func = cls . __init__ args = func . __code__ . co_varnames defaults = func . __defaults__ index = - len ( defaults ) return { k : v for k , v in zip ( args [ index : ] , defaults ) }'], ['Recreate the class based in your args multiple uses', 'def recreate ( cls , * args , ** kwargs ) : cls . check_arguments ( kwargs ) first_is_callable = True if any ( args ) and callable ( args [ 0 ] ) else False signature = cls . default_arguments ( ) allowed_arguments = { k : v for k , v in kwargs . items ( ) if k in signature } if ( any ( allowed_arguments ) or any ( args ) ) and not first_is_callable : if any ( args ) and not first_is_callable : return cls ( args [ 0 ] , ** allowed_arguments ) elif any ( allowed_arguments ) : return cls ( ** allowed_arguments ) return cls . instances [ - 1 ] if any ( cls . instances ) else cls ( )'], ['Put warnings of arguments whose can t be handle by the class', 'def check_arguments ( cls , passed ) : defaults = list ( cls . default_arguments ( ) . keys ( ) ) template = ( \"Pass arg {argument:!r} in {cname:!r}, can be a typo? \" \"Supported key arguments: {defaults}\" ) fails = [ ] for arg in passed : if arg not in defaults : warn ( template . format ( argument = arg , cname = cls . __name__ , defaults = defaults ) ) fails . append ( arg ) return any ( fails )'], ['process the specified type then process its children', 'def process ( self , data , type , history ) : if type in history : return if type . enum ( ) : return history . append ( type ) resolved = type . resolve ( ) value = None if type . multi_occurrence ( ) : value = [ ] else : if len ( resolved ) > 0 : if resolved . mixed ( ) : value = Factory . property ( resolved . name ) md = value . __metadata__ md . sxtype = resolved else : value = Factory . object ( resolved . name ) md = value . __metadata__ md . sxtype = resolved md . ordering = self . ordering ( resolved ) setattr ( data , type . name , value ) if value is not None : data = value if not isinstance ( data , list ) : self . add_attributes ( data , resolved ) for child , ancestry in resolved . children ( ) : if self . skip_child ( child , ancestry ) : continue self . process ( data , child , history [ : ] )'], ['get whether or not to skip the specified child', 'def skip_child ( self , child , ancestry ) : if child . any ( ) : return True for x in ancestry : if x . choice ( ) : return True return False'], ['Checks whether knocks are enabled for the model given as argument', \"def active_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : return True return _thread_locals . knock_enabled . get ( obj . __class__ , True )\"], ['Context manager to suspend sending knocks for the given model', \"def pause_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : _thread_locals . knock_enabled = { } obj . __class__ . _disconnect ( ) _thread_locals . knock_enabled [ obj . __class__ ] = False yield _thread_locals . knock_enabled [ obj . __class__ ] = True obj . __class__ . _connect ( )\"], ['Loop over the report progress', 'def _loopreport ( self ) : while 1 : eventlet . sleep ( 0.2 ) ac2popenlist = { } for action in self . session . _actions : for popen in action . _popenlist : if popen . poll ( ) is None : lst = ac2popenlist . setdefault ( action . activity , [ ] ) lst . append ( popen ) if not action . _popenlist and action in self . _actionmayfinish : super ( RetoxReporter , self ) . logaction_finish ( action ) self . _actionmayfinish . remove ( action ) self . screen . draw_next_frame ( repeat = False )'], ['Send markdown email', \"def send ( email , subject = None , from_email = None , to_email = None , cc = None , bcc = None , reply_to = None , smtp = None ) : if is_string ( email ) : email = EmailContent ( email ) from_email = sanitize_email_address ( from_email or email . headers . get ( 'from' ) ) to_email = sanitize_email_address ( to_email or email . headers . get ( 'to' ) ) cc = sanitize_email_address ( cc or email . headers . get ( 'cc' ) ) bcc = sanitize_email_address ( bcc or email . headers . get ( 'bcc' ) ) reply_to = sanitize_email_address ( reply_to or email . headers . get ( 'reply-to' ) ) message_args = { 'html' : email . html , 'text' : email . text , 'subject' : ( subject or email . headers . get ( 'subject' , '' ) ) , 'mail_from' : from_email , 'mail_to' : to_email } if cc : message_args [ 'cc' ] = cc if bcc : message_args [ 'bcc' ] = bcc if reply_to : message_args [ 'headers' ] = { 'reply-to' : reply_to } message = emails . Message ( ** message_args ) for filename , data in email . inline_images : message . attach ( filename = filename , content_disposition = 'inline' , data = data ) message . send ( smtp = smtp )\"], ['Process timezone casting and conversion .', 'def _process_tz ( self , dt , naive , tz ) : def _tz ( t ) : if t in ( None , \\'naive\\' ) : return t if t == \\'local\\' : if __debug__ and not localtz : raise ValueError ( \"Requested conversion to local timezone, but `localtz` not installed.\" ) t = localtz if not isinstance ( t , tzinfo ) : if __debug__ and not localtz : raise ValueError ( \"The `pytz` package must be installed to look up timezone: \" + repr ( t ) ) t = get_tz ( t ) if not hasattr ( t , \\'normalize\\' ) and get_tz : t = get_tz ( t . tzname ( dt ) ) return t naive = _tz ( naive ) tz = _tz ( tz ) if not dt . tzinfo and naive : if hasattr ( naive , \\'localize\\' ) : dt = naive . localize ( dt ) else : dt = dt . replace ( tzinfo = naive ) if not tz : return dt if hasattr ( tz , \\'normalize\\' ) : dt = tz . normalize ( dt . astimezone ( tz ) ) elif tz == \\'naive\\' : dt = dt . replace ( tzinfo = None ) else : dt = dt . astimezone ( tz ) return dt'], ['Trigger assignment of default values .', 'def _prepare_defaults ( self ) : for name , field in self . __fields__ . items ( ) : if field . assign : getattr ( self , name )'], ['Convert data coming in from the MongoDB wire driver into a Document instance .', \"def from_mongo ( cls , doc ) : if doc is None : return None if isinstance ( doc , Document ) : return doc if cls . __type_store__ and cls . __type_store__ in doc : cls = load ( doc [ cls . __type_store__ ] , 'marrow.mongo.document' ) instance = cls ( _prepare_defaults = False ) instance . __data__ = doc instance . _prepare_defaults ( ) return instance\"], ['Retrieve and remove a value from the backing store optionally with a default .', 'def pop ( self , name , default = SENTINEL ) : if default is SENTINEL : return self . __data__ . pop ( name ) return self . __data__ . pop ( name , default )'], ['A basic operation operating on a single value .', 'def _op ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _op ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) if other is not None : other = f . transformer . foreign ( other , ( f , self . _document ) ) return Filter ( { self . _name : { operation : other } } )'], ['An iterative operation operating on multiple values . Consumes iterators to construct a concrete list at time of execution .', 'def _iop ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _iop ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) def _t ( o ) : for value in o : yield None if value is None else f . transformer . foreign ( value , ( f , self . _document ) ) other = other if len ( other ) > 1 else other [ 0 ] values = list ( _t ( other ) ) return Filter ( { self . _name : { operation : values } } )']]\n",
            "Trimmed to 299 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "des 1088\n",
            "cod 3221\n",
            "[['Return either the full or truncated version of a QIIME - formatted taxonomy string .', 'def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ]'], ['Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .', 'def ensure_dir ( d ) : if not os . path . exists ( d ) : try : os . makedirs ( d ) except OSError as oe : if os . errno == errno . ENOENT : msg = twdd ( ) return msg . format ( d ) else : msg = twdd ( ) return msg . format ( d , oe . strerror )'], ['Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .', 'def file_handle ( fnh , mode = \"rU\" ) : handle = None if isinstance ( fnh , file ) : if fnh . closed : raise ValueError ( \"Input file is closed.\" ) handle = fnh elif isinstance ( fnh , str ) : handle = open ( fnh , mode ) return handle'], ['Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .', 'def gather_categories ( imap , header , categories = None ) : if categories is None : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } cat_ids = [ header . index ( cat ) for cat in categories if cat in header and \"=\" not in cat ] table = OrderedDict ( ) conditions = defaultdict ( set ) for i , cat in enumerate ( categories ) : if \"=\" in cat and cat . split ( \"=\" ) [ 0 ] in header : cat_name = header [ header . index ( cat . split ( \"=\" ) [ 0 ] ) ] conditions [ cat_name ] . add ( cat . split ( \"=\" ) [ 1 ] ) if not cat_ids and not conditions : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } if cat_ids and not conditions : for sid , row in imap . items ( ) : cat_name = \"_\" . join ( [ row [ cid ] for cid in cat_ids ] ) if cat_name not in table : table [ cat_name ] = DataCategory ( set ( ) , { } ) table [ cat_name ] . sids . add ( sid ) return table cond_ids = set ( ) for k in conditions : try : cond_ids . add ( header . index ( k ) ) except ValueError : continue idx_to_test = set ( cat_ids ) . union ( cond_ids ) for sid , row in imap . items ( ) : if all ( [ row [ header . index ( c ) ] in conditions [ c ] for c in conditions ] ) : key = \"_\" . join ( [ row [ idx ] for idx in idx_to_test ] ) try : assert key in table . keys ( ) except AssertionError : table [ key ] = DataCategory ( set ( ) , { } ) table [ key ] . sids . add ( sid ) try : assert len ( table ) > 0 except AssertionError : return { \"default\" : DataCategory ( set ( imap . keys ( ) ) , { } ) } else : return table'], ['Parses the unifrac results file into a dictionary', 'def parse_unifrac ( unifracFN ) : with open ( unifracFN , \"rU\" ) as uF : first = uF . next ( ) . split ( \"\\\\t\" ) lines = [ line . strip ( ) for line in uF ] unifrac = { \"pcd\" : OrderedDict ( ) , \"eigvals\" : [ ] , \"varexp\" : [ ] } if first [ 0 ] == \"pc vector number\" : return parse_unifrac_v1_8 ( unifrac , lines ) elif first [ 0 ] == \"Eigvals\" : return parse_unifrac_v1_9 ( unifrac , lines ) else : raise ValueError ( \"File format not supported/recognized. Please check input \" \"unifrac file.\" )'], ['Function to parse data from older version of unifrac file obtained from Qiime version 1 . 8 and earlier .', 'def parse_unifrac_v1_8 ( unifrac , file_data ) : for line in file_data : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ - 2 ] . split ( \"\\\\t\" ) [ 1 : ] ] unifrac [ \"varexp\" ] = [ float ( entry ) for entry in file_data [ - 1 ] . split ( \"\\\\t\" ) [ 1 : ] ] return unifrac'], ['Function to parse data from newer version of unifrac file obtained from Qiime version 1 . 9 and later .', 'def parse_unifrac_v1_9 ( unifrac , file_data ) : unifrac [ \"eigvals\" ] = [ float ( entry ) for entry in file_data [ 0 ] . split ( \"\\\\t\" ) ] unifrac [ \"varexp\" ] = [ float ( entry ) * 100 for entry in file_data [ 3 ] . split ( \"\\\\t\" ) ] for line in file_data [ 8 : ] : if line == \"\" : break line = line . split ( \"\\\\t\" ) unifrac [ \"pcd\" ] [ line [ 0 ] ] = [ float ( e ) for e in line [ 1 : ] ] return unifrac'], ['Determine color - category mapping . If color_column was specified then map the category names to color values . Otherwise use the palettable colors to automatically generate a set of colors for the group values .', 'def color_mapping ( sample_map , header , group_column , color_column = None ) : group_colors = OrderedDict ( ) group_gather = gather_categories ( sample_map , header , [ group_column ] ) if color_column is not None : color_gather = gather_categories ( sample_map , header , [ color_column ] ) for group in group_gather : for color in color_gather : if group_gather [ group ] . sids . intersection ( color_gather [ color ] . sids ) : group_colors [ group ] = color else : bcolors = itertools . cycle ( Set3_12 . hex_colors ) for group in group_gather : group_colors [ group ] = bcolors . next ( ) return group_colors'], ['return reverse completment of read', \"def rev_c ( read ) : rc = [ ] rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } for base in read : rc . extend ( rc_nucs [ base . upper ( ) ] ) return rc [ : : - 1 ]\"], ['randomly shuffle genome', \"def shuffle_genome ( genome , cat , fraction = float ( 100 ) , plot = True , alpha = 0.1 , beta = 100000 , min_length = 1000 , max_length = 200000 ) : header = '>randomized_%s' % ( genome . name ) sequence = list ( '' . join ( [ i [ 1 ] for i in parse_fasta ( genome ) ] ) ) length = len ( sequence ) shuffled = [ ] while sequence is not False : s = int ( random . gammavariate ( alpha , beta ) ) if s <= min_length or s >= max_length : continue if len ( sequence ) < s : seq = sequence [ 0 : ] else : seq = sequence [ 0 : s ] sequence = sequence [ s : ] shuffled . append ( '' . join ( seq ) ) if sequence == [ ] : break random . shuffle ( shuffled ) if fraction == float ( 100 ) : subset = shuffled else : max_pieces = int ( length * fraction / 100 ) subset , total = [ ] , 0 for fragment in shuffled : length = len ( fragment ) if total + length <= max_pieces : subset . append ( fragment ) total += length else : diff = max_pieces - total subset . append ( fragment [ 0 : diff ] ) break if cat is True : yield [ header , '' . join ( subset ) ] else : for i , seq in enumerate ( subset ) : yield [ '%s fragment:%s' % ( header , i ) , seq ]\"], ['If the fit contains statistically insignificant parameters remove them . Returns a pruned fit where all parameters have p - values of the t - statistic below p_max', \"def _prune ( self , fit , p_max ) : def remove_from_model_desc ( x , model_desc ) : rhs_termlist = [ ] for t in model_desc . rhs_termlist : if not t . factors : rhs_termlist . append ( t ) elif not x == t . factors [ 0 ] . _varname : rhs_termlist . append ( t ) md = ModelDesc ( model_desc . lhs_termlist , rhs_termlist ) return md corrected_model_desc = ModelDesc ( fit . model . formula . lhs_termlist [ : ] , fit . model . formula . rhs_termlist [ : ] ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass while pars_to_prune : corrected_model_desc = remove_from_model_desc ( pars_to_prune [ 0 ] , corrected_model_desc ) fit = fm . ols ( corrected_model_desc , data = self . df ) . fit ( ) pars_to_prune = fit . pvalues . where ( fit . pvalues > p_max ) . dropna ( ) . index . tolist ( ) try : pars_to_prune . remove ( 'Intercept' ) except : pass return fit\"], ['Return the best fit based on rsquared', 'def find_best_rsquared ( list_of_fits ) : res = sorted ( list_of_fits , key = lambda x : x . rsquared ) return res [ - 1 ]'], ['Return a df with predictions and confidence interval', \"def _predict ( self , fit , df ) : df_res = df . copy ( ) if 'Intercept' in fit . model . exog_names : df_res [ 'Intercept' ] = 1.0 df_res [ 'predicted' ] = fit . predict ( df_res ) if not self . allow_negative_predictions : df_res . loc [ df_res [ 'predicted' ] < 0 , 'predicted' ] = 0 prstd , interval_l , interval_u = wls_prediction_std ( fit , df_res [ fit . model . exog_names ] , alpha = 1 - self . confint ) df_res [ 'interval_l' ] = interval_l df_res [ 'interval_u' ] = interval_u if 'Intercept' in df_res : df_res . drop ( labels = [ 'Intercept' ] , axis = 1 , inplace = True ) return df_res\"], ['Calculate the relative abundance of each OTUID in a Sample .', 'def relative_abundance ( biomf , sampleIDs = None ) : if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating relative abundances: The sampleIDs provided do\" \" not match the sampleIDs in biom file. Please double check the sampleIDs\" \" provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) norm_biomf = biomf . norm ( inplace = False ) return { sample : { otuID : norm_biomf . get_value_by_ids ( otuID , sample ) for otuID in otuIDs } for sample in sampleIDs }'], ['Calculate the mean OTU abundance percentage .', 'def mean_otu_pct_abundance ( ra , otuIDs ) : sids = ra . keys ( ) otumeans = defaultdict ( int ) for oid in otuIDs : otumeans [ oid ] = sum ( [ ra [ sid ] [ oid ] for sid in sids if oid in ra [ sid ] ] ) / len ( sids ) * 100 return otumeans'], ['Calculate the mean relative abundance percentage .', 'def MRA ( biomf , sampleIDs = None , transform = None ) : ra = relative_abundance ( biomf , sampleIDs ) if transform is not None : ra = { sample : { otuID : transform ( abd ) for otuID , abd in ra [ sample ] . items ( ) } for sample in ra . keys ( ) } otuIDs = biomf . ids ( axis = \"observation\" ) return mean_otu_pct_abundance ( ra , otuIDs )'], ['Calculate the total number of sequences in each OTU or SampleID .', 'def raw_abundance ( biomf , sampleIDs = None , sample_abd = True ) : results = defaultdict ( int ) if sampleIDs is None : sampleIDs = biomf . ids ( ) else : try : for sid in sampleIDs : assert sid in biomf . ids ( ) except AssertionError : raise ValueError ( \"\\\\nError while calculating raw total abundances: The sampleIDs provided \" \"do not match the sampleIDs in biom file. Please double check the \" \"sampleIDs provided.\\\\n\" ) otuIDs = biomf . ids ( axis = \"observation\" ) for sampleID in sampleIDs : for otuID in otuIDs : abd = biomf . get_value_by_ids ( otuID , sampleID ) if sample_abd : results [ sampleID ] += abd else : results [ otuID ] += abd return results'], ['Function to transform the total abundance calculation for each sample ID to another format based on user given transformation function .', 'def transform_raw_abundance ( biomf , fn = math . log10 , sampleIDs = None , sample_abd = True ) : totals = raw_abundance ( biomf , sampleIDs , sample_abd ) return { sid : fn ( abd ) for sid , abd in totals . items ( ) }'], ['Compute the Mann - Whitney U test for unequal group sample sizes .', 'def print_MannWhitneyU ( div_calc ) : try : x = div_calc . values ( ) [ 0 ] . values ( ) y = div_calc . values ( ) [ 1 ] . values ( ) except : return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \" \"significance testing.\" T , p = stats . mannwhitneyu ( x , y ) print \"\\\\nMann-Whitney U test statistic:\" , T print \"Two-tailed p-value: {}\" . format ( 2 * p )'], ['Compute the Kruskal - Wallis H - test for independent samples . A typical rule is that each group must have at least 5 measurements .', 'def print_KruskalWallisH ( div_calc ) : calc = defaultdict ( list ) try : for k1 , v1 in div_calc . iteritems ( ) : for k2 , v2 in v1 . iteritems ( ) : calc [ k1 ] . append ( v2 ) except : return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \" \"significance testing.\" h , p = stats . kruskal ( * calc . values ( ) ) print \"\\\\nKruskal-Wallis H-test statistic for {} groups: {}\" . format ( str ( len ( div_calc ) ) , h ) print \"p-value: {}\" . format ( p )'], ['Parses the given options passed in at the command line .', 'def handle_program_options ( ) : parser = argparse . ArgumentParser ( description = \"Calculate the alpha diversity\\\\                                     of a set of samples using one or more \\\\                                     metrics and output a kernal density \\\\                                     estimator-smoothed histogram of the \\\\                                     results.\" ) parser . add_argument ( \"-m\" , \"--map_file\" , help = \"QIIME mapping file.\" ) parser . add_argument ( \"-i\" , \"--biom_fp\" , help = \"Path to the BIOM table\" ) parser . add_argument ( \"-c\" , \"--category\" , help = \"Specific category from the mapping file.\" ) parser . add_argument ( \"-d\" , \"--diversity\" , default = [ \"shannon\" ] , nargs = \"+\" , help = \"The alpha diversity metric. Default \\\\                             value is \\'shannon\\', which will calculate the Shannon\\\\                             entropy. Multiple metrics can be specified (space separated).\\\\                             The full list of metrics is available at:\\\\                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\\                             Beta diversity metrics will be supported in the future.\" ) parser . add_argument ( \"--x_label\" , default = [ None ] , nargs = \"+\" , help = \"The name of the diversity metric to be displayed on the\\\\                        plot as the X-axis label. If multiple metrics are specified,\\\\                        then multiple entries for the X-axis label should be given.\" ) parser . add_argument ( \"--color_by\" , help = \"A column name in the mapping file containing\\\\                              hexadecimal (#FF0000) color values that will\\\\                              be used to color the groups. Each sample ID must\\\\                              have a color entry.\" ) parser . add_argument ( \"--plot_title\" , default = \"\" , help = \"A descriptive title that will appear at the top \\\\                        of the output plot. Surround with quotes if there are\\\\                        spaces in the title.\" ) parser . add_argument ( \"-o\" , \"--output_dir\" , default = \".\" , help = \"The directory plots will be saved to.\" ) parser . add_argument ( \"--image_type\" , default = \"png\" , help = \"The type of image to save: png, svg, pdf, eps, etc...\" ) parser . add_argument ( \"--save_calculations\" , help = \"Path and name of text file to store the calculated \" \"diversity metrics.\" ) parser . add_argument ( \"--suppress_stats\" , action = \"store_true\" , help = \"Do not display \" \"significance testing results which are shown by default.\" ) parser . add_argument ( \"--show_available_metrics\" , action = \"store_true\" , help = \"Supply this parameter to see which alpha diversity metrics \" \" are available for usage. No calculations will be performed\" \" if this parameter is provided.\" ) return parser . parse_args ( )'], ['make blast db', \"def blastdb ( fasta , maxfile = 10000000 ) : db = fasta . rsplit ( '.' , 1 ) [ 0 ] type = check_type ( fasta ) if type == 'nucl' : type = [ 'nhr' , type ] else : type = [ 'phr' , type ] if os . path . exists ( '%s.%s' % ( db , type [ 0 ] ) ) is False and os . path . exists ( '%s.00.%s' % ( db , type [ 0 ] ) ) is False : print ( '# ... making blastdb for: %s' % ( fasta ) , file = sys . stderr ) os . system ( 'makeblastdb \\\\                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' % ( fasta , db , type [ 1 ] , maxfile ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['make usearch db', \"def usearchdb ( fasta , alignment = 'local' , usearch_loc = 'usearch' ) : if '.udb' in fasta : print ( '# ... database found: %s' % ( fasta ) , file = sys . stderr ) return fasta type = check_type ( fasta ) db = '%s.%s.udb' % ( fasta . rsplit ( '.' , 1 ) [ 0 ] , type ) if os . path . exists ( db ) is False : print ( '# ... making usearch db for: %s' % ( fasta ) , file = sys . stderr ) if alignment == 'local' : os . system ( '%s -makeudb_ublast %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) elif alignment == 'global' : os . system ( '%s -makeudb_usearch %s -output %s >> log.txt' % ( usearch_loc , fasta , db ) ) else : print ( '# ... database found for: %s' % ( fasta ) , file = sys . stderr ) return db\"], ['Pretty print .', \"def _pp ( dict_data ) : for key , val in dict_data . items ( ) : print ( '{0:<11}: {1}' . format ( key , val ) )\"], ['Print licenses .', \"def print_licences ( params , metadata ) : if hasattr ( params , 'licenses' ) : if params . licenses : _pp ( metadata . licenses_desc ( ) ) sys . exit ( 0 )\"], ['Check repository existence .', 'def check_repository_existence ( params ) : repodir = os . path . join ( params . outdir , params . name ) if os . path . isdir ( repodir ) : raise Conflict ( \\'Package repository \"{0}\" has already exists.\\' . format ( repodir ) )'], ['Generate package repository .', 'def generate_package ( params ) : pkg_data = package . PackageData ( params ) pkg_tree = package . PackageTree ( pkg_data ) pkg_tree . generate ( ) pkg_tree . move ( ) VCS ( os . path . join ( pkg_tree . outdir , pkg_tree . name ) , pkg_tree . pkg_data )'], ['print single reads to stderr', \"def print_single ( line , rev ) : if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] fq = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] print ( '\\\\n' . join ( fq ) , file = sys . stderr )\"], ['convert sam to fastq', \"def sam2fastq ( sam , singles = False , force = False ) : L , R = None , None for line in sam : if line . startswith ( '@' ) is True : continue line = line . strip ( ) . split ( ) bit = [ True if i == '1' else False for i in bin ( int ( line [ 1 ] ) ) . split ( 'b' ) [ 1 ] [ : : - 1 ] ] while len ( bit ) < 8 : bit . append ( False ) pair , proper , na , nap , rev , mrev , left , right = bit if pair is False : if singles is True : print_single ( line , rev ) continue if rev is True : seq = rc ( [ '' , line [ 9 ] ] ) [ 1 ] qual = line [ 10 ] [ : : - 1 ] else : seq = line [ 9 ] qual = line [ 10 ] if left is True : if L is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if L is not None : L = None continue L = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if R is not None : yield L yield R L , R = None , None if right is True : if R is not None and force is False : print ( 'sam file is not sorted' , file = sys . stderr ) print ( '\\\\te.g.: %s' % ( line [ 0 ] ) , file = sys . stderr ) exit ( ) if R is not None : R = None continue R = [ '@%s' % line [ 0 ] , seq , '+%s' % line [ 0 ] , qual ] if L is not None : yield L yield R L , R = None , None\"], ['sort sam file', 'def sort_sam ( sam , sort ) : tempdir = \\'%s/\\' % ( os . path . abspath ( sam ) . rsplit ( \\'/\\' , 1 ) [ 0 ] ) if sort is True : mapping = \\'%s.sorted.sam\\' % ( sam . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if sam != \\'-\\' : if os . path . exists ( mapping ) is False : os . system ( \"\\\\                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\\                    \" % ( sbuffer , tempdir , mapping , sam ) ) else : mapping = \\'stdin-sam.sorted.sam\\' p = Popen ( \"sort -k1 --buffer-size=%sG -T %s -o %s\" % ( sbuffer , tempdir , mapping ) , stdin = sys . stdin , shell = True ) p . communicate ( ) mapping = open ( mapping ) else : if sam == \\'-\\' : mapping = sys . stdin else : mapping = open ( sam ) return mapping'], ['randomly subset sam file', \"def sub_sam ( sam , percent , sort = True , sbuffer = False ) : mapping = sort_sam ( sam , sort ) pool = [ 1 for i in range ( 0 , percent ) ] + [ 0 for i in range ( 0 , 100 - percent ) ] c = cycle ( [ 1 , 2 ] ) for line in mapping : line = line . strip ( ) . split ( ) if line [ 0 ] . startswith ( '@' ) : yield line continue if int ( line [ 1 ] ) <= 20 : if random . choice ( pool ) == 1 : yield line else : n = next ( c ) if n == 1 : prev = line if n == 2 and random . choice ( pool ) == 1 : yield prev yield line\"], ['convert fq to fa', \"def fq2fa ( fq ) : c = cycle ( [ 1 , 2 , 3 , 4 ] ) for line in fq : n = next ( c ) if n == 1 : seq = [ '>%s' % ( line . strip ( ) . split ( '@' , 1 ) [ 1 ] ) ] if n == 2 : seq . append ( line . strip ( ) ) yield seq\"], ['Converts the returned value of wrapped function to the type of the first arg or to the type specified by a kwarg key return_type s value .', \"def change_return_type ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) elif len ( args ) > 0 : return_type = type ( args [ 0 ] ) return return_type ( f ( * args , ** kwargs ) ) else : return f ( * args , ** kwargs ) return wrapper\"], ['Converts all args to set type via self . setify function .', 'def convert_args_to_sets ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : args = ( setify ( x ) for x in args ) return f ( * args , ** kwargs ) return wrapper'], ['Membuat objek - objek entri dari laman yang diambil .', \"def _init_entri ( self , laman ) : sup = BeautifulSoup ( laman . text , 'html.parser' ) estr = '' for label in sup . find ( 'hr' ) . next_siblings : if label . name == 'hr' : self . entri . append ( Entri ( estr ) ) break if label . name == 'h2' : if estr : self . entri . append ( Entri ( estr ) ) estr = '' estr += str ( label ) . strip ( )\"], ['Memproses kata dasar yang ada dalam nama entri .', \"def _init_kata_dasar ( self , dasar ) : for tiap in dasar : kata = tiap . find ( 'a' ) dasar_no = kata . find ( 'sup' ) kata = ambil_teks_dalam_label ( kata ) self . kata_dasar . append ( kata + ' [{}]' . format ( dasar_no . text . strip ( ) ) if dasar_no else kata )\"], ['Mengembalikan hasil serialisasi objek Entri ini .', 'def serialisasi ( self ) : return { \"nama\" : self . nama , \"nomor\" : self . nomor , \"kata_dasar\" : self . kata_dasar , \"pelafalan\" : self . pelafalan , \"bentuk_tidak_baku\" : self . bentuk_tidak_baku , \"varian\" : self . varian , \"makna\" : [ makna . serialisasi ( ) for makna in self . makna ] }'], ['Mengembalikan representasi string untuk semua makna entri ini .', 'def _makna ( self ) : if len ( self . makna ) > 1 : return \\'\\\\n\\' . join ( str ( i ) + \". \" + str ( makna ) for i , makna in enumerate ( self . makna , 1 ) ) return str ( self . makna [ 0 ] )'], ['Mengembalikan representasi string untuk nama entri ini .', 'def _nama ( self ) : hasil = self . nama if self . nomor : hasil += \" [{}]\" . format ( self . nomor ) if self . kata_dasar : hasil = \" » \". j oin( s elf. k ata_dasar)      » \" + h sil return hasil'], ['Mengembalikan representasi string untuk varian entri ini . Dapat digunakan untuk Varian maupun Bentuk tidak baku .', 'def _varian ( self , varian ) : if varian == self . bentuk_tidak_baku : nama = \"Bentuk tidak baku\" elif varian == self . varian : nama = \"Varian\" else : return \\'\\' return nama + \\': \\' + \\', \\' . join ( varian )'], ['Memproses kelas kata yang ada dalam makna .', \"def _init_kelas ( self , makna_label ) : kelas = makna_label . find ( color = 'red' ) lain = makna_label . find ( color = 'darkgreen' ) info = makna_label . find ( color = 'green' ) if kelas : kelas = kelas . find_all ( 'span' ) if lain : self . kelas = { lain . text . strip ( ) : lain [ 'title' ] . strip ( ) } self . submakna = lain . next_sibling . strip ( ) self . submakna += ' ' + makna_label . find ( color = 'grey' ) . text . strip ( ) else : self . kelas = { k . text . strip ( ) : k [ 'title' ] . strip ( ) for k in kelas } if kelas else { } self . info = info . text . strip ( ) if info else ''\"], ['Memproses contoh yang ada dalam makna .', \"def _init_contoh ( self , makna_label ) : indeks = makna_label . text . find ( ': ' ) if indeks != - 1 : contoh = makna_label . text [ indeks + 2 : ] . strip ( ) self . contoh = contoh . split ( '; ' ) else : self . contoh = [ ]\"], ['Mengembalikan hasil serialisasi objek Makna ini .', 'def serialisasi ( self ) : return { \"kelas\" : self . kelas , \"submakna\" : self . submakna , \"info\" : self . info , \"contoh\" : self . contoh }'], ['Build sphinx documentation .', 'def build_sphinx ( pkg_data , projectdir ) : try : version , _minor_version = pkg_data . version . rsplit ( \\'.\\' , 1 ) except ValueError : version = pkg_data . version args = \\' \\' . join ( ( \\'sphinx-quickstart\\' , \\'--sep\\' , \\'-q\\' , \\'-p \"{name}\"\\' , \\'-a \"{author}\"\\' , \\'-v \"{version}\"\\' , \\'-r \"{release}\"\\' , \\'-l en\\' , \\'--suffix=.rst\\' , \\'--master=index\\' , \\'--ext-autodoc\\' , \\'--ext-viewcode\\' , \\'--makefile\\' , \\'{projectdir}\\' ) ) . format ( name = pkg_data . name , author = pkg_data . author , version = version , release = pkg_data . version , projectdir = projectdir ) if subprocess . call ( shlex . split ( args ) ) == 0 : _touch_gitkeep ( projectdir )'], ['make bowtie db', \"def bowtiedb ( fa , keepDB ) : btdir = '%s/bt2' % ( os . getcwd ( ) ) if not os . path . exists ( btdir ) : os . mkdir ( btdir ) btdb = '%s/%s' % ( btdir , fa . rsplit ( '/' , 1 ) [ - 1 ] ) if keepDB is True : if os . path . exists ( '%s.1.bt2' % ( btdb ) ) : return btdb p = subprocess . Popen ( 'bowtie2-build -q %s %s' % ( fa , btdb ) , shell = True ) p . communicate ( ) return btdb\"], ['generate bowtie2 command', \"def bowtie ( sam , btd , f , r , u , opt , no_shrink , threads ) : bt2 = 'bowtie2 -x %s -p %s ' % ( btd , threads ) if f is not False : bt2 += '-1 %s -2 %s ' % ( f , r ) if u is not False : bt2 += '-U %s ' % ( u ) bt2 += opt if no_shrink is False : if f is False : bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' | shrinksam -k %s-shrunk.sam ' % ( sam ) else : bt2 += ' > %s.sam' % ( sam ) return bt2\"], ['map all read sets against all fasta files', \"def crossmap ( fas , reads , options , no_shrink , keepDB , threads , cluster , nodes ) : if cluster is True : threads = '48' btc = [ ] for fa in fas : btd = bowtiedb ( fa , keepDB ) F , R , U = reads if F is not False : if U is False : u = False for i , f in enumerate ( F ) : r = R [ i ] if U is not False : u = U [ i ] sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , f . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) else : f = False r = False for u in U : sam = '%s/%s-vs-%s' % ( os . getcwd ( ) , fa . rsplit ( '/' , 1 ) [ - 1 ] , u . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 3 ) [ 0 ] ) btc . append ( bowtie ( sam , btd , f , r , u , options , no_shrink , threads ) ) if cluster is False : for i in btc : p = subprocess . Popen ( i , shell = True ) p . communicate ( ) else : ID = '' . join ( random . choice ( [ str ( i ) for i in range ( 0 , 9 ) ] ) for _ in range ( 5 ) ) for node , commands in enumerate ( chunks ( btc , nodes ) , 1 ) : bs = open ( '%s/crossmap-qsub.%s.%s.sh' % ( os . getcwd ( ) , ID , node ) , 'w' ) print ( '\\\\n' . join ( commands ) , file = bs ) bs . close ( ) p = subprocess . Popen ( 'qsub -V -N crossmap %s' % ( bs . name ) , shell = True ) p . communicate ( )\"], ['Returns a connection object from the router given args .', \"def get_conn ( self , * args , ** kwargs ) : connections = self . __connections_for ( 'get_conn' , args = args , kwargs = kwargs ) if len ( connections ) is 1 : return connections [ 0 ] else : return connections\"], ['return the non - direct init if the direct algorithm has been selected .', 'def __get_nondirect_init ( self , init ) : crc = init for i in range ( self . Width ) : bit = crc & 0x01 if bit : crc ^= self . Poly crc >>= 1 if bit : crc |= self . MSB_Mask return crc & self . Mask'], ['reflect a data word i . e . reverts the bit order .', 'def reflect ( self , data , width ) : x = data & 0x01 for i in range ( width - 1 ) : data >>= 1 x = ( x << 1 ) | ( data & 0x01 ) return x'], ['Classic simple and slow CRC implementation . This function iterates bit by bit over the augmented input message and returns the calculated CRC value at the end .', 'def bit_by_bit ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] register = self . NonDirectInit for octet in in_data : if self . ReflectIn : octet = self . reflect ( octet , 8 ) for i in range ( 8 ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) | ( ( octet >> ( 7 - i ) ) & 0x01 ) if topbit : register ^= self . Poly for i in range ( self . Width ) : topbit = register & self . MSB_Mask register = ( ( register << 1 ) & self . Mask ) if topbit : register ^= self . Poly if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['This function generates the CRC table used for the table_driven CRC algorithm . The Python version cannot handle tables of an index width other than 8 . See the generated C code for tables with different sizes instead .', 'def gen_table ( self ) : table_length = 1 << self . TableIdxWidth tbl = [ 0 ] * table_length for i in range ( table_length ) : register = i if self . ReflectIn : register = self . reflect ( register , self . TableIdxWidth ) register = register << ( self . Width - self . TableIdxWidth + self . CrcShift ) for j in range ( self . TableIdxWidth ) : if register & ( self . MSB_Mask << self . CrcShift ) != 0 : register = ( register << 1 ) ^ ( self . Poly << self . CrcShift ) else : register = ( register << 1 ) if self . ReflectIn : register = self . reflect ( register >> self . CrcShift , self . Width ) << self . CrcShift tbl [ i ] = register & ( self . Mask << self . CrcShift ) return tbl'], ['The Standard table_driven CRC algorithm .', 'def table_driven ( self , in_data ) : if isinstance ( in_data , str ) : in_data = [ ord ( c ) for c in in_data ] tbl = self . gen_table ( ) register = self . DirectInit << self . CrcShift if not self . ReflectIn : for octet in in_data : tblidx = ( ( register >> ( self . Width - self . TableIdxWidth + self . CrcShift ) ) ^ octet ) & 0xff register = ( ( register << ( self . TableIdxWidth - self . CrcShift ) ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = register >> self . CrcShift else : register = self . reflect ( register , self . Width + self . CrcShift ) << self . CrcShift for octet in in_data : tblidx = ( ( register >> self . CrcShift ) ^ octet ) & 0xff register = ( ( register >> self . TableIdxWidth ) ^ tbl [ tblidx ] ) & ( self . Mask << self . CrcShift ) register = self . reflect ( register , self . Width + self . CrcShift ) & self . Mask if self . ReflectOut : register = self . reflect ( register , self . Width ) return register ^ self . XorOut'], ['parse masked sequence into non - masked and masked regions', 'def parse_masked ( seq , min_len ) : nm , masked = [ ] , [ [ ] ] prev = None for base in seq [ 1 ] : if base . isupper ( ) : nm . append ( base ) if masked != [ [ ] ] and len ( masked [ - 1 ] ) < min_len : nm . extend ( masked [ - 1 ] ) del masked [ - 1 ] prev = False elif base . islower ( ) : if prev is False : masked . append ( [ ] ) masked [ - 1 ] . append ( base ) prev = True return nm , masked'], ['remove masked regions from fasta file as long as they are longer than min_len', \"def strip_masked ( fasta , min_len , print_masked ) : for seq in parse_fasta ( fasta ) : nm , masked = parse_masked ( seq , min_len ) nm = [ '%s removed_masked >=%s' % ( seq [ 0 ] , min_len ) , '' . join ( nm ) ] yield [ 0 , nm ] if print_masked is True : for i , m in enumerate ( [ i for i in masked if i != [ ] ] , 1 ) : m = [ '%s insertion:%s' % ( seq [ 0 ] , i ) , '' . join ( m ) ] yield [ 1 , m ]\"], ['Return arcsine transformed relative abundance from a BIOM format file .', 'def get_relative_abundance ( biomfile ) : biomf = biom . load_table ( biomfile ) norm_biomf = biomf . norm ( inplace = False ) rel_abd = { } for sid in norm_biomf . ids ( ) : rel_abd [ sid ] = { } for otuid in norm_biomf . ids ( \"observation\" ) : otuname = oc . otu_name ( norm_biomf . metadata ( otuid , axis = \"observation\" ) [ \"taxonomy\" ] ) otuname = \" \" . join ( otuname . split ( \"_\" ) ) abd = norm_biomf . get_value_by_ids ( otuid , sid ) rel_abd [ sid ] [ otuname ] = abd ast_rel_abd = bc . arcsine_sqrt_transform ( rel_abd ) return ast_rel_abd'], ['Find an OTU ID in a Newick - format tree . Return the starting position of the ID or None if not found .', 'def find_otu ( otuid , tree ) : for m in re . finditer ( otuid , tree ) : before , after = tree [ m . start ( ) - 1 ] , tree [ m . start ( ) + len ( otuid ) ] if before in [ \"(\" , \",\" , \")\" ] and after in [ \":\" , \";\" ] : return m . start ( ) return None'], ['Replace the OTU ids in the Newick phylogenetic tree format with truncated OTU names', 'def newick_replace_otuids ( tree , biomf ) : for val , id_ , md in biomf . iter ( axis = \"observation\" ) : otu_loc = find_otu ( id_ , tree ) if otu_loc is not None : tree = tree [ : otu_loc ] + oc . otu_name ( md [ \"taxonomy\" ] ) + tree [ otu_loc + len ( id_ ) : ] return tree'], ['return genome info for choosing representative', \"def genome_info ( genome , info ) : try : scg = info [ '#SCGs' ] dups = info [ '#SCG duplicates' ] length = info [ 'genome size (bp)' ] return [ scg - dups , length , genome ] except : return [ False , False , info [ 'genome size (bp)' ] , genome ]\"], ['choose represenative genome and print cluster information', \"def print_clusters ( fastas , info , ANI ) : header = [ '#cluster' , 'num. genomes' , 'rep.' , 'genome' , '#SCGs' , '#SCG duplicates' , 'genome size (bp)' , 'fragments' , 'list' ] yield header in_cluster = [ ] for cluster_num , cluster in enumerate ( connected_components ( ANI ) ) : cluster = sorted ( [ genome_info ( genome , info [ genome ] ) for genome in cluster ] , key = lambda x : x [ 0 : ] , reverse = True ) rep = cluster [ 0 ] [ - 1 ] cluster = [ i [ - 1 ] for i in cluster ] size = len ( cluster ) for genome in cluster : in_cluster . append ( genome ) try : stats = [ size , rep , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] except : stats = [ size , rep , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , cluster ] if rep == genome : stats = [ '*%s' % ( cluster_num ) ] + stats else : stats = [ cluster_num ] + stats yield stats try : start = cluster_num + 1 except : start = 0 fastas = set ( [ i . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] for i in fastas ] ) for cluster_num , genome in enumerate ( fastas . difference ( set ( in_cluster ) ) , start ) : try : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , info [ genome ] [ '#SCGs' ] , info [ genome ] [ '#SCG duplicates' ] , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] except : stats = [ '*%s' % ( cluster_num ) , 1 , genome , genome , 'n/a' , 'n/a' , info [ genome ] [ 'genome size (bp)' ] , info [ genome ] [ '# contigs' ] , [ genome ] ] yield stats\"], ['convert ggKbase genome info tables to dictionary', \"def parse_ggKbase_tables ( tables , id_type ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'name' ) : header = line header [ 4 ] = 'genome size (bp)' header [ 12 ] = '#SCGs' header [ 13 ] = '#SCG duplicates' continue name , code , info = line [ 0 ] , line [ 1 ] , line info = [ to_int ( i ) for i in info ] if id_type is False : if 'UNK' in code or 'unknown' in code : code = name if ( name != code ) and ( name and code in g2info ) : print ( '# duplicate name or code in table(s)' , file = sys . stderr ) print ( '# %s and/or %s' % ( name , code ) , file = sys . stderr ) exit ( ) if name not in g2info : g2info [ name ] = { item : stat for item , stat in zip ( header , info ) } if code not in g2info : g2info [ code ] = { item : stat for item , stat in zip ( header , info ) } else : if id_type == 'name' : ID = name elif id_type == 'code' : ID = code else : print ( '# specify name or code column using -id' , file = sys . stderr ) exit ( ) ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['convert checkM genome info tables to dictionary', \"def parse_checkM_tables ( tables ) : g2info = { } for table in tables : for line in open ( table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( 'Bin Id' ) : header = line header [ 8 ] = 'genome size (bp)' header [ 5 ] = '#SCGs' header [ 6 ] = '#SCG duplicates' continue ID , info = line [ 0 ] , line info = [ to_int ( i ) for i in info ] ID = ID . replace ( ' ' , '' ) g2info [ ID ] = { item : stat for item , stat in zip ( header , info ) } if g2info [ ID ] [ 'genome size (bp)' ] == '' : g2info [ ID ] [ 'genome size (bp)' ] = 0 return g2info\"], ['get genome lengths', \"def genome_lengths ( fastas , info ) : if info is False : info = { } for genome in fastas : name = genome . rsplit ( '.' , 1 ) [ 0 ] . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.contigs' ) [ 0 ] if name in info : continue length = 0 fragments = 0 for seq in parse_fasta ( genome ) : length += len ( seq [ 1 ] ) fragments += 1 info [ name ] = { 'genome size (bp)' : length , '# contigs' : fragments } return info\"], ['Returns a list of db keys to route the given call to .', 'def get_dbs ( self , attr , args , kwargs , ** fkwargs ) : if not self . _ready : if not self . setup_router ( args = args , kwargs = kwargs , ** fkwargs ) : raise self . UnableToSetupRouter ( ) retval = self . _pre_routing ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) if retval is not None : args , kwargs = retval if not ( args or kwargs ) : return self . cluster . hosts . keys ( ) try : db_nums = self . _route ( attr = attr , args = args , kwargs = kwargs , ** fkwargs ) except Exception as e : self . _handle_exception ( e ) db_nums = [ ] return self . _post_routing ( attr = attr , db_nums = db_nums , args = args , kwargs = kwargs , ** fkwargs )'], ['Call method to perform any setup', 'def setup_router ( self , args , kwargs , ** fkwargs ) : self . _ready = self . _setup_router ( args = args , kwargs = kwargs , ** fkwargs ) return self . _ready'], ['Perform routing and return db_nums', 'def _route ( self , attr , args , kwargs , ** fkwargs ) : return self . cluster . hosts . keys ( )'], ['Iterates through all connections which were previously listed as unavailable and marks any that have expired their retry_timeout as being up .', 'def check_down_connections ( self ) : now = time . time ( ) for db_num , marked_down_at in self . _down_connections . items ( ) : if marked_down_at + self . retry_timeout <= now : self . mark_connection_up ( db_num )'], ['Marks all connections which were previously listed as unavailable as being up .', 'def flush_down_connections ( self ) : self . _get_db_attempts = 0 for db_num in self . _down_connections . keys ( ) : self . mark_connection_up ( db_num )'], ['Compute standby power', \"def standby ( df , resolution = '24h' , time_window = None ) : if df . empty : raise EmptyDataFrame ( ) df = pd . DataFrame ( df ) def parse_time ( t ) : if isinstance ( t , numbers . Number ) : return pd . Timestamp . utcfromtimestamp ( t ) . time ( ) else : return pd . Timestamp ( t ) . time ( ) if time_window is not None : t_start = parse_time ( time_window [ 0 ] ) t_end = parse_time ( time_window [ 1 ] ) if t_start > t_end : df = df [ ( df . index . time >= t_start ) | ( df . index . time < t_end ) ] else : df = df [ ( df . index . time >= t_start ) & ( df . index . time < t_end ) ] return df . resample ( resolution ) . min ( )\"], ['Compute the share of the standby power in the total consumption .', \"def share_of_standby ( df , resolution = '24h' , time_window = None ) : p_sb = standby ( df , resolution , time_window ) df = df . resample ( resolution ) . mean ( ) p_tot = df . sum ( ) p_standby = p_sb . sum ( ) share_standby = p_standby / p_tot res = share_standby . iloc [ 0 ] return res\"], ['Toggle counter for gas boilers', 'def count_peaks ( ts ) : on_toggles = ts . diff ( ) > 3000 shifted = np . logical_not ( on_toggles . shift ( 1 ) ) result = on_toggles & shifted count = result . sum ( ) return count'], ['Calculate the ratio of input vs . norm over a given interval .', 'def load_factor ( ts , resolution = None , norm = None ) : if norm is None : norm = ts . max ( ) if resolution is not None : ts = ts . resample ( rule = resolution ) . mean ( ) lf = ts / norm return lf'], ['get top hits after sorting by column number', 'def top_hits ( hits , num , column , reverse ) : hits . sort ( key = itemgetter ( column ) , reverse = reverse ) for hit in hits [ 0 : num ] : yield hit'], ['parse b6 output with sorting', \"def numBlast_sort ( blast , numHits , evalueT , bitT ) : header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header hmm = { h : [ ] for h in header } for line in blast : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( '\\\\t' ) line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if evalueT is not False and evalue > evalueT : continue if bitT is not False and bit < bitT : continue for i , h in zip ( line , header ) : hmm [ h ] . append ( i ) hmm = pd . DataFrame ( hmm ) for query , df in hmm . groupby ( by = [ '#query' ] ) : df = df . sort_values ( by = [ 'bitscore' ] , ascending = False ) for hit in df [ header ] . values [ 0 : numHits ] : yield hit\"], ['parse b6 output', \"def numBlast ( blast , numHits , evalueT = False , bitT = False , sort = False ) : if sort is True : for hit in numBlast_sort ( blast , numHits , evalueT , bitT ) : yield hit return header = [ '#query' , 'target' , 'pident' , 'alen' , 'mismatch' , 'gapopen' , 'qstart' , 'qend' , 'tstart' , 'tend' , 'evalue' , 'bitscore' ] yield header prev , hits = None , [ ] for line in blast : line = line . strip ( ) . split ( '\\\\t' ) ID = line [ 0 ] line [ 10 ] , line [ 11 ] = float ( line [ 10 ] ) , float ( line [ 11 ] ) evalue , bit = line [ 10 ] , line [ 11 ] if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 11 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 11 , True ) : yield hit\"], ['parse hmm domain table output this version is faster but does not work unless the table is sorted', \"def numDomtblout ( domtblout , numHits , evalueT , bitT , sort ) : if sort is True : for hit in numDomtblout_sort ( domtblout , numHits , evalueT , bitT ) : yield hit return header = [ '#target name' , 'target accession' , 'tlen' , 'query name' , 'query accession' , 'qlen' , 'full E-value' , 'full score' , 'full bias' , 'domain #' , '# domains' , 'domain c-Evalue' , 'domain i-Evalue' , 'domain score' , 'domain bias' , 'hmm from' , 'hmm to' , 'seq from' , 'seq to' , 'env from' , 'env to' , 'acc' , 'target description' ] yield header prev , hits = None , [ ] for line in domtblout : if line . startswith ( '#' ) : continue line = line . strip ( ) . split ( ) desc = ' ' . join ( line [ 18 : ] ) line = line [ 0 : 18 ] line . append ( desc ) ID = line [ 0 ] + line [ 9 ] line [ 11 ] , line [ 13 ] = float ( line [ 11 ] ) , float ( line [ 13 ] ) evalue , bitscore = line [ 11 ] , line [ 13 ] line [ 11 ] , line [ 13 ] = evalue , bitscore if ID != prev : if len ( hits ) > 0 : for hit in top_hits ( hits , numHits , 13 , True ) : yield hit hits = [ ] if evalueT == False and bitT == False : hits . append ( line ) elif evalue <= evalueT and bitT == False : hits . append ( line ) elif evalue <= evalueT and bit >= bitT : hits . append ( line ) elif evalueT == False and bit >= bitT : hits . append ( line ) prev = ID for hit in top_hits ( hits , numHits , 13 , True ) : yield hit\"], ['convert stockholm to fasta', \"def stock2fa ( stock ) : seqs = { } for line in stock : if line . startswith ( '#' ) is False and line . startswith ( ' ' ) is False and len ( line ) > 3 : id , seq = line . strip ( ) . split ( ) id = id . rsplit ( '/' , 1 ) [ 0 ] id = re . split ( '[0-9]\\\\|' , id , 1 ) [ - 1 ] if id not in seqs : seqs [ id ] = [ ] seqs [ id ] . append ( seq ) if line . startswith ( '//' ) : break return seqs\"], ['Return boolean time series following given week schedule .', \"def week_schedule ( index , on_time = None , off_time = None , off_days = None ) : if on_time is None : on_time = '9:00' if off_time is None : off_time = '17:00' if off_days is None : off_days = [ 'Sunday' , 'Monday' ] if not isinstance ( on_time , datetime . time ) : on_time = pd . to_datetime ( on_time , format = '%H:%M' ) . time ( ) if not isinstance ( off_time , datetime . time ) : off_time = pd . to_datetime ( off_time , format = '%H:%M' ) . time ( ) times = ( index . time >= on_time ) & ( index . time < off_time ) & ( ~ index . weekday_name . isin ( off_days ) ) return pd . Series ( times , index = index )\"], ['Draw a carpet plot of a pandas timeseries .', 'def carpet ( timeseries , ** kwargs ) : cmap = kwargs . pop ( \\'cmap\\' , cm . coolwarm ) norm = kwargs . pop ( \\'norm\\' , LogNorm ( ) ) interpolation = kwargs . pop ( \\'interpolation\\' , \\'nearest\\' ) cblabel = kwargs . pop ( \\'zlabel\\' , timeseries . name if timeseries . name else \\'\\' ) title = kwargs . pop ( \\'title\\' , \\'carpet plot: \\' + timeseries . name if timeseries . name else \\'\\' ) if timeseries . dropna ( ) . empty : print ( \\'skipped {} - no data\\' . format ( title ) ) return ts = timeseries . resample ( \\'15min\\' ) . interpolate ( ) vmin = max ( 0.1 , kwargs . pop ( \\'vmin\\' , ts [ ts > 0 ] . min ( ) ) ) vmax = max ( vmin , kwargs . pop ( \\'vmax\\' , ts . quantile ( .999 ) ) ) mpldatetimes = date2num ( ts . index . to_pydatetime ( ) ) ts . index = pd . MultiIndex . from_arrays ( [ np . floor ( mpldatetimes ) , 2 + mpldatetimes % 1 ] ) df = ts . unstack ( ) fig , ax = plt . subplots ( ) extent = [ df . columns [ 0 ] , df . columns [ - 1 ] , df . index [ - 1 ] + 0.5 , df . index [ 0 ] - 0.5 ] im = plt . imshow ( df , vmin = vmin , vmax = vmax , extent = extent , cmap = cmap , aspect = \\'auto\\' , norm = norm , interpolation = interpolation , ** kwargs ) ax . xaxis_date ( ) ax . xaxis . set_major_locator ( HourLocator ( interval = 2 ) ) ax . xaxis . set_major_formatter ( DateFormatter ( \\'%H:%M\\' ) ) ax . xaxis . grid ( True ) plt . xlabel ( \\'UTC Time\\' ) ax . yaxis_date ( ) dmin , dmax = ax . yaxis . get_data_interval ( ) number_of_days = ( num2date ( dmax ) - num2date ( dmin ) ) . days if abs ( number_of_days ) <= 35 : ax . yaxis . set_major_locator ( DayLocator ( ) ) else : ax . yaxis . set_major_locator ( AutoDateLocator ( ) ) ax . yaxis . set_major_formatter ( DateFormatter ( \"%a, %d %b %Y\" ) ) cbticks = np . logspace ( np . log10 ( vmin ) , np . log10 ( vmax ) , 11 , endpoint = True ) cb = plt . colorbar ( format = \\'%.0f\\' , ticks = cbticks ) cb . set_label ( cblabel ) plt . title ( title ) return im'], ['calculate percent identity', \"def calc_pident_ignore_gaps ( a , b ) : m = 0 mm = 0 for A , B in zip ( list ( a ) , list ( b ) ) : if A == '-' or A == '.' or B == '-' or B == '.' : continue if A == B : m += 1 else : mm += 1 try : return float ( float ( m ) / float ( ( m + mm ) ) ) * 100 except : return 0\"], ['skip column if either is a gap', \"def remove_gaps ( A , B ) : a_seq , b_seq = [ ] , [ ] for a , b in zip ( list ( A ) , list ( B ) ) : if a == '-' or a == '.' or b == '-' or b == '.' : continue a_seq . append ( a ) b_seq . append ( b ) return '' . join ( a_seq ) , '' . join ( b_seq )\"], ['compare pairs of sequences', \"def compare_seqs ( seqs ) : A , B , ignore_gaps = seqs a , b = A [ 1 ] , B [ 1 ] if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) if ignore_gaps is True : pident = calc_pident_ignore_gaps ( a , b ) else : pident = calc_pident ( a , b ) return A [ 0 ] , B [ 0 ] , pident\"], ['calculate Levenshtein ratio of sequences', \"def compare_seqs_leven ( seqs ) : A , B , ignore_gaps = seqs a , b = remove_gaps ( A [ 1 ] , B [ 1 ] ) if len ( a ) != len ( b ) : print ( '# reads are not the same length' , file = sys . stderr ) exit ( ) pident = lr ( a , b ) * 100 return A [ 0 ] , B [ 0 ] , pident\"], ['make pairwise sequence comparisons between aligned sequences', 'def pairwise_compare ( afa , leven , threads , print_list , ignore_gaps ) : seqs = { seq [ 0 ] : seq for seq in nr_fasta ( [ afa ] , append_index = True ) } num_seqs = len ( seqs ) pairs = ( ( i [ 0 ] , i [ 1 ] , ignore_gaps ) for i in itertools . combinations ( list ( seqs . values ( ) ) , 2 ) ) pool = multithread ( threads ) if leven is True : pident = pool . map ( compare_seqs_leven , pairs ) else : compare = pool . imap_unordered ( compare_seqs , pairs ) pident = [ i for i in tqdm ( compare , total = ( num_seqs * num_seqs ) / 2 ) ] pool . close ( ) pool . terminate ( ) pool . join ( ) return to_dictionary ( pident , print_list )'], ['print matrix of pidents to stdout', \"def print_pairwise ( pw , median = False ) : names = sorted ( set ( [ i for i in pw ] ) ) if len ( names ) != 0 : if '>' in names [ 0 ] : yield [ '#' ] + [ i . split ( '>' ) [ 1 ] for i in names if '>' in i ] else : yield [ '#' ] + names for a in names : if '>' in a : yield [ a . split ( '>' ) [ 1 ] ] + [ pw [ a ] [ b ] for b in names ] else : out = [ ] for b in names : if b in pw [ a ] : if median is False : out . append ( max ( pw [ a ] [ b ] ) ) else : out . append ( np . median ( pw [ a ] [ b ] ) ) else : out . append ( '-' ) yield [ a ] + out\"], ['print stats for comparisons', \"def print_comps ( comps ) : if comps == [ ] : print ( 'n/a' ) else : print ( '# min: %s, max: %s, mean: %s' % ( min ( comps ) , max ( comps ) , np . mean ( comps ) ) )\"], ['print min . pident within each clade and then matrix of between - clade max .', \"def compare_clades ( pw ) : names = sorted ( set ( [ i for i in pw ] ) ) for i in range ( 0 , 4 ) : wi , bt = { } , { } for a in names : for b in pw [ a ] : if ';' not in a or ';' not in b : continue pident = pw [ a ] [ b ] cA , cB = a . split ( ';' ) [ i ] , b . split ( ';' ) [ i ] if i == 0 and '_' in cA and '_' in cB : cA = cA . rsplit ( '_' , 1 ) [ 1 ] cB = cB . rsplit ( '_' , 1 ) [ 1 ] elif '>' in cA or '>' in cB : cA = cA . split ( '>' ) [ 1 ] cB = cB . split ( '>' ) [ 1 ] if cA == cB : if cA not in wi : wi [ cA ] = [ ] wi [ cA ] . append ( pident ) else : if cA not in bt : bt [ cA ] = { } if cB not in bt [ cA ] : bt [ cA ] [ cB ] = [ ] bt [ cA ] [ cB ] . append ( pident ) print ( '\\\\n# min. within' ) for clade , pidents in list ( wi . items ( ) ) : print ( '\\\\t' . join ( [ 'wi:%s' % str ( i ) , clade , str ( min ( pidents ) ) ] ) ) comps = [ ] print ( '\\\\n# max. between' ) for comp in print_pairwise ( bt ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps ) comps = [ ] print ( '\\\\n# median between' ) for comp in print_pairwise ( bt , median = True ) : if comp is not None : print ( '\\\\t' . join ( [ 'bt:%s' % str ( i ) ] + [ str ( j ) for j in comp ] ) ) if comp [ 0 ] != '#' : comps . extend ( [ j for j in comp [ 1 : ] if j != '-' ] ) print_comps ( comps )\"], ['convert matrix to dictionary of comparisons', \"def matrix2dictionary ( matrix ) : pw = { } for line in matrix : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : names = line [ 1 : ] continue a = line [ 0 ] for i , pident in enumerate ( line [ 1 : ] ) : b = names [ i ] if a not in pw : pw [ a ] = { } if b not in pw : pw [ b ] = { } if pident != '-' : pident = float ( pident ) pw [ a ] [ b ] = pident pw [ b ] [ a ] = pident return pw\"], ['Set argument parser option .', \"def setoption ( parser , metadata = None ) : parser . add_argument ( '-v' , action = 'version' , version = __version__ ) subparsers = parser . add_subparsers ( help = 'sub commands help' ) create_cmd = subparsers . add_parser ( 'create' ) create_cmd . add_argument ( 'name' , help = 'Specify Python package name.' ) create_cmd . add_argument ( '-d' , dest = 'description' , action = 'store' , help = 'Short description about your package.' ) create_cmd . add_argument ( '-a' , dest = 'author' , action = 'store' , required = True , help = 'Python package author name.' ) create_cmd . add_argument ( '-e' , dest = 'email' , action = 'store' , required = True , help = 'Python package author email address.' ) create_cmd . add_argument ( '-l' , dest = 'license' , choices = metadata . licenses ( ) . keys ( ) , default = 'GPLv3+' , help = 'Specify license. (default: %(default)s)' ) create_cmd . add_argument ( '-s' , dest = 'status' , choices = metadata . status ( ) . keys ( ) , default = 'Alpha' , help = ( 'Specify development status. ' '(default: %(default)s)' ) ) create_cmd . add_argument ( '--no-check' , action = 'store_true' , help = 'No checking package name in PyPI.' ) create_cmd . add_argument ( '--with-samples' , action = 'store_true' , help = 'Generate package with sample code.' ) group = create_cmd . add_mutually_exclusive_group ( required = True ) group . add_argument ( '-U' , dest = 'username' , action = 'store' , help = 'Specify GitHub username.' ) group . add_argument ( '-u' , dest = 'url' , action = 'store' , type = valid_url , help = 'Python package homepage url.' ) create_cmd . add_argument ( '-o' , dest = 'outdir' , action = 'store' , default = os . path . abspath ( os . path . curdir ) , help = 'Specify output directory. (default: $PWD)' ) list_cmd = subparsers . add_parser ( 'list' ) list_cmd . add_argument ( '-l' , dest = 'licenses' , action = 'store_true' , help = 'show license choices.' )\"], ['Parse argument options .', \"def parse_options ( metadata ) : parser = argparse . ArgumentParser ( description = '%(prog)s usage:' , prog = __prog__ ) setoption ( parser , metadata = metadata ) return parser\"], ['Execute main processes .', \"def main ( ) : try : pkg_version = Update ( ) if pkg_version . updatable ( ) : pkg_version . show_message ( ) metadata = control . retreive_metadata ( ) parser = parse_options ( metadata ) argvs = sys . argv if len ( argvs ) <= 1 : parser . print_help ( ) sys . exit ( 1 ) args = parser . parse_args ( ) control . print_licences ( args , metadata ) control . check_repository_existence ( args ) control . check_package_existence ( args ) control . generate_package ( args ) except ( RuntimeError , BackendFailure , Conflict ) as exc : sys . stderr . write ( '{0}\\\\n' . format ( exc ) ) sys . exit ( 1 )\"], ['Check key and set default vaule when it does not exists .', \"def _check_or_set_default_params ( self ) : if not hasattr ( self , 'date' ) : self . _set_param ( 'date' , datetime . utcnow ( ) . strftime ( '%Y-%m-%d' ) ) if not hasattr ( self , 'version' ) : self . _set_param ( 'version' , self . default_version ) if not hasattr ( self , 'description' ) or self . description is None : getattr ( self , '_set_param' ) ( 'description' , self . warning_message )\"], ['Move directory from working directory to output directory .', 'def move ( self ) : if not os . path . isdir ( self . outdir ) : os . makedirs ( self . outdir ) shutil . move ( self . tmpdir , os . path . join ( self . outdir , self . name ) )'], ['Initialize VCS repository .', 'def vcs_init ( self ) : VCS ( os . path . join ( self . outdir , self . name ) , self . pkg_data )'], ['Finds the location of the current Steam installation on Windows machines . Returns None for any non - Windows machines or for Windows machines where Steam is not installed .', 'def find_steam_location ( ) : if registry is None : return None key = registry . CreateKey ( registry . HKEY_CURRENT_USER , \"Software\\\\Valve\\\\Steam\" ) return registry . QueryValueEx ( key , \"SteamPath\" ) [ 0 ]'], ['Plot PCoA principal coordinates scaled by the relative abundances of otu_name .', 'def plot_PCoA ( cat_data , otu_name , unifrac , names , colors , xr , yr , outDir , save_as , plot_style ) : fig = plt . figure ( figsize = ( 14 , 8 ) ) ax = fig . add_subplot ( 111 ) for i , cat in enumerate ( cat_data ) : plt . scatter ( cat_data [ cat ] [ \"pc1\" ] , cat_data [ cat ] [ \"pc2\" ] , cat_data [ cat ] [ \"size\" ] , color = colors [ cat ] , alpha = 0.85 , marker = \"o\" , edgecolor = \"black\" , label = cat ) lgnd = plt . legend ( loc = \"best\" , scatterpoints = 3 , fontsize = 13 ) for i in range ( len ( colors . keys ( ) ) ) : lgnd . legendHandles [ i ] . _sizes = [ 80 ] plt . title ( \" \" . join ( otu_name . split ( \"_\" ) ) , style = \"italic\" ) plt . ylabel ( \"PC2 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 1 ] ) ) ) plt . xlabel ( \"PC1 (Percent Explained Variance {:.3f}%)\" . format ( float ( unifrac [ \"varexp\" ] [ 0 ] ) ) ) plt . xlim ( round ( xr [ 0 ] * 1.5 , 1 ) , round ( xr [ 1 ] * 1.5 , 1 ) ) plt . ylim ( round ( yr [ 0 ] * 1.5 , 1 ) , round ( yr [ 1 ] * 1.5 , 1 ) ) if plot_style : gu . ggplot2_style ( ax ) fc = \"0.8\" else : fc = \"none\" fig . savefig ( os . path . join ( outDir , \"_\" . join ( otu_name . split ( ) ) ) + \".\" + save_as , facecolor = fc , edgecolor = \"none\" , format = save_as , bbox_inches = \"tight\" , pad_inches = 0.2 ) plt . close ( fig )'], ['Split up the column data in a biom table by mapping category value .', \"def split_by_category ( biom_cols , mapping , category_id ) : columns = defaultdict ( list ) for i , col in enumerate ( biom_cols ) : columns [ mapping [ col [ 'id' ] ] [ category_id ] ] . append ( ( i , col ) ) return columns\"], ['print line if starts with ...', \"def print_line ( l ) : print_lines = [ '# STOCKHOLM' , '#=GF' , '#=GS' , ' ' ] if len ( l . split ( ) ) == 0 : return True for start in print_lines : if l . startswith ( start ) : return True return False\"], ['convert stockholm to single line format', \"def stock2one ( stock ) : lines = { } for line in stock : line = line . strip ( ) if print_line ( line ) is True : yield line continue if line . startswith ( '//' ) : continue ID , seq = line . rsplit ( ' ' , 1 ) if ID not in lines : lines [ ID ] = '' else : seq = seq . strip ( ) lines [ ID ] += seq for ID , line in lines . items ( ) : yield '\\\\t' . join ( [ ID , line ] ) yield '\\\\n//'\"], ['Statics the methods . wut .', \"def math_func ( f ) : @ wraps ( f ) def wrapper ( * args , ** kwargs ) : if len ( args ) > 0 : return_type = type ( args [ 0 ] ) if kwargs . has_key ( 'return_type' ) : return_type = kwargs [ 'return_type' ] kwargs . pop ( 'return_type' ) return return_type ( f ( * args , ** kwargs ) ) args = list ( ( setify ( x ) for x in args ) ) return return_type ( f ( * args , ** kwargs ) ) return wrapper\"], ['Show stats when pings are done', 'def dump_stats ( myStats ) : print ( \"\\\\n----%s PYTHON PING Statistics----\" % ( myStats . thisIP ) ) if myStats . pktsSent > 0 : myStats . fracLoss = ( myStats . pktsSent - myStats . pktsRcvd ) / myStats . pktsSent print ( ( \"%d packets transmitted, %d packets received, \" \"%0.1f%% packet loss\" ) % ( myStats . pktsSent , myStats . pktsRcvd , 100.0 * myStats . fracLoss ) ) if myStats . pktsRcvd > 0 : print ( \"round-trip (ms)  min/avg/max = %d/%0.1f/%d\" % ( myStats . minTime , myStats . totTime / myStats . pktsRcvd , myStats . maxTime ) ) print ( \"\" ) return'], ['bootstrap - py package updatable? .', 'def updatable ( self ) : if self . latest_version > self . current_version : updatable_version = self . latest_version else : updatable_version = False return updatable_version'], ['Show message updatable .', \"def show_message ( self ) : print ( 'current version: {current_version}\\\\n' 'latest version : {latest_version}' . format ( current_version = self . current_version , latest_version = self . latest_version ) )\"], ['Traverse the input otu - sequence file collect the non - unique OTU IDs and file the sequences associated with then under the unique OTU ID as defined by the input matrix .', 'def condense_otus ( otuF , nuniqueF ) : uniqueOTUs = set ( ) nuOTUs = { } for line in nuniqueF : line = line . split ( ) uOTU = line [ 0 ] for nuOTU in line [ 1 : ] : nuOTUs [ nuOTU ] = uOTU uniqueOTUs . add ( uOTU ) otuFilter = defaultdict ( list ) for line in otuF : line = line . split ( ) otuID , seqIDs = line [ 0 ] , line [ 1 : ] if otuID in uniqueOTUs : otuFilter [ otuID ] . extend ( seqIDs ) elif otuID in nuOTUs : otuFilter [ nuOTUs [ otuID ] ] . extend ( seqIDs ) return otuFilter'], ['determine if read overlaps with rna if so count bases', 'def rna_bases ( rna_cov , scaffold , bases , line ) : start = int ( line [ 3 ] ) stop = start + bases - 1 if scaffold not in rna_cov : return rna_cov for pos in rna_cov [ scaffold ] [ 2 ] : ol = get_overlap ( [ start , stop ] , pos ) rna_cov [ scaffold ] [ 0 ] += ol return rna_cov'], ['parse ggKbase scaffold - to - bin mapping - scaffolds - to - bins and bins - to - scaffolds', \"def parse_s2bins ( s2bins ) : s2b = { } b2s = { } for line in s2bins : line = line . strip ( ) . split ( ) s , b = line [ 0 ] , line [ 1 ] if 'UNK' in b : continue if len ( line ) > 2 : g = ' ' . join ( line [ 2 : ] ) else : g = 'n/a' b = '%s\\\\t%s' % ( b , g ) s2b [ s ] = b if b not in b2s : b2s [ b ] = [ ] b2s [ b ] . append ( s ) return s2b , b2s\"], ['remove any bins that don t have 16S', 'def filter_missing_rna ( s2bins , bins2s , rna_cov ) : for bin , scaffolds in list ( bins2s . items ( ) ) : c = 0 for s in scaffolds : if s in rna_cov : c += 1 if c == 0 : del bins2s [ bin ] for scaffold , bin in list ( s2bins . items ( ) ) : if bin not in bins2s : del s2bins [ scaffold ] return s2bins , bins2s'], ['calculate bin coverage', 'def calc_bin_cov ( scaffolds , cov ) : bases = sum ( [ cov [ i ] [ 0 ] for i in scaffolds if i in cov ] ) length = sum ( [ cov [ i ] [ 1 ] for i in scaffolds if i in cov ] ) if length == 0 : return 0 return float ( float ( bases ) / float ( length ) )'], ['Make sure there is at least a translation has been filled in . If a default language has been specified make sure that it exists amongst translations .', \"def clean ( self ) : super ( TranslationFormSet , self ) . clean ( ) if settings . HIDE_LANGUAGE : return if len ( self . forms ) > 0 : if settings . DEFAULT_LANGUAGE and not any ( self . errors ) : for form in self . forms : language_code = form . cleaned_data . get ( 'language_code' , None ) if language_code == settings . DEFAULT_LANGUAGE : return raise forms . ValidationError ( _ ( 'No translation provided for default language \\\\'%s\\\\'.' ) % settings . DEFAULT_LANGUAGE ) else : raise forms . ValidationError ( _ ( 'At least one translation should be provided.' ) )\"], ['If a default language has been set and is still available in self . available_languages return it and remove it from the list .', \"def _get_default_language ( self ) : assert hasattr ( self , 'available_languages' ) , 'No available languages have been generated.' assert len ( self . available_languages ) > 0 , 'No available languages to select from.' if ( settings . DEFAULT_LANGUAGE and settings . DEFAULT_LANGUAGE in self . available_languages ) or ( 'language_code' not in self . form . base_fields ) : self . available_languages . remove ( settings . DEFAULT_LANGUAGE ) return settings . DEFAULT_LANGUAGE else : return self . available_languages . pop ( 0 )\"], ['Construct the form overriding the initial value for language_code .', \"def _construct_form ( self , i , ** kwargs ) : if not settings . HIDE_LANGUAGE : self . _construct_available_languages ( ) form = super ( TranslationFormSet , self ) . _construct_form ( i , ** kwargs ) if settings . HIDE_LANGUAGE : form . instance . language_code = settings . DEFAULT_LANGUAGE else : language_code = form . instance . language_code if language_code : logger . debug ( u'Removing translation choice %s for instance %s' u' in form %d' , language_code , form . instance , i ) self . available_languages . remove ( language_code ) else : initial_language_code = self . _get_default_language ( ) logger . debug ( u'Preselecting language code %s for form %d' , initial_language_code , i ) form . initial [ 'language_code' ] = initial_language_code return form\"], ['merge separate fastq files', 'def fq_merge ( R1 , R2 ) : c = itertools . cycle ( [ 1 , 2 , 3 , 4 ] ) for r1 , r2 in zip ( R1 , R2 ) : n = next ( c ) if n == 1 : pair = [ [ ] , [ ] ] pair [ 0 ] . append ( r1 . strip ( ) ) pair [ 1 ] . append ( r2 . strip ( ) ) if n == 4 : yield pair'], ['Creates hash ring .', \"def _build_circle ( self ) : total_weight = 0 for node in self . _nodes : total_weight += self . _weights . get ( node , 1 ) for node in self . _nodes : weight = self . _weights . get ( node , 1 ) ks = math . floor ( ( 40 * len ( self . _nodes ) * weight ) / total_weight ) for i in xrange ( 0 , int ( ks ) ) : b_key = self . _md5_digest ( '%s-%s-salt' % ( node , i ) ) for l in xrange ( 0 , 4 ) : key = ( ( b_key [ 3 + l * 4 ] << 24 ) | ( b_key [ 2 + l * 4 ] << 16 ) | ( b_key [ 1 + l * 4 ] << 8 ) | b_key [ l * 4 ] ) self . _hashring [ key ] = node self . _sorted_keys . append ( key ) self . _sorted_keys . sort ( )\"], ['Return long integer for a given key that represent it place on the hash ring .', 'def _gen_key ( self , key ) : b_key = self . _md5_digest ( key ) return self . _hashi ( b_key , lambda x : x )'], ['Returns True if there exists a custom image for app_id .', 'def has_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) return any ( map ( os . path . exists , possible_paths ) )'], ['Returns the custom image associated with a given app . If there are multiple candidate images on disk one is chosen arbitrarily .', 'def get_custom_image ( user_context , app_id ) : possible_paths = _valid_custom_image_paths ( user_context , app_id ) existing_images = filter ( os . path . exists , possible_paths ) if len ( existing_images ) > 0 : return existing_images [ 0 ]'], ['Sets the custom image for app_id to be the image located at image_path . If there already exists a custom image for app_id it will be deleted . Returns True is setting the image was successful .', 'def set_custom_image ( user_context , app_id , image_path ) : if image_path is None : return False if not os . path . exists ( image_path ) : return False ( root , ext ) = os . path . splitext ( image_path ) if not is_valid_extension ( ext ) : return False if has_custom_image ( user_context , app_id ) : img = get_custom_image ( user_context , app_id ) assert ( img is not None ) os . remove ( img ) parent_dir = paths . custom_images_directory ( user_context ) new_path = os . path . join ( parent_dir , app_id + ext ) shutil . copyfile ( image_path , new_path ) return True'], ['Read an orthography profile from a metadata file or a default tab - separated profile file .', \"def from_file ( cls , fname , form = None ) : try : tg = TableGroup . from_file ( fname ) opfname = None except JSONDecodeError : tg = TableGroup . fromvalue ( cls . MD ) opfname = fname if len ( tg . tables ) != 1 : raise ValueError ( 'profile description must contain exactly one table' ) metadata = tg . common_props metadata . update ( fname = Path ( fname ) , form = form ) return cls ( * [ { k : None if ( k != cls . GRAPHEME_COL and v == cls . NULL ) else v for k , v in d . items ( ) } for d in tg . tables [ 0 ] . iterdicts ( fname = opfname ) ] , ** metadata )\"], ['Create a Profile instance from the Unicode graphemes found in text .', \"def from_text ( cls , text , mapping = 'mapping' ) : graphemes = Counter ( grapheme_pattern . findall ( text ) ) specs = [ OrderedDict ( [ ( cls . GRAPHEME_COL , grapheme ) , ( 'frequency' , frequency ) , ( mapping , grapheme ) ] ) for grapheme , frequency in graphemes . most_common ( ) ] return cls ( * specs )\"], ['split fasta file into separate fasta files based on list of scaffolds that belong to each separate file', \"def split_fasta ( f , id2f ) : opened = { } for seq in parse_fasta ( f ) : id = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] if id not in id2f : continue fasta = id2f [ id ] if fasta not in opened : opened [ fasta ] = '%s.fa' % fasta seq [ 1 ] += '\\\\n' with open ( opened [ fasta ] , 'a+' ) as f_out : f_out . write ( '\\\\n' . join ( seq ) )\"], ['Check whether pathname is a valid user data directory', 'def _is_user_directory ( self , pathname ) : fullpath = os . path . join ( self . userdata_location ( ) , pathname ) return os . path . isdir ( fullpath ) and pathname . isdigit ( )'], ['Returns an array of user ids for users on the filesystem', 'def local_users ( self ) : userdirs = filter ( self . _is_user_directory , os . listdir ( self . userdata_location ( ) ) ) return map ( lambda userdir : user . User ( self , int ( userdir ) ) , userdirs )'], ['Calculates degree days starting with a series of temperature equivalent values', \"def _calculate_degree_days ( temperature_equivalent , base_temperature , cooling = False ) : if cooling : ret = temperature_equivalent - base_temperature else : ret = base_temperature - temperature_equivalent ret [ ret < 0 ] = 0 prefix = 'CDD' if cooling else 'HDD' ret . name = '{}_{}' . format ( prefix , base_temperature ) return ret\"], ['Development status .', \"def status ( self ) : return { self . _acronym_status ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_status ) }\"], ['OSI Approved license .', \"def licenses ( self ) : return { self . _acronym_lic ( l ) : l for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Remove prefix .', \"def licenses_desc ( self ) : return { self . _acronym_lic ( l ) : l . split ( self . prefix_lic ) [ 1 ] for l in self . resp_text . split ( '\\\\n' ) if l . startswith ( self . prefix_lic ) }\"], ['Convert license acronym .', \"def _acronym_lic ( self , license_statement ) : pat = re . compile ( r'\\\\(([\\\\w+\\\\W?\\\\s?]+)\\\\)' ) if pat . search ( license_statement ) : lic = pat . search ( license_statement ) . group ( 1 ) if lic . startswith ( 'CNRI' ) : acronym_licence = lic [ : 4 ] else : acronym_licence = lic . replace ( ' ' , '' ) else : acronym_licence = '' . join ( [ w [ 0 ] for w in license_statement . split ( self . prefix_lic ) [ 1 ] . split ( ) ] ) return acronym_licence\"], ['calc MD5 based on path', \"def calcMD5 ( path ) : if os . path . exists ( path ) is False : yield False else : command = [ 'md5sum' , path ] p = Popen ( command , stdout = PIPE ) for line in p . communicate ( ) [ 0 ] . splitlines ( ) : yield line . decode ( 'ascii' ) . strip ( ) . split ( ) [ 0 ] p . wait ( ) yield False\"], ['download files with wget', \"def wget ( ftp , f = False , exclude = False , name = False , md5 = False , tries = 10 ) : if f is False : f = ftp . rsplit ( '/' , 1 ) [ - 1 ] t = 0 while md5check ( f , ftp , md5 , exclude ) is not True : t += 1 if name is not False : print ( '# downloading:' , name , f ) if exclude is False : command = 'wget -q --random-wait %s' % ( ftp ) else : command = 'wget -q --random-wait -R %s %s' % ( exclude , ftp ) p = Popen ( command , shell = True ) p . communicate ( ) if t >= tries : print ( 'not downloaded:' , name , f ) return [ f , False ] return [ f , True ]\"], ['check that at least one of queries is in list l', \"def check ( line , queries ) : line = line . strip ( ) spLine = line . replace ( '.' , ' ' ) . split ( ) matches = set ( spLine ) . intersection ( queries ) if len ( matches ) > 0 : return matches , line . split ( '\\\\t' ) return matches , False\"], ['search entrez using specified database and accession', \"def entrez ( db , acc ) : c1 = [ 'esearch' , '-db' , db , '-query' , acc ] c2 = [ 'efetch' , '-db' , 'BioSample' , '-format' , 'docsum' ] p1 = Popen ( c1 , stdout = PIPE , stderr = PIPE ) p2 = Popen ( c2 , stdin = p1 . stdout , stdout = PIPE , stderr = PIPE ) return p2 . communicate ( )\"], ['attempt to use NCBI Entrez to get BioSample ID', \"def searchAccession ( acc ) : out , error = entrez ( 'genome' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'nucleotide' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) out , error = entrez ( 'assembly' , acc ) for line in out . splitlines ( ) : line = line . decode ( 'ascii' ) . strip ( ) if 'Assembly_Accession' in line or 'BioSample' in line : newAcc = line . split ( '>' ) [ 1 ] . split ( '<' ) [ 0 ] . split ( '.' ) [ 0 ] . split ( ',' ) [ 0 ] if len ( newAcc ) > 0 : return ( True , acc , newAcc ) for error in error . splitlines ( ) : error = error . decode ( 'ascii' ) . strip ( ) if '500 Can' in error : return ( False , acc , 'no network' ) return ( False , acc , 'efetch failed' )\"], ['download genome info from NCBI', \"def getFTPs ( accessions , ftp , search , exclude , convert = False , threads = 1 , attempt = 1 , max_attempts = 2 ) : info = wget ( ftp ) [ 0 ] allMatches = [ ] for genome in open ( info , encoding = 'utf8' ) : genome = str ( genome ) matches , genomeInfo = check ( genome , accessions ) if genomeInfo is not False : f = genomeInfo [ 0 ] + search Gftp = genomeInfo [ 19 ] Gftp = Gftp + '/' + search allMatches . extend ( matches ) yield ( Gftp , f , exclude , matches ) newAccs = [ ] missing = accessions . difference ( set ( allMatches ) ) if convert is True : pool = Pool ( threads ) pool = pool . imap_unordered ( searchAccession , missing ) for newAcc in tqdm ( pool , total = len ( missing ) ) : status , accession , newAcc = newAcc if status is True : newAccs . append ( newAcc ) print ( 'not found:' , accession , '->' , newAcc ) else : for accession in missing : print ( 'not found:' , accession ) if len ( newAccs ) > 0 and attempt <= max_attempts : print ( 'convert accession attempt' , attempt ) attempt += 1 for hit in getFTPs ( set ( newAccs ) , ftp , search , exclude , convert , threads = 1 , attempt = attempt ) : yield hit\"], ['download genomes from NCBI', \"def download ( args ) : accessions , infoFTP = set ( args [ 'g' ] ) , args [ 'i' ] search , exclude = args [ 's' ] , args [ 'e' ] FTPs = getFTPs ( accessions , infoFTP , search , exclude , threads = args [ 't' ] , convert = args [ 'convert' ] ) if args [ 'test' ] is True : for genome in FTPs : print ( 'found:' , ';' . join ( genome [ - 1 ] ) , genome [ 0 ] ) return FTPs pool = Pool ( args [ 't' ] ) pool = pool . imap_unordered ( wgetGenome , FTPs ) files = [ ] for f in tqdm ( pool , total = len ( accessions ) ) : files . append ( f ) return files\"], ['remove pesky characters from fasta file header', 'def fix_fasta ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 0 ] = remove_char ( seq [ 0 ] ) if len ( seq [ 1 ] ) > 0 : yield seq'], ['Compute a DataFrame summary of a Stats object .', \"def _calc_frames ( stats ) : timings = [ ] callers = [ ] for key , values in iteritems ( stats . stats ) : timings . append ( pd . Series ( key + values [ : - 1 ] , index = timing_colnames , ) ) for caller_key , caller_values in iteritems ( values [ - 1 ] ) : callers . append ( pd . Series ( key + caller_key + caller_values , index = caller_columns , ) ) timings_df = pd . DataFrame ( timings ) callers_df = pd . DataFrame ( callers ) timings_df [ 'filename:funcname' ] = ( timings_df [ 'filename' ] + ':' + timings_df [ 'funcname' ] ) timings_df = timings_df . groupby ( 'filename:funcname' ) . sum ( ) return timings_df , callers_df\"], ['get unmapped reads', \"def unmapped ( sam , mates ) : for read in sam : if read . startswith ( '@' ) is True : continue read = read . strip ( ) . split ( ) if read [ 2 ] == '*' and read [ 6 ] == '*' : yield read elif mates is True : if read [ 2 ] == '*' or read [ 6 ] == '*' : yield read for i in read : if i == 'YT:Z:UP' : yield read\"], ['execute jobs in processes using N threads', 'def parallel ( processes , threads ) : pool = multithread ( threads ) pool . map ( run_process , processes ) pool . close ( ) pool . join ( )'], ['the final log processor that structlog requires to render .', 'def define_log_renderer ( fmt , fpath , quiet ) : if fmt : return structlog . processors . JSONRenderer ( ) if fpath is not None : return structlog . processors . JSONRenderer ( ) if sys . stderr . isatty ( ) and not quiet : return structlog . dev . ConsoleRenderer ( ) return structlog . processors . JSONRenderer ( )'], ['Add unique id type and hostname', \"def _structlog_default_keys_processor ( logger_class , log_method , event ) : global HOSTNAME if 'id' not in event : event [ 'id' ] = '%s_%s' % ( datetime . utcnow ( ) . strftime ( '%Y%m%dT%H%M%S' ) , uuid . uuid1 ( ) . hex ) if 'type' not in event : event [ 'type' ] = 'log' event [ 'host' ] = HOSTNAME return event\"], ['log processors that structlog executes before final rendering', 'def define_log_processors ( ) : return [ structlog . processors . TimeStamper ( fmt = \"iso\" ) , _structlog_default_keys_processor , structlog . stdlib . PositionalArgumentsFormatter ( ) , structlog . processors . StackInfoRenderer ( ) , structlog . processors . format_exc_info , ]'], ['configures a logger when required write to stderr or a file', 'def _configure_logger ( fmt , quiet , level , fpath , pre_hooks , post_hooks , metric_grouping_interval ) : level = getattr ( logging , level . upper ( ) ) global _GLOBAL_LOG_CONFIGURED if _GLOBAL_LOG_CONFIGURED : return def wrap_hook ( fn ) : @ wraps ( fn ) def processor ( logger , method_name , event_dict ) : fn ( event_dict ) return event_dict return processor processors = define_log_processors ( ) processors . extend ( [ wrap_hook ( h ) for h in pre_hooks ] ) if metric_grouping_interval : processors . append ( metrics_grouping_processor ) log_renderer = define_log_renderer ( fmt , fpath , quiet ) stderr_required = ( not quiet ) pretty_to_stderr = ( stderr_required and ( fmt == \"pretty\" or ( fmt is None and sys . stderr . isatty ( ) ) ) ) should_inject_pretty_renderer = ( pretty_to_stderr and not isinstance ( log_renderer , structlog . dev . ConsoleRenderer ) ) if should_inject_pretty_renderer : stderr_required = False processors . append ( StderrConsoleRenderer ( ) ) processors . append ( log_renderer ) processors . extend ( [ wrap_hook ( h ) for h in post_hooks ] ) streams = [ ] if stderr_required : streams . append ( sys . stderr ) if fpath is not None : streams . append ( open ( fpath , \\'a\\' ) ) assert len ( streams ) != 0 , \"cannot configure logger for 0 streams\" stream = streams [ 0 ] if len ( streams ) == 1 else Stream ( * streams ) atexit . register ( stream . close ) structlog . configure ( processors = processors , context_class = dict , logger_factory = LevelLoggerFactory ( stream , level = level ) , wrapper_class = BoundLevelLogger , cache_logger_on_first_use = True , ) stdlib_root_log = logging . getLogger ( ) stdlib_root_log . addHandler ( StdlibStructlogHandler ( ) ) stdlib_root_log . setLevel ( level ) _GLOBAL_LOG_CONFIGURED = True'], ['Instead of using a processor adding basic information like caller filename etc here .', 'def _add_base_info ( self , event_dict ) : f = sys . _getframe ( ) level_method_frame = f . f_back caller_frame = level_method_frame . f_back return event_dict'], ['Propagate a method call to the wrapped logger .', \"def _proxy_to_logger ( self , method_name , event , * event_args , ** event_kw ) : if isinstance ( event , bytes ) : event = event . decode ( 'utf-8' ) if event_args : event_kw [ 'positional_args' ] = event_args return super ( BoundLevelLogger , self ) . _proxy_to_logger ( method_name , event = event , ** event_kw )\"], ['Given four points of a rectangle translate the rectangle to the specified x and y coordinates and optionally change the width .', 'def translate ( rect , x , y , width = 1 ) : return ( ( rect [ 0 ] [ 0 ] + x , rect [ 0 ] [ 1 ] + y ) , ( rect [ 1 ] [ 0 ] + x , rect [ 1 ] [ 1 ] + y ) , ( rect [ 2 ] [ 0 ] + x + width , rect [ 2 ] [ 1 ] + y ) , ( rect [ 3 ] [ 0 ] + x + width , rect [ 3 ] [ 1 ] + y ) )'], ['remove problem characters from string', \"def remove_bad ( string ) : remove = [ ':' , ',' , '(' , ')' , ' ' , '|' , ';' , '\\\\'' ] for c in remove : string = string . replace ( c , '_' ) return string\"], ['make copy of sequences with short identifier', \"def get_ids ( a ) : a_id = '%s.id.fa' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) a_id_lookup = '%s.id.lookup' % ( a . rsplit ( '.' , 1 ) [ 0 ] ) if check ( a_id ) is True : return a_id , a_id_lookup a_id_f = open ( a_id , 'w' ) a_id_lookup_f = open ( a_id_lookup , 'w' ) ids = [ ] for seq in parse_fasta ( open ( a ) ) : id = id_generator ( ) while id in ids : id = id_generator ( ) ids . append ( id ) header = seq [ 0 ] . split ( '>' ) [ 1 ] name = remove_bad ( header ) seq [ 0 ] = '>%s %s' % ( id , header ) print ( '\\\\n' . join ( seq ) , file = a_id_f ) print ( '%s\\\\t%s\\\\t%s' % ( id , name , header ) , file = a_id_lookup_f ) return a_id , a_id_lookup\"], ['convert fasta to phylip because RAxML is ridiculous', 'def convert2phylip ( convert ) : out = \\'%s.phy\\' % ( convert . rsplit ( \\'.\\' , 1 ) [ 0 ] ) if check ( out ) is False : convert = open ( convert , \\'rU\\' ) out_f = open ( out , \\'w\\' ) alignments = AlignIO . parse ( convert , \"fasta\" ) AlignIO . write ( alignments , out , \"phylip\" ) return out'], ['run IQ - Tree', 'def run_iqtree ( phy , model , threads , cluster , node ) : if threads > 24 : ppn = 24 else : ppn = threads tree = \\'%s.treefile\\' % ( phy ) if check ( tree ) is False : if model is False : model = \\'TEST\\' dir = os . getcwd ( ) command = \\'iqtree-omp -s %s -m %s -nt %s -quiet\\' % ( phy , model , threads ) if cluster is False : p = Popen ( command , shell = True ) else : if node is False : node = \\'1\\' qsub = \\'qsub -l nodes=%s:ppn=%s -m e -N iqtree\\' % ( node , ppn ) command = \\'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree\\' % ( dir , phy , command , dir ) re_call = \\'cd %s; %s --no-fast --iq\\' % ( dir . rsplit ( \\'/\\' , 1 ) [ 0 ] , \\' \\' . join ( sys . argv ) ) p = Popen ( \\'echo \"%s;%s\" | %s\\' % ( command , re_call , qsub ) , shell = True ) p . communicate ( ) return tree'], ['get the names for sequences in the raxml tree', \"def fix_tree ( tree , a_id_lookup , out ) : if check ( out ) is False and check ( tree ) is True : tree = open ( tree ) . read ( ) for line in open ( a_id_lookup ) : id , name , header = line . strip ( ) . split ( '\\\\t' ) tree = tree . replace ( id + ':' , name + ':' ) out_f = open ( out , 'w' ) print ( tree . strip ( ) , file = out_f ) return out\"], ['Creates a new Nydus cluster from the given settings .', \"def create_cluster ( settings ) : settings = copy . deepcopy ( settings ) backend = settings . pop ( 'engine' , settings . pop ( 'backend' , None ) ) if isinstance ( backend , basestring ) : Conn = import_string ( backend ) elif backend : Conn = backend else : raise KeyError ( 'backend' ) cluster = settings . pop ( 'cluster' , None ) if not cluster : Cluster = Conn . get_cluster ( ) elif isinstance ( cluster , basestring ) : Cluster = import_string ( cluster ) else : Cluster = cluster router = settings . pop ( 'router' , None ) if not router : Router = BaseRouter elif isinstance ( router , basestring ) : Router = import_string ( router ) else : Router = router return Cluster ( router = Router , backend = Conn , ** settings )\"], ['Gets the translation of a specific field for a specific language code .', \"def _get_translation ( self , field , code ) : if not code in self . _translation_cache : translations = self . translations . select_related ( ) logger . debug ( u'Matched with field %s for language %s. Attempting lookup.' , field , code ) try : translation_obj = translations . get ( language_code = code ) except ObjectDoesNotExist : translation_obj = None self . _translation_cache [ code ] = translation_obj logger . debug ( u'Translation not found in cache.' ) else : logger . debug ( u'Translation found in cache.' ) translation_obj = self . _translation_cache . get ( code ) if not translation_obj : raise ObjectDoesNotExist field_value = getattr ( translation_obj , field ) logger . debug ( u'Found translation object %s, returning value %s.' , translation_obj , field_value ) return field_value\"], ['Wrapper to allow for easy unicode representation of an object by the specified property . If this wrapper is not able to find the right translation of the specified property it will return the default value instead .', \"def unicode_wrapper ( self , property , default = ugettext ( 'Untitled' ) ) : try : value = getattr ( self , property ) except ValueError : logger . warn ( u'ValueError rendering unicode for %s object.' , self . _meta . object_name ) value = None if not value : value = default return value\"], ['remove insertion columns from aligned fasta file', \"def strip_inserts ( fasta ) : for seq in parse_fasta ( fasta ) : seq [ 1 ] = '' . join ( [ b for b in seq [ 1 ] if b == '-' or b . isupper ( ) ] ) yield seq\"], ['Transform a string s graphemes into the mappings given in a different column in the orthography profile .', 'def transform ( self , word , column = Profile . GRAPHEME_COL , error = errors . replace ) : assert self . op , \\'method can only be called with orthography profile.\\' if column != Profile . GRAPHEME_COL and column not in self . op . column_labels : raise ValueError ( \"Column {0} not found in profile.\" . format ( column ) ) word = self . op . tree . parse ( word , error ) if column == Profile . GRAPHEME_COL : return word out = [ ] for token in word : try : target = self . op . graphemes [ token ] [ column ] except KeyError : target = self . _errors [ \\'replace\\' ] ( token ) if target is not None : if isinstance ( target , ( tuple , list ) ) : out . extend ( target ) else : out . append ( target ) return out'], ['Function to tokenize input string and return output of str with ortho rules applied .', 'def rules ( self , word ) : return self . _rules . apply ( word ) if self . _rules else word'], ['Given a string that is space - delimited on Unicode grapheme clusters group Unicode modifier letters with their preceding base characters deal with tie bars etc .', 'def combine_modifiers ( self , graphemes ) : result = [ ] temp = \"\" count = len ( graphemes ) for grapheme in reversed ( graphemes ) : count -= 1 if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Lm\" and not ord ( grapheme ) in [ 712 , 716 ] : temp = grapheme + temp if count == 0 : result [ - 1 ] = temp + result [ - 1 ] continue if len ( grapheme ) == 1 and ord ( grapheme ) in [ 712 , 716 ] : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue if len ( grapheme ) == 1 and unicodedata . category ( grapheme ) == \"Sk\" : if len ( result ) == 0 : result . append ( grapheme ) temp = \"\" continue else : if unicodedata . category ( result [ - 1 ] [ 0 ] ) == \"Sk\" : result [ - 1 ] = grapheme + result [ - 1 ] temp = \"\" continue result . append ( grapheme + temp ) temp = \"\" segments = result [ : : - 1 ] i = 0 r = [ ] while i < len ( segments ) : if ord ( segments [ i ] [ - 1 ] ) in [ 865 , 860 ] : r . append ( segments [ i ] + segments [ i + 1 ] ) i += 2 else : r . append ( segments [ i ] ) i += 1 return r'], ['parse catalytic RNAs to gff format', \"def parse_catalytic ( insertion , gff ) : offset = insertion [ 'offset' ] GeneStrand = insertion [ 'strand' ] if type ( insertion [ 'intron' ] ) is not str : return gff for intron in parse_fasta ( insertion [ 'intron' ] . split ( '|' ) ) : ID , annot , strand , pos = intron [ 0 ] . split ( '>' ) [ 1 ] . split ( ) Start , End = [ int ( i ) for i in pos . split ( '-' ) ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Rfam' ) gff [ 'feature' ] . append ( 'Catalytic RNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse ORF to gff format', \"def parse_orf ( insertion , gff ) : offset = insertion [ 'offset' ] if type ( insertion [ 'orf' ] ) is not str : return gff for orf in parse_fasta ( insertion [ 'orf' ] . split ( '|' ) ) : ID = orf [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End , strand = [ int ( i ) for i in orf [ 0 ] . split ( ' # ' ) [ 1 : 4 ] ] if strand == 1 : strand = '+' else : strand = '-' GeneStrand = insertion [ 'strand' ] if strand != GeneStrand : if strand == '+' : strand = '-' else : strand = '+' Start , End = End - 2 , Start - 2 Start , End = abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 annot = orf [ 0 ] . split ( ) [ 1 ] if annot == 'n/a' : annot = 'unknown' gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( 'Prodigal and Pfam' ) gff [ 'feature' ] . append ( 'CDS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( ID , annot ) ) return gff\"], ['parse insertion to gff format', \"def parse_insertion ( insertion , gff ) : offset = insertion [ 'offset' ] for ins in parse_fasta ( insertion [ 'insertion sequence' ] . split ( '|' ) ) : strand = insertion [ 'strand' ] ID = ins [ 0 ] . split ( '>' ) [ 1 ] . split ( ) [ 0 ] Start , End = [ int ( i ) for i in ins [ 0 ] . split ( 'gene-pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] Start , End = abs ( Start + offset ) , abs ( End + offset ) if strand == '-' : Start , End = End , Start gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( insertion [ 'source' ] ) gff [ 'feature' ] . append ( 'IVS' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s' % ( ID ) ) return gff\"], ['parse rRNA to gff format', \"def parse_rRNA ( insertion , seq , gff ) : offset = insertion [ 'offset' ] strand = insertion [ 'strand' ] for rRNA in parse_masked ( seq , 0 ) [ 0 ] : rRNA = '' . join ( rRNA ) Start = seq [ 1 ] . find ( rRNA ) + 1 End = Start + len ( rRNA ) - 1 if strand == '-' : Start , End = End - 2 , Start - 2 pos = ( abs ( Start + offset ) - 1 , abs ( End + offset ) - 1 ) Start , End = min ( pos ) , max ( pos ) source = insertion [ 'source' ] annot = '%s rRNA' % ( source . split ( 'from' , 1 ) [ 0 ] ) gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'rRNA' ) gff [ 'start' ] . append ( Start ) gff [ 'end' ] . append ( End ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'Name=%s' % ( annot ) ) return gff\"], ['convert iTable to gff file', \"def iTable2GFF ( iTable , fa , contig = False ) : columns = [ '#seqname' , 'source' , 'feature' , 'start' , 'end' , 'score' , 'strand' , 'frame' , 'attribute' ] gff = { c : [ ] for c in columns } for insertion in iTable . iterrows ( ) : insertion = insertion [ 1 ] if insertion [ 'ID' ] not in fa : continue strand = insertion [ 'sequence' ] . split ( 'strand=' , 1 ) [ 1 ] . split ( ) [ 0 ] if contig is True : gene = [ int ( i ) for i in insertion [ 'sequence' ] . split ( 'pos=' , 1 ) [ 1 ] . split ( ) [ 0 ] . split ( '-' ) ] if strand == '-' : offset = - 1 * ( gene [ 1 ] ) else : offset = gene [ 0 ] else : strand = '+' gene = [ 1 , int ( insertion [ 'sequence' ] . split ( 'total-len=' , 1 ) [ 1 ] . split ( ) [ 0 ] ) ] offset = gene [ 0 ] insertion [ 'strand' ] = strand insertion [ 'offset' ] = offset source = insertion [ 'sequence' ] . split ( '::model' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ - 1 ] insertion [ 'source' ] = source geneAnnot = '%s rRNA gene' % ( source . split ( 'from' , 1 ) [ 0 ] ) geneNum = insertion [ 'sequence' ] . split ( 'seq=' , 1 ) [ 1 ] . split ( ) [ 0 ] gff [ '#seqname' ] . append ( insertion [ 'ID' ] ) gff [ 'source' ] . append ( source ) gff [ 'feature' ] . append ( 'Gene' ) gff [ 'start' ] . append ( gene [ 0 ] ) gff [ 'end' ] . append ( gene [ 1 ] ) gff [ 'score' ] . append ( '.' ) gff [ 'strand' ] . append ( strand ) gff [ 'frame' ] . append ( '.' ) gff [ 'attribute' ] . append ( 'ID=%s; Name=%s' % ( geneNum , geneAnnot ) ) gff = parse_rRNA ( insertion , fa [ insertion [ 'ID' ] ] , gff ) gff = parse_insertion ( insertion , gff ) gff = parse_orf ( insertion , gff ) gff = parse_catalytic ( insertion , gff ) return pd . DataFrame ( gff ) [ columns ] . drop_duplicates ( )\"], ['Given an abundance table group the counts by every taxonomic level .', \"def summarize_taxa ( biom ) : tamtcounts = defaultdict ( int ) tot_seqs = 0.0 for row , col , amt in biom [ 'data' ] : tot_seqs += amt rtax = biom [ 'rows' ] [ row ] [ 'metadata' ] [ 'taxonomy' ] for i , t in enumerate ( rtax ) : t = t . strip ( ) if i == len ( rtax ) - 1 and len ( t ) > 3 and len ( rtax [ - 1 ] ) > 3 : t = 's__' + rtax [ i - 1 ] . strip ( ) . split ( '_' ) [ - 1 ] + '_' + t . split ( '_' ) [ - 1 ] tamtcounts [ t ] += amt lvlData = { lvl : levelData ( tamtcounts , tot_seqs , lvl ) for lvl in [ 'k' , 'p' , 'c' , 'o' , 'f' , 'g' , 's' ] } return tot_seqs , lvlData\"], ['Returns the path to the custom image set for this game or None if no image is set', 'def custom_image ( self , user ) : for ext in self . valid_custom_image_extensions ( ) : image_location = self . _custom_image_path ( user , ext ) if os . path . isfile ( image_location ) : return image_location return None'], ['Sets a custom image for the game . image_path should refer to an image file on disk', 'def set_image ( self , user , image_path ) : _ , ext = os . path . splitext ( image_path ) shutil . copy ( image_path , self . _custom_image_path ( user , ext ) )'], ['get a list of mapped reads', \"def sam_list ( sam ) : list = [ ] for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : list . append ( id ) return set ( list )\"], ['get a list of mapped reads require that both pairs are mapped in the sam file in order to remove the reads', \"def sam_list_paired ( sam ) : list = [ ] pair = [ '1' , '2' ] prev = '' for file in sam : for line in file : if line . startswith ( '@' ) is False : line = line . strip ( ) . split ( ) id , map = line [ 0 ] , int ( line [ 1 ] ) if map != 4 and map != 8 : read = id . rsplit ( '/' ) [ 0 ] if read == prev : list . append ( read ) prev = read return set ( list )\"], ['require that both pairs are mapped in the sam file in order to remove the reads', \"def filter_paired ( list ) : pairs = { } filtered = [ ] for id in list : read = id . rsplit ( '/' ) [ 0 ] if read not in pairs : pairs [ read ] = [ ] pairs [ read ] . append ( id ) for read in pairs : ids = pairs [ read ] if len ( ids ) == 2 : filtered . extend ( ids ) return set ( filtered )\"], ['print fastq from sam', \"def sam2fastq ( line ) : fastq = [ ] fastq . append ( '@%s' % line [ 0 ] ) fastq . append ( line [ 9 ] ) fastq . append ( '+%s' % line [ 0 ] ) fastq . append ( line [ 10 ] ) return fastq\"], ['- check to see if the read maps with < = threshold number of mismatches - mm_option = one or both depending on whether or not one or both reads in a pair need to pass the mismatch threshold - pair can be False if read does not have a pair - make sure alignment score is not 0 which would indicate that the read was not aligned to the reference', \"def check_mismatches ( read , pair , mismatches , mm_option , req_map ) : if pair is False : mm = count_mismatches ( read ) if mm is False : return False if mismatches is False : return True if mm <= mismatches : return True r_mm = count_mismatches ( read ) p_mm = count_mismatches ( pair ) if r_mm is False and p_mm is False : return False if mismatches is False : return True if req_map is True : if r_mm is False or p_mm is False : return False if mm_option == 'one' : if ( r_mm is not False and r_mm <= mismatches ) or ( p_mm is not False and p_mm <= mismatches ) : return True if mm_option == 'both' : if r_mm is False : if p_mm <= mismatches : return True elif p_mm is False : if r_mm <= mismatches : return True elif ( r_mm is not False and r_mm <= mismatches ) and ( p_mm is not False and p_mm <= mismatches ) : return True return False\"], ['determine whether or not reads map to specific region of scaffold', 'def check_region ( read , pair , region ) : if region is False : return True for mapping in read , pair : if mapping is False : continue start , length = int ( mapping [ 3 ] ) , len ( mapping [ 9 ] ) r = [ start , start + length - 1 ] if get_overlap ( r , region ) > 0 : return True return False'], ['Returns a Steam object representing the current Steam installation on the users computer . If the user doesn t have Steam installed returns None .', \"def get_steam ( ) : helper = lambda udd : Steam ( udd ) if os . path . exists ( udd ) else None plat = platform . system ( ) if plat == 'Darwin' : return helper ( paths . default_osx_userdata_path ( ) ) if plat == 'Linux' : return helper ( paths . default_linux_userdata_path ( ) ) if plat == 'Windows' : possible_dir = winutils . find_userdata_directory ( ) return helper ( possible_dir ) if possible_dir is not None else None return None\"], ['normalize from zero to one for row or table', \"def zero_to_one ( table , option ) : if option == 'table' : m = min ( min ( table ) ) ma = max ( max ( table ) ) t = [ ] for row in table : t_row = [ ] if option != 'table' : m , ma = min ( row ) , max ( row ) for i in row : if ma == m : t_row . append ( 0 ) else : t_row . append ( ( i - m ) / ( ma - m ) ) t . append ( t_row ) return t\"], ['calculate percent of total', \"def pertotal ( table , option ) : if option == 'table' : total = sum ( [ i for line in table for i in line ] ) t = [ ] for row in table : t_row = [ ] if option != 'table' : total = sum ( row ) for i in row : if total == 0 : t_row . append ( 0 ) else : t_row . append ( i / total * 100 ) t . append ( t_row ) return t\"], ['scale table based on the column with the largest sum', 'def scale ( table ) : t = [ ] columns = [ [ ] for i in table [ 0 ] ] for row in table : for i , v in enumerate ( row ) : columns [ i ] . append ( v ) sums = [ float ( sum ( i ) ) for i in columns ] scale_to = float ( max ( sums ) ) scale_factor = [ scale_to / i for i in sums if i != 0 ] for row in table : t . append ( [ a * b for a , b in zip ( row , scale_factor ) ] ) return t'], ['fit to normal distribution', \"def norm ( table ) : print ( '# norm dist is broken' , file = sys . stderr ) exit ( ) from matplotlib . pyplot import hist as hist t = [ ] for i in table : t . append ( np . ndarray . tolist ( hist ( i , bins = len ( i ) , normed = True ) [ 0 ] ) ) return t\"], ['log transform each value in table', 'def log_trans ( table ) : t = [ ] all = [ item for sublist in table for item in sublist ] if min ( all ) == 0 : scale = min ( [ i for i in all if i != 0 ] ) * 10e-10 else : scale = 0 for i in table : t . append ( np . ndarray . tolist ( np . log10 ( [ j + scale for j in i ] ) ) ) return t'], ['box - cox transform table', 'def box_cox ( table ) : from scipy . stats import boxcox as bc t = [ ] for i in table : if min ( i ) == 0 : scale = min ( [ j for j in i if j != 0 ] ) * 10e-10 else : scale = 0 t . append ( np . ndarray . tolist ( bc ( np . array ( [ j + scale for j in i ] ) ) [ 0 ] ) ) return t'], ['inverse hyperbolic sine transformation', 'def inh ( table ) : t = [ ] for i in table : t . append ( np . ndarray . tolist ( np . arcsinh ( i ) ) ) return t'], ['from SparCC - randomly draw from the corresponding posterior Dirichlet distribution with a uniform prior', 'def diri ( table ) : t = [ ] for i in table : a = [ j + 1 for j in i ] t . append ( np . ndarray . tolist ( np . random . mtrand . dirichlet ( a ) ) ) return t'], ['Given a list of sample IDs generate unique n - base barcodes for each . Note that only 4^n unique barcodes are possible .', \"def generate_barcodes ( nIds , codeLen = 12 ) : def next_code ( b , c , i ) : return c [ : i ] + b + ( c [ i + 1 : ] if i < - 1 else '' ) def rand_base ( ) : return random . choice ( [ 'A' , 'T' , 'C' , 'G' ] ) def rand_seq ( n ) : return '' . join ( [ rand_base ( ) for _ in range ( n ) ] ) hpf = re . compile ( 'aaaa|cccc|gggg|tttt' , re . IGNORECASE ) while True : codes = [ rand_seq ( codeLen ) ] if ( hpf . search ( codes [ 0 ] ) is None ) : break idx = 0 while len ( codes ) < nIds : idx -= 1 if idx < - codeLen : idx = - 1 codes . append ( rand_seq ( codeLen ) ) else : nc = next_code ( rand_base ( ) , codes [ - 1 ] , idx ) if hpf . search ( nc ) is None : codes . append ( nc ) codes = list ( set ( codes ) ) return codes\"], ['Given a sample ID and a mapping modify a Sanger FASTA file to include the barcode and primer in the sequence data and change the description line as needed .', \"def scrobble_data_dir ( dataDir , sampleMap , outF , qualF = None , idopt = None , utf16 = False ) : seqcount = 0 outfiles = [ osp . split ( outF . name ) [ 1 ] ] if qualF : outfiles . append ( osp . split ( qualF . name ) [ 1 ] ) for item in os . listdir ( dataDir ) : if item in outfiles or not osp . isfile ( os . path . join ( dataDir , item ) ) : continue if osp . splitext ( item ) [ 1 ] in file_types [ 'fasta' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'fasta' ) for record in records : if isinstance ( idopt , tuple ) : sep , field = idopt sampleID = record . id . split ( sep ) [ field - 1 ] else : sampleID = osp . splitext ( item ) [ 0 ] record . seq = ( sampleMap [ sampleID ] . barcode + sampleMap [ sampleID ] . primer + record . seq ) SeqIO . write ( record , outF , 'fasta' ) seqcount += 1 fh . close ( ) elif qualF and osp . splitext ( item ) [ 1 ] in file_types [ 'qual' ] : fh = open_enc ( os . path . join ( dataDir , item ) , utf16 ) records = SeqIO . parse ( fh , 'qual' ) for record in records : mi = sampleMap [ sampleMap . keys ( ) [ 0 ] ] quals = [ 40 for _ in range ( len ( mi . barcode ) + len ( mi . primer ) ) ] record . letter_annotations [ 'phred_quality' ] [ 0 : 0 ] = quals SeqIO . write ( record , qualF , 'qual' ) fh . close ( ) return seqcount\"], ['Applies the arcsine square root transform to the given BIOM - format table', 'def arcsin_sqrt ( biom_tbl ) : arcsint = lambda data , id_ , md : np . arcsin ( np . sqrt ( data ) ) tbl_relabd = relative_abd ( biom_tbl ) tbl_asin = tbl_relabd . transform ( arcsint , inplace = False ) return tbl_asin'], ['parse sam file and check mapping quality', \"def parse_sam ( sam , qual ) : for line in sam : if line . startswith ( '@' ) : continue line = line . strip ( ) . split ( ) if int ( line [ 4 ] ) == 0 or int ( line [ 4 ] ) < qual : continue yield line\"], ['reverse completement stats', \"def rc_stats ( stats ) : rc_nucs = { 'A' : 'T' , 'T' : 'A' , 'G' : 'C' , 'C' : 'G' , 'N' : 'N' } rcs = [ ] for pos in reversed ( stats ) : rc = { } rc [ 'reference frequencey' ] = pos [ 'reference frequency' ] rc [ 'consensus frequencey' ] = pos [ 'consensus frequency' ] rc [ 'In' ] = pos [ 'In' ] rc [ 'Del' ] = pos [ 'Del' ] rc [ 'ref' ] = rc_nucs [ pos [ 'ref' ] ] rc [ 'consensus' ] = ( rc_nucs [ pos [ 'consensus' ] [ 0 ] ] , pos [ 'consensus' ] [ 1 ] ) for base , stat in list ( pos . items ( ) ) : if base in rc_nucs : rc [ rc_nucs [ base ] ] = stat rcs . append ( rc ) return rcs\"], ['parse codon nucleotide positions in range start - > end wrt strand', 'def parse_codons ( ref , start , end , strand ) : codon = [ ] c = cycle ( [ 1 , 2 , 3 ] ) ref = ref [ start - 1 : end ] if strand == - 1 : ref = rc_stats ( ref ) for pos in ref : n = next ( c ) codon . append ( pos ) if n == 3 : yield codon codon = [ ]'], ['calculate coverage for positions in range start - > end', 'def calc_coverage ( ref , start , end , length , nucs ) : ref = ref [ start - 1 : end ] bases = 0 for pos in ref : for base , count in list ( pos . items ( ) ) : if base in nucs : bases += count return float ( bases ) / float ( length )'], ['parse gbk file', \"def parse_gbk ( gbks ) : for gbk in gbks : for record in SeqIO . parse ( open ( gbk ) , 'genbank' ) : for feature in record . features : if feature . type == 'gene' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : continue if feature . type == 'CDS' : try : locus = feature . qualifiers [ 'locus_tag' ] [ 0 ] except : pass start = int ( feature . location . start ) + int ( feature . qualifiers [ 'codon_start' ] [ 0 ] ) end , strand = int ( feature . location . end ) , feature . location . strand if strand is None : strand = 1 else : strand = - 1 contig = record . id yield contig , [ locus , [ start , end , strand ] , feature . qualifiers ]\"], ['parse gene call information from Prodigal fasta output', \"def parse_fasta_annotations ( fastas , annot_tables , trans_table ) : if annot_tables is not False : annots = { } for table in annot_tables : for cds in open ( table ) : ID , start , end , strand = cds . strip ( ) . split ( ) annots [ ID ] = [ start , end , int ( strand ) ] for fasta in fastas : for seq in parse_fasta ( fasta ) : if ( '# ;gc_cont' not in seq [ 0 ] and '# ID=' not in seq [ 0 ] ) and annot_tables is False : print ( '# specify fasta from Prodigal or annotations table (-t)' , file = sys . stderr ) exit ( ) if 'ID=' in seq [ 0 ] : ID = seq [ 0 ] . rsplit ( 'ID=' , 1 ) [ 1 ] . split ( ';' , 1 ) [ 0 ] contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_%s' % ( ID ) , 1 ) [ 0 ] else : contig = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] . rsplit ( '_' , 1 ) [ 0 ] locus = seq [ 0 ] . split ( ) [ 0 ] . split ( '>' ) [ 1 ] if ( '# ;gc_cont' in seq [ 0 ] or '# ID=' in seq [ 0 ] ) : info = seq [ 0 ] . split ( ' # ' ) start , end , strand = int ( info [ 1 ] ) , int ( info [ 2 ] ) , info [ 3 ] if strand == '1' : strand = 1 else : strand = - 1 product = [ '' . join ( info [ 4 ] . split ( ) [ 1 : ] ) ] else : start , end , strand = annots [ locus ] product = seq [ 0 ] . split ( ' ' , 1 ) [ 1 ] info = { 'transl_table' : [ trans_table ] , 'translation' : [ seq [ 1 ] ] , 'product' : product } yield contig , [ locus , [ start , end , strand ] , info ]\"], ['parse annotations in either gbk or Prodigal fasta format', 'def parse_annotations ( annots , fmt , annot_tables , trans_table ) : annotations = { } if fmt is False : for contig , feature in parse_gbk ( annots ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) else : for contig , feature in parse_fasta_annotations ( annots , annot_tables , trans_table ) : if contig not in annotations : annotations [ contig ] = [ ] annotations [ contig ] . append ( feature ) return annotations'], ['convert codon to amino acid', \"def codon2aa ( codon , trans_table ) : return Seq ( '' . join ( codon ) , IUPAC . ambiguous_dna ) . translate ( table = trans_table ) [ 0 ]\"], ['find consensus base based on nucleotide frequencies', \"def find_consensus ( bases ) : nucs = [ 'A' , 'T' , 'G' , 'C' , 'N' ] total = sum ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) try : top = max ( [ bases [ nuc ] for nuc in nucs if nuc in bases ] ) except : bases [ 'consensus' ] = ( 'N' , 'n/a' ) bases [ 'consensus frequency' ] = 'n/a' bases [ 'reference frequency' ] = 'n/a' return bases top = [ ( nuc , bases [ nuc ] ) for nuc in bases if bases [ nuc ] == top ] if top [ 0 ] [ 1 ] == 0 : bases [ 'consensus' ] = ( 'n/a' , 0 ) else : bases [ 'consensus' ] = random . choice ( top ) if total == 0 : c_freq = 'n/a' ref_freq = 'n/a' else : c_freq = float ( bases [ 'consensus' ] [ 1 ] ) / float ( total ) if bases [ 'ref' ] not in bases : ref_freq = 0 else : ref_freq = float ( bases [ bases [ 'ref' ] ] ) / float ( total ) bases [ 'consensus frequency' ] = c_freq bases [ 'reference frequency' ] = ref_freq return bases\"], ['print consensensus sequences for each genome and sample', \"def print_consensus ( genomes ) : cons = { } for genome , contigs in list ( genomes . items ( ) ) : cons [ genome ] = { } for contig , samples in list ( contigs . items ( ) ) : for sample , stats in list ( samples . items ( ) ) : if sample not in cons [ genome ] : cons [ genome ] [ sample ] = { } seq = cons [ genome ] [ sample ] [ contig ] = [ ] for pos , ps in enumerate ( stats [ 'bp_stats' ] , 1 ) : ref , consensus = ps [ 'ref' ] , ps [ 'consensus' ] [ 0 ] if consensus == 'n/a' : consensus = ref . lower ( ) seq . append ( consensus ) for genome , samples in cons . items ( ) : for sample , contigs in samples . items ( ) : fn = '%s.%s.consensus.fa' % ( genome , sample ) f = open ( fn , 'w' ) for contig , seq in contigs . items ( ) : print ( '>%s' % ( contig ) , file = f ) print ( '' . join ( seq ) , file = f ) f . close ( ) return cons\"], ['calculate genome coverage from scaffold coverage table', \"def parse_cov ( cov_table , scaffold2genome ) : size = { } mapped = { } for line in open ( cov_table ) : line = line . strip ( ) . split ( '\\\\t' ) if line [ 0 ] . startswith ( '#' ) : samples = line [ 1 : ] samples = [ i . rsplit ( '/' , 1 ) [ - 1 ] . split ( '.' , 1 ) [ 0 ] for i in samples ] continue scaffold , length = line [ 0 ] . split ( ': ' ) length = float ( length ) covs = [ float ( i ) for i in line [ 1 : ] ] bases = [ c * length for c in covs ] if scaffold not in scaffold2genome : continue genome = scaffold2genome [ scaffold ] if genome not in size : size [ genome ] = 0 mapped [ genome ] = { sample : 0 for sample in samples } size [ genome ] += length for sample , count in zip ( samples , bases ) : mapped [ genome ] [ sample ] += count coverage = { 'genome' : [ ] , 'genome size (bp)' : [ ] , 'sample' : [ ] , 'coverage' : [ ] } for genome , length in size . items ( ) : for sample in samples : cov = mapped [ genome ] [ sample ] / length coverage [ 'genome' ] . append ( genome ) coverage [ 'genome size (bp)' ] . append ( length ) coverage [ 'sample' ] . append ( sample ) coverage [ 'coverage' ] . append ( cov ) return pd . DataFrame ( coverage )\"], ['calculate genome coverage from scaffold coverage', 'def genome_coverage ( covs , s2b ) : COV = [ ] for cov in covs : COV . append ( parse_cov ( cov , s2b ) ) return pd . concat ( COV )'], ['convert s2b files to dictionary', \"def parse_s2bs ( s2bs ) : s2b = { } for s in s2bs : for line in open ( s ) : line = line . strip ( ) . split ( '\\\\t' ) s , b = line [ 0 ] , line [ 1 ] s2b [ s ] = b return s2b\"], ['convert fastas to s2b dictionary', \"def fa2s2b ( fastas ) : s2b = { } for fa in fastas : for seq in parse_fasta ( fa ) : s = seq [ 0 ] . split ( '>' , 1 ) [ 1 ] . split ( ) [ 0 ] s2b [ s ] = fa . rsplit ( '/' , 1 ) [ - 1 ] . rsplit ( '.' , 1 ) [ 0 ] return s2b\"], ['Filters out sequences with too much ambiguity as defined by the method parameters .', \"def filter_ambiguity ( records , percent = 0.5 ) : seqs = [ ] count = 0 for record in records : if record . seq . count ( 'N' ) / float ( len ( record ) ) < percent : seqs . append ( record ) count += 1 return seqs , count\"], ['Search package .', 'def package_existent ( name ) : try : response = requests . get ( PYPI_URL . format ( name ) ) if response . ok : msg = ( \\'[error] \"{0}\" is registered already in PyPI.\\\\n\\' \\'\\\\tSpecify another package name.\\' ) . format ( name ) raise Conflict ( msg ) except ( socket . gaierror , Timeout , ConnectionError , HTTPError ) as exc : raise BackendFailure ( exc )'], ['add index to id to make it unique wrt ids', \"def append_index_id ( id , ids ) : index = 1 mod = '%s_%s' % ( id , index ) while mod in ids : index += 1 mod = '%s_%s' % ( id , index ) ids . append ( mod ) return mod , ids\"], ['de - replicate fastas based on sequence names', \"def de_rep ( fastas , append_index , return_original = False ) : ids = [ ] for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ) id = header [ 0 ] if id not in ids : ids . append ( id ) if return_original is True : yield [ header , seq ] else : yield seq elif append_index == True : new , ids = append_index_id ( id , ids ) if return_original is True : yield [ header , [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ] ] else : yield [ '>%s %s' % ( new , ' ' . join ( header [ 1 : : ] ) ) , seq [ 1 ] ]\"], ['Request data associated with postcode .', \"def get ( postcode ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) url = '%s/postcode/%s.json' % ( END_POINT , postcode ) return _get_json_resp ( url )\"], ['Request all postcode data within distance miles of postcode .', \"def get_from_postcode ( postcode , distance ) : postcode = quote ( postcode . replace ( ' ' , '' ) ) return _get_from ( distance , 'postcode=%s' % postcode )\"], ['Checks if latitude and longitude correct', 'def _check_point ( self , lat , lng ) : if abs ( lat ) > 90 or abs ( lng ) > 180 : msg = \"Illegal lat and/or lng, (%s, %s) provided.\" % ( lat , lng ) raise IllegalPointException ( msg )'], ['Checks for cached responses before requesting from web - service', 'def _lookup ( self , skip_cache , fun , * args , ** kwargs ) : if args not in self . cache or skip_cache : self . cache [ args ] = fun ( * args , ** kwargs ) return self . cache [ args ]'], ['Calls postcodes . get_nearest but checks correctness of lat and long and by default utilises a local cache .', 'def get_nearest ( self , lat , lng , skip_cache = False ) : lat , lng = float ( lat ) , float ( lng ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_nearest , lat , lng )'], ['Calls postcodes . get_from_postcode but checks correctness of distance and by default utilises a local cache .', 'def get_from_postcode ( self , postcode , distance , skip_cache = False ) : distance = float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) postcode = postcode . lower ( ) . replace ( \\' \\' , \\'\\' ) return self . _lookup ( skip_cache , get_from_postcode , postcode , float ( distance ) )'], ['Calls postcodes . get_from_geo but checks the correctness of all arguments and by default utilises a local cache .', 'def get_from_geo ( self , lat , lng , distance , skip_cache = False ) : lat , lng , distance = float ( lat ) , float ( lng ) , float ( distance ) if distance < 0 : raise IllegalDistanceException ( \"Distance must not be negative\" ) self . _check_point ( lat , lng ) return self . _lookup ( skip_cache , get_from_geo , lat , lng , distance )'], ['get coordinates of insertions from insertion - masked sequence', 'def insertions_from_masked ( seq ) : insertions = [ ] prev = True for i , base in enumerate ( seq ) : if base . isupper ( ) and prev is True : insertions . append ( [ ] ) prev = False elif base . islower ( ) : insertions [ - 1 ] . append ( i ) prev = True return [ [ min ( i ) , max ( i ) ] for i in insertions if i != [ ] ]'], ['get insertion information from header', \"def seq_info ( names , id2names , insertions , sequences ) : seqs = { } for name in names : id = id2names [ name ] gene = name . split ( 'fromHMM::' , 1 ) [ 0 ] . rsplit ( ' ' , 1 ) [ 1 ] model = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( '=' , 1 ) [ 1 ] . split ( ) [ 0 ] i_gene_pos = insertions [ id ] i_model_pos = name . split ( 'fromHMM::' , 1 ) [ 1 ] . split ( 'model-pos(ins-len)=' ) [ 1 ] . split ( ) [ 0 ] . split ( ';' ) i_info = [ ] for i , ins in enumerate ( i_gene_pos ) : model_pos = i_model_pos [ i ] . split ( '-' ) [ 1 ] . split ( '(' ) [ 0 ] length = i_model_pos [ i ] . split ( '(' ) [ 1 ] . split ( ')' ) [ 0 ] iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s' % ( id , ( i + 1 ) , ( i + 1 ) , ins [ 0 ] , ins [ 1 ] , model_pos ) iseq = sequences [ id ] [ 1 ] [ ins [ 0 ] : ( ins [ 1 ] + 1 ) ] iseq = [ iheader , iseq ] info = [ ins , model_pos , length , iseq , [ ] , [ ] ] i_info . append ( info ) seqs [ id ] = [ gene , model , i_info ] return seqs\"], ['make sure thresh % feature is contained within insertion', 'def check_overlap ( pos , ins , thresh ) : ins_pos = ins [ 0 ] ins_len = ins [ 2 ] ol = overlap ( ins_pos , pos ) feat_len = pos [ 1 ] - pos [ 0 ] + 1 if float ( ol ) / float ( feat_len ) >= thresh : return True return False'], ['length of largest insertion', 'def max_insertion ( seqs , gene , domain ) : seqs = [ i [ 2 ] for i in list ( seqs . values ( ) ) if i [ 2 ] != [ ] and i [ 0 ] == gene and i [ 1 ] == domain ] lengths = [ ] for seq in seqs : for ins in seq : lengths . append ( int ( ins [ 2 ] ) ) if lengths == [ ] : return 100 return max ( lengths )'], ['get length of model', \"def model_length ( gene , domain ) : if gene == '16S' : domain2max = { 'E_coli_K12' : int ( 1538 ) , 'bacteria' : int ( 1689 ) , 'archaea' : int ( 1563 ) , 'eukarya' : int ( 2652 ) } return domain2max [ domain ] elif gene == '23S' : domain2max = { 'E_coli_K12' : int ( 2903 ) , 'bacteria' : int ( 3146 ) , 'archaea' : int ( 3774 ) , 'eukarya' : int ( 9079 ) } return domain2max [ domain ] else : print ( sys . stderr , '# length unknown for gene: %s, domain: %s' % ( gene , domain ) ) exit ( )\"], ['setup unique marker for every orf annotation - change size if necessary', \"def setup_markers ( seqs ) : family2marker = { } markers = cycle ( [ '^' , 'p' , '*' , '+' , 'x' , 'd' , '|' , 'v' , '>' , '<' , '8' ] ) size = 60 families = [ ] for seq in list ( seqs . values ( ) ) : for insertion in seq [ 2 ] : for family in list ( insertion [ - 1 ] . values ( ) ) : if family not in families : families . append ( family ) for family in families : marker = next ( markers ) if marker == '^' : size = size * 0.5 family2marker [ family ] = [ marker , size ] return family2marker\"], ['plot insertions for each gene and domain', 'def plot_by_gene_and_domain ( name , seqs , tax , id2name ) : for gene in set ( [ seq [ 0 ] for seq in list ( seqs . values ( ) ) ] ) : for domain in set ( [ seq [ 1 ] for seq in list ( seqs . values ( ) ) ] ) : plot_insertions ( name , seqs , gene , domain , tax , id2name )'], ['get the description for each ORF', \"def get_descriptions ( fastas ) : id2desc = { } for fasta in fastas : for seq in parse_fasta ( fasta ) : header = seq [ 0 ] . split ( '>' ) [ 1 ] . split ( ' ' ) id = header [ 0 ] if len ( header ) > 1 : desc = ' ' . join ( header [ 1 : ] ) else : desc = 'n/a' length = float ( len ( [ i for i in seq [ 1 ] . strip ( ) if i != '*' ] ) ) id2desc [ id ] = [ fasta , desc , length ] return id2desc\"], ['optimize later? slow ... should combine with calculate_threshold module', \"def print_genome_matrix ( hits , fastas , id2desc , file_name ) : out = open ( file_name , 'w' ) fastas = sorted ( fastas ) print ( '## percent identity between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : average = '-' else : average = numpy . average ( [ hits [ fasta ] [ other ] [ i ] [ 3 ] for i in hits [ fasta ] [ other ] ] ) line . append ( str ( average ) ) print ( '\\\\t' . join ( line ) , file = out ) print ( '' , file = out ) print ( '## percent of orfs that are orthologous between genomes' , file = out ) print ( '# - \\\\t %s' % ( '\\\\t' . join ( fastas ) ) , file = out ) for fasta in fastas : line = [ fasta ] for other in fastas : if other == fasta : percent = '-' else : orthologs = float ( len ( hits [ fasta ] [ other ] ) ) orfs = float ( len ( [ i for i in id2desc if id2desc [ i ] [ 0 ] == fasta ] ) ) percent = float ( orthologs / orfs ) * 100 line . append ( str ( percent ) ) print ( '\\\\t' . join ( line ) , file = out )\"], ['compare genome to self to get the best possible bit score for each ORF', \"def self_compare ( fastas , id2desc , algorithm ) : for fasta in fastas : blast = open ( search ( fasta , fasta , method = algorithm , alignment = 'local' ) ) for hit in best_blast ( blast , 1 ) : id , bit = hit [ 0 ] . split ( ) [ 0 ] , float ( hit [ - 1 ] ) id2desc [ id ] . append ( bit ) return id2desc\"], ['if thresholds are not specififed calculate based on the distribution of normalized bit scores', \"def calc_thresholds ( rbh , file_name , thresholds = [ False , False , False , False ] , stdevs = 2 ) : calc_threshold = thresholds [ - 1 ] norm_threshold = { } for pair in itertools . permutations ( [ i for i in rbh ] , 2 ) : if pair [ 0 ] not in norm_threshold : norm_threshold [ pair [ 0 ] ] = { } norm_threshold [ pair [ 0 ] ] [ pair [ 1 ] ] = { } out = open ( file_name , 'w' ) print ( '#### summary of rbh comparisons\\\\n' , file = out ) comparisons = [ ] for genome in rbh : for compare in rbh [ genome ] : pair = '' . join ( sorted ( [ genome , compare ] ) ) if pair in comparisons : continue comparisons . append ( pair ) scores = { 'percent identity' : [ ] , 'e-value' : [ ] , 'bit score' : [ ] , 'normalized bit score' : [ ] , 'alignment length fraction' : [ ] } print ( '### blast between %s and %s\\\\n' % ( genome , compare ) , file = out ) for id in rbh [ genome ] [ compare ] : pident , length_fraction , e , bit , norm_bit = rbh [ genome ] [ compare ] [ id ] [ 3 : ] scores [ 'percent identity' ] . append ( pident ) scores [ 'alignment length fraction' ] . append ( length_fraction ) scores [ 'e-value' ] . append ( e ) scores [ 'bit score' ] . append ( bit ) scores [ 'normalized bit score' ] . append ( norm_bit ) if calc_threshold is True : norms = scores [ 'normalized bit score' ] average = numpy . average ( norms ) std = numpy . std ( norms ) normal_thresh = average - ( std * stdevs ) print ( '## average normalized bit score: %s' % average , file = out ) print ( '## standard deviation of normalized bit scores: %s' % std , file = out ) print ( '## normalized bit score threshold set to: %s\\\\n' % ( normal_thresh ) , file = out ) norm_threshold [ genome ] [ compare ] , norm_threshold [ compare ] [ genome ] = normal_thresh , normal_thresh for score in scores : print ( '## %s' % ( score ) , file = out ) if len ( scores [ score ] ) > 0 : print ( '## average: %s' % numpy . average ( scores [ score ] ) , file = out ) print ( '' , file = out ) out . close ( ) if calc_threshold is True : return thresholds [ 0 : - 1 ] + [ norm_threshold ] else : return thresholds\"], ['make and split a rbh network', \"def neto ( fastas , algorithm = 'usearch' , e = 0.01 , bit = 40 , length = .65 , norm_bit = False ) : thresholds = [ e , bit , length , norm_bit ] id2desc = get_descriptions ( fastas ) id2desc = self_compare ( fastas , id2desc , algorithm ) hits = compare_genomes ( fastas , id2desc , algorithm ) calc_thresholds ( hits , file_name = 'fbh.scores.summary.txt' ) rbh_network ( id2desc , hits , file_name = 'fbh.network.edges.txt' ) hits , rbh = find_rbh ( hits , id2desc ) thresholds = calc_thresholds ( rbh , 'rbh.scores.summary.txt' , thresholds ) g = rbh_network ( id2desc , rbh , file_name = 'rbh.network.edges.txt' ) filtered_g , filtered_rbh = rbh_network ( id2desc , rbh , 'rbh.filtered.network.edges.txt' , thresholds ) calc_thresholds ( filtered_rbh , file_name = 'rbh.filtered.scores.summary.txt' ) print_summary ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.nodes.txt' ) print_network_matrix ( filtered_g , fastas , id2desc , file_name = 'rbh.filtered.network.matrix.txt' ) print_genome_matrix ( filtered_rbh , fastas , id2desc , file_name = 'rbh.filtered.network.genome_matrix.txt' ) split_g = split_network ( filtered_g , id2desc , file_name = 'rbh.filtered.split.network.edges.txt' ) print_summary ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.nodes.txt' ) print_network_matrix ( split_g , fastas , id2desc , file_name = 'rbh.filtered.split.network.matrix.txt' ) return split_g\"], ['Collapses multiple dimensions into a single raster_info complex struct', \"def _parse_raster_info ( self , prop = RASTER_INFO ) : raster_info = { } . fromkeys ( _iso_definitions [ prop ] , u'' ) raster_info [ 'dimensions' ] = get_default_for_complex_sub ( prop = prop , subprop = 'dimensions' , value = parse_property ( self . _xml_tree , None , self . _data_map , '_ri_num_dims' ) , xpath = self . _data_map [ '_ri_num_dims' ] ) xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] for dimension in parse_complex_list ( self . _xml_tree , xpath_root , xpath_map , RASTER_DIMS ) : dimension_type = dimension [ 'type' ] . lower ( ) if dimension_type == 'vertical' : raster_info [ 'vertical_count' ] = dimension [ 'size' ] elif dimension_type == 'column' : raster_info [ 'column_count' ] = dimension [ 'size' ] raster_info [ 'x_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) elif dimension_type == 'row' : raster_info [ 'row_count' ] = dimension [ 'size' ] raster_info [ 'y_resolution' ] = u' ' . join ( dimension [ k ] for k in [ 'value' , 'units' ] ) . strip ( ) return raster_info if any ( raster_info [ k ] for k in raster_info ) else { }\"], ['Derives multiple dimensions from a single raster_info complex struct', \"def _update_raster_info ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = update_props . pop ( 'values' ) xroot , xpath = None , self . _data_map [ '_ri_num_dims' ] raster_info = [ update_property ( tree_to_update , xroot , xpath , prop , values . get ( 'dimensions' , u'' ) ) ] xpath_root = self . _get_xroot_for ( prop ) xpath_map = self . _data_structures [ prop ] v_dimension = { } if values . get ( 'vertical_count' ) : v_dimension = v_dimension . fromkeys ( xpath_map , u'' ) v_dimension [ 'type' ] = 'vertical' v_dimension [ 'size' ] = values . get ( 'vertical_count' , u'' ) x_dimension = { } if values . get ( 'column_count' ) or values . get ( 'x_resolution' ) : x_dimension = x_dimension . fromkeys ( xpath_map , u'' ) x_dimension [ 'type' ] = 'column' x_dimension [ 'size' ] = values . get ( 'column_count' , u'' ) x_dimension [ 'value' ] = values . get ( 'x_resolution' , u'' ) y_dimension = { } if values . get ( 'row_count' ) or values . get ( 'y_resolution' ) : y_dimension = y_dimension . fromkeys ( xpath_map , u'' ) y_dimension [ 'type' ] = 'row' y_dimension [ 'size' ] = values . get ( 'row_count' , u'' ) y_dimension [ 'value' ] = values . get ( 'y_resolution' , u'' ) update_props [ 'prop' ] = RASTER_DIMS update_props [ 'values' ] = [ v_dimension , x_dimension , y_dimension ] raster_info += update_complex_list ( xpath_root = xpath_root , xpath_map = xpath_map , ** update_props ) return raster_info\"], ['Removes primitive type tags from an XPATH', 'def _trim_xpath ( self , xpath , prop ) : xroot = self . _get_xroot_for ( prop ) if xroot is None and isinstance ( xpath , string_types ) : xtags = xpath . split ( XPATH_DELIM ) if xtags [ - 1 ] in _iso_tag_primitives : xroot = XPATH_DELIM . join ( xtags [ : - 1 ] ) return xroot'], ['Generates the app id for a given shortcut . Steam uses app ids as a unique identifier for games but since shortcuts dont have a canonical serverside representation they need to be generated on the fly . The important part about this function is that it will generate the same app id as Steam does for a given shortcut', \"def shortcut_app_id ( shortcut ) : algorithm = Crc ( width = 32 , poly = 0x04C11DB7 , reflect_in = True , xor_in = 0xffffffff , reflect_out = True , xor_out = 0xffffffff ) crc_input = '' . join ( [ shortcut . exe , shortcut . name ] ) high_32 = algorithm . bit_by_bit ( crc_input ) | 0x80000000 full_64 = ( high_32 << 32 ) | 0x02000000 return str ( full_64 )\"], ['Execute git config .', \"def _config ( self ) : cfg_wr = self . repo . config_writer ( ) cfg_wr . add_section ( 'user' ) cfg_wr . set_value ( 'user' , 'name' , self . metadata . author ) cfg_wr . set_value ( 'user' , 'email' , self . metadata . email ) cfg_wr . release ( )\"], ['Execute git remote add .', \"def _remote_add ( self ) : self . repo . create_remote ( 'origin' , 'git@github.com:{username}/{repo}.git' . format ( username = self . metadata . username , repo = self . metadata . name ) )\"], ['Starts execution of the script', 'def start ( self ) : try : self . args . func ( ) except SystemExit as e : if e . code != 0 : raise except KeyboardInterrupt : self . log . warning ( \"exited via keyboard interrupt\" ) except : self . log . exception ( \"exited start function\" ) finally : self . _flush_metrics_q . put ( None , block = True ) self . _flush_metrics_q . put ( None , block = True , timeout = 1 ) self . log . debug ( \"exited_successfully\" )'], ['Define basic command - line arguments required by the script .', 'def define_baseargs ( self , parser ) : parser . add_argument ( \\'--name\\' , default = sys . argv [ 0 ] , help = \\'Name to identify this instance\\' ) parser . add_argument ( \\'--log-level\\' , default = None , help = \\'Logging level as picked from the logging module\\' ) parser . add_argument ( \\'--log-format\\' , default = None , choices = ( \"json\" , \"pretty\" , ) , help = ( \"Force the format of the logs. By default, if the \" \"command is from a terminal, print colorful logs. \" \"Otherwise print json.\" ) , ) parser . add_argument ( \\'--log-file\\' , default = None , help = \\'Writes logs to log file if specified, default: %(default)s\\' , ) parser . add_argument ( \\'--quiet\\' , default = False , action = \"store_true\" , help = \\'if true, does not print logs to stderr, default: %(default)s\\' , ) parser . add_argument ( \\'--metric-grouping-interval\\' , default = None , type = int , help = \\'To group metrics based on time interval ex:10 i.e;(10 sec)\\' , ) parser . add_argument ( \\'--debug\\' , default = False , action = \"store_true\" , help = \\'To run the code in debug mode\\' , )'], ['Basically turns payload that looks like \\\\\\\\ n to . In the calling function if this function returns no object is added for that payload .', \"def cleanup_payload ( self , payload ) : p = payload . replace ( '\\\\n' , '' ) p = p . rstrip ( ) p = p . lstrip ( ) return p\"], ['Ensures complex property types have the correct default values', \"def get_default_for ( prop , value ) : prop = prop . strip ( '_' ) val = reduce_value ( value ) if prop in _COMPLEX_LISTS : return wrap_value ( val ) elif prop in _COMPLEX_STRUCTS : return val or { } else : return u'' if val is None else val\"], ['Either update the tree the default way or call the custom updater', \"def update_property ( tree_to_update , xpath_root , xpaths , prop , values , supported = None ) : if supported and prop . startswith ( '_' ) and prop . strip ( '_' ) in supported : values = u'' else : values = get_default_for ( prop , values ) if not xpaths : return [ ] elif not isinstance ( xpaths , ParserProperty ) : return _update_property ( tree_to_update , xpath_root , xpaths , values ) else : return xpaths . set_prop ( tree_to_update = tree_to_update , prop = prop , values = values )\"], ['Default update operation for a single parser property . If xpaths contains one xpath then one element per value will be inserted at that location in the tree_to_update ; otherwise the number of values must match the number of xpaths .', \"def _update_property ( tree_to_update , xpath_root , xpaths , values ) : def update_element ( elem , idx , root , path , vals ) : has_root = bool ( root and len ( path ) > len ( root ) and path . startswith ( root ) ) path , attr = get_xpath_tuple ( path ) if attr : removed = [ get_element ( elem , path ) ] remove_element_attributes ( removed [ 0 ] , attr ) elif not has_root : removed = wrap_value ( remove_element ( elem , path ) ) else : path = get_xpath_branch ( root , path ) removed = [ ] if idx != 0 else [ remove_element ( e , path , True ) for e in get_elements ( elem , root ) ] if not vals : return removed items = [ ] for i , val in enumerate ( wrap_value ( vals ) ) : elem_to_update = elem if has_root : elem_to_update = insert_element ( elem , ( i + idx ) , root ) val = val . decode ( 'utf-8' ) if not isinstance ( val , string_types ) else val if not attr : items . append ( insert_element ( elem_to_update , i , path , val ) ) elif path : items . append ( insert_element ( elem_to_update , i , path , ** { attr : val } ) ) else : set_element_attributes ( elem_to_update , ** { attr : val } ) items . append ( elem_to_update ) return items xpaths = reduce_value ( xpaths ) values = filter_empty ( values ) if isinstance ( xpaths , string_types ) : return update_element ( tree_to_update , 0 , xpath_root , xpaths , values ) else : each = [ ] for index , xpath in enumerate ( xpaths ) : value = values [ index ] if values else None each . extend ( update_element ( tree_to_update , index , xpath_root , xpath , value ) ) return each\"], ['Default validation for single complex data structure', \"def validate_complex ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for complex_prop , complex_val in iteritems ( value ) : complex_key = '.' . join ( ( prop , complex_prop ) ) if complex_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) validate_type ( complex_key , complex_val , ( string_types , list ) )\"], ['Default validation for Attribute Details data structure', \"def validate_complex_list ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = { } if xpath_map is None else xpath_map for idx , complex_struct in enumerate ( wrap_value ( value ) ) : cs_idx = prop + '[' + str ( idx ) + ']' validate_type ( cs_idx , complex_struct , dict ) for cs_prop , cs_val in iteritems ( complex_struct ) : cs_key = '.' . join ( ( cs_idx , cs_prop ) ) if cs_prop not in complex_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) if not isinstance ( cs_val , list ) : validate_type ( cs_key , cs_val , ( string_types , list ) ) else : for list_idx , list_val in enumerate ( cs_val ) : list_prop = cs_key + '[' + str ( list_idx ) + ']' validate_type ( list_prop , list_val , string_types )\"], ['Default validation for Date Types data structure', \"def validate_dates ( prop , value , xpath_map = None ) : if value is not None : validate_type ( prop , value , dict ) date_keys = set ( value ) if date_keys : if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys : if prop in _complex_definitions : complex_keys = _complex_definitions [ prop ] else : complex_keys = _complex_definitions [ DATES ] if xpath_map is None else xpath_map _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( complex_keys ) ) ) ) date_type = value [ DATE_TYPE ] if date_type not in DATE_TYPES : _validation_error ( 'dates.type' , None , date_type , DATE_TYPES ) date_vals = value [ DATE_VALUES ] validate_type ( 'dates.values' , date_vals , list ) dates_len = len ( date_vals ) if date_type == DATE_TYPE_MISSING and dates_len != 0 : _validation_error ( 'len(dates.values)' , None , dates_len , 0 ) if date_type == DATE_TYPE_SINGLE and dates_len != 1 : _validation_error ( 'len(dates.values)' , None , dates_len , 1 ) if date_type == DATE_TYPE_RANGE and dates_len != 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 2 ) if date_type == DATE_TYPE_MULTIPLE and dates_len < 2 : _validation_error ( 'len(dates.values)' , None , dates_len , 'at least two' ) for idx , date in enumerate ( date_vals ) : date_key = 'dates.value[' + str ( idx ) + ']' validate_type ( date_key , date , string_types )\"], ['Default validation for Process Steps data structure', \"def validate_process_steps ( prop , value ) : if value is not None : validate_type ( prop , value , ( dict , list ) ) procstep_keys = set ( _complex_definitions [ prop ] ) for idx , procstep in enumerate ( wrap_value ( value ) ) : ps_idx = prop + '[' + str ( idx ) + ']' validate_type ( ps_idx , procstep , dict ) for ps_prop , ps_val in iteritems ( procstep ) : ps_key = '.' . join ( ( ps_idx , ps_prop ) ) if ps_prop not in procstep_keys : _validation_error ( prop , None , value , ( 'keys: {0}' . format ( ',' . join ( procstep_keys ) ) ) ) if ps_prop != 'sources' : validate_type ( ps_key , ps_val , string_types ) else : validate_type ( ps_key , ps_val , ( string_types , list ) ) for src_idx , src_val in enumerate ( wrap_value ( ps_val ) ) : src_key = ps_key + '[' + str ( src_idx ) + ']' validate_type ( src_key , src_val , string_types )\"], ['Default validation for all types', 'def validate_type ( prop , value , expected ) : if value is not None and not isinstance ( value , expected ) : _validation_error ( prop , type ( value ) . __name__ , None , expected )'], ['Default validation for updated properties', \"def _validation_error ( prop , prop_type , prop_value , expected ) : if prop_type is None : attrib = 'value' assigned = prop_value else : attrib = 'type' assigned = prop_type raise ValidationError ( 'Invalid property {attrib} for {prop}:\\\\n\\\\t{attrib}: {assigned}\\\\n\\\\texpected: {expected}' , attrib = attrib , prop = prop , assigned = assigned , expected = expected , invalid = { prop : prop_value } if attrib == 'value' else { } )\"], ['Calls the getter with no arguments and returns its value', 'def get_prop ( self , prop ) : if self . _parser is None : raise ConfigurationError ( \\'Cannot call ParserProperty.\"get_prop\" with no parser configured\\' ) return self . _parser ( prop ) if prop else self . _parser ( )'], ['Returns a boolean representing whether these commands can be grouped together or not .', \"def can_group_commands ( command , next_command ) : multi_capable_commands = ( 'get' , 'set' , 'delete' ) if next_command is None : return False name = command . get_name ( ) if name not in multi_capable_commands : return False if name != next_command . get_name ( ) : return False if grouped_args_for_command ( command ) != grouped_args_for_command ( next_command ) : return False if command . get_kwargs ( ) != next_command . get_kwargs ( ) : return False return True\"], ['define ribosomal proteins and location of curated databases', \"def find_databases ( databases ) : proteins = [ 'L15' , 'L18' , 'L6' , 'S8' , 'L5' , 'L24' , 'L14' , 'S17' , 'L16' , 'S3' , 'L22' , 'S19' , 'L2' , 'L4' , 'L3' , 'S10' ] protein_databases = { 'L14' : 'rpL14_JGI_MDM.filtered.faa' , 'L15' : 'rpL15_JGI_MDM.filtered.faa' , 'L16' : 'rpL16_JGI_MDM.filtered.faa' , 'L18' : 'rpL18_JGI_MDM.filtered.faa' , 'L22' : 'rpL22_JGI_MDM.filtered.faa' , 'L24' : 'rpL24_JGI_MDM.filtered.faa' , 'L2' : 'rpL2_JGI_MDM.filtered.faa' , 'L3' : 'rpL3_JGI_MDM.filtered.faa' , 'L4' : 'rpL4_JGI_MDM.filtered.faa' , 'L5' : 'rpL5_JGI_MDM.filtered.faa' , 'L6' : 'rpL6_JGI_MDM.filtered.faa' , 'S10' : 'rpS10_JGI_MDM.filtered.faa' , 'S17' : 'rpS17_JGI_MDM.filtered.faa' , 'S19' : 'rpS19_JGI_MDM.filtered.faa' , 'S3' : 'rpS3_JGI_MDM.filtered.faa' , 'S8' : 'rpS8_JGI_MDM.filtered.faa' } protein_databases = { key : '%s/%s' % ( databases , database ) for key , database in list ( protein_databases . items ( ) ) } return proteins , protein_databases\"], ['which protein has the best hit the one to the right or to the left?', 'def find_next ( start , stop , i2hits ) : if start not in i2hits and stop in i2hits : index = stop elif stop not in i2hits and start in i2hits : index = start elif start not in i2hits and stop not in i2hits : index = choice ( [ start , stop ] ) i2hits [ index ] = [ [ False ] ] else : A , B = i2hits [ start ] [ 0 ] , i2hits [ stop ] [ 0 ] if B [ 10 ] <= A [ 10 ] : index = stop else : index = start if index == start : nstart = start - 1 nstop = stop else : nstop = stop + 1 nstart = start match = i2hits [ index ] [ 0 ] rp = match [ - 1 ] return index , nstart , nstop , rp , match'], ['determine which hits represent real ribosomal proteins identify each in syntenic block max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold', 'def find_ribosomal ( rps , scaffolds , s2rp , min_hits , max_hits_rp , max_errors ) : for scaffold , proteins in list ( s2rp . items ( ) ) : hits = { p : [ i for i in sorted ( hits , key = itemgetter ( 10 ) ) ] [ 0 : max_hits_rp ] for p , hits in list ( proteins . items ( ) ) if len ( hits ) > 0 } if len ( hits ) < min_hits : continue best = sorted ( [ hit [ 0 ] + [ p ] for p , hit in list ( hits . items ( ) ) ] , key = itemgetter ( 10 ) ) [ 0 ] block = find_block ( rps , scaffolds [ scaffold ] , hits , best , max_errors ) if ( len ( block ) - 1 ) >= min_hits : yield scaffold , block'], ['Parse the rep set file and remove all sequences not associated with unique OTUs .', 'def filter_rep_set ( inF , otuSet ) : seqs = [ ] for record in SeqIO . parse ( inF , \"fasta\" ) : if record . id in otuSet : seqs . append ( record ) return seqs'], ['Update the text for each element at the configured path if attribute matches', \"def _update_report_item ( self , ** update_props ) : tree_to_update = update_props [ 'tree_to_update' ] prop = update_props [ 'prop' ] values = wrap_value ( update_props [ 'values' ] ) xroot = self . _get_xroot_for ( prop ) attr_key = 'type' attr_val = u'' if prop == 'attribute_accuracy' : attr_val = 'DQQuanAttAcc' elif prop == 'dataset_completeness' : attr_val = 'DQCompOm' for elem in get_elements ( tree_to_update , xroot ) : if get_element_attributes ( elem ) . get ( attr_key ) == attr_val : clear_element ( elem ) remove_empty_element ( tree_to_update , xroot ) attrs = { attr_key : attr_val } updated = [ ] for idx , value in enumerate ( values ) : elem = insert_element ( tree_to_update , idx , xroot , ** attrs ) updated . append ( insert_element ( elem , idx , 'measDesc' , value ) ) return updated\"], ['Clear the specified interrupt bit in the interrupt status register .', 'def _clear_interrupt ( self , intbit ) : int_status = self . _device . readU8 ( VCNL4010_INTSTAT ) int_status &= ~ intbit self . _device . write8 ( VCNL4010_INTSTAT , int_status )'], ['Swaps two nodes', 'def move ( self ) : a = random . randint ( 0 , len ( self . state ) - 1 ) b = random . randint ( 0 , len ( self . state ) - 1 ) self . state [ [ a , b ] ] = self . state [ [ b , a ] ]'], ['A bool - if the certificate should be self - signed .', 'def self_signed ( self , value ) : self . _self_signed = bool ( value ) if self . _self_signed : self . _issuer = None'], ['Grabs the first URL out of a asn1crypto . x509 . CRLDistributionPoints object', \"def _get_crl_url ( self , distribution_points ) : if distribution_points is None : return None for distribution_point in distribution_points : name = distribution_point [ 'distribution_point' ] if name . name == 'full_name' and name . chosen [ 0 ] . name == 'uniform_resource_identifier' : return name . chosen [ 0 ] . chosen . native return None\"], ['A bool - if the certificate should have the OCSP no check extension . Only applicable to certificates created for signing OCSP responses . Such certificates should normally be issued for a very short period of time since they are effectively whitelisted by clients .', 'def ocsp_no_check ( self , value ) : if value is None : self . _ocsp_no_check = None else : self . _ocsp_no_check = bool ( value )'], ['Removes empty line .', \"def emptylineless ( parser , token ) : nodelist = parser . parse ( ( 'endemptylineless' , ) ) parser . delete_first_token ( ) return EmptylinelessNode ( nodelist )\"], ['Do an HTTP PURGE of the given asset . The URL is run through urlparse and must point to the varnish instance not the varnishadm', \"def http_purge_url ( url ) : url = urlparse ( url ) connection = HTTPConnection ( url . hostname , url . port or 80 ) path = url . path or '/' connection . request ( 'PURGE' , '%s?%s' % ( path , url . query ) if url . query else path , '' , { 'Host' : '%s:%s' % ( url . hostname , url . port ) if url . port else url . hostname } ) response = connection . getresponse ( ) if response . status != 200 : logging . error ( 'Purge failed with status: %s' % response . status ) return response\"], ['Non - threaded batch command runner returning output results', \"def run ( addr , * commands , ** kwargs ) : results = [ ] handler = VarnishHandler ( addr , ** kwargs ) for cmd in commands : if isinstance ( cmd , tuple ) and len ( cmd ) > 1 : results . extend ( [ getattr ( handler , c [ 0 ] . replace ( '.' , '_' ) ) ( * c [ 1 : ] ) for c in cmd ] ) else : results . append ( getattr ( handler , cmd . replace ( '.' , '_' ) ) ( * commands [ 1 : ] ) ) break handler . close ( ) return results\"], ['add stylesheet files in HTML head', 'def add_stylesheets ( self , * css_files ) : for css_file in css_files : self . main_soup . style . append ( self . _text_file ( css_file ) )'], ['add javascripts files in HTML body', \"def add_javascripts ( self , * js_files ) : if self . main_soup . script is None : script_tag = self . main_soup . new_tag ( 'script' ) self . main_soup . body . append ( script_tag ) for js_file in js_files : self . main_soup . script . append ( self . _text_file ( js_file ) )\"], ['return the object in a file', \"def export ( self ) : with open ( self . export_url , 'w' , encoding = 'utf-8' ) as file : file . write ( self . build ( ) ) if self . open_browser : webbrowser . open_new_tab ( self . export_url )\"], ['convert Markdown text as html . return the html file as string', \"def build ( self ) : markdown_html = markdown . markdown ( self . markdown_text , extensions = [ TocExtension ( ) , 'fenced_code' , 'markdown_checklist.extension' , 'markdown.extensions.tables' ] ) markdown_soup = BeautifulSoup ( markdown_html , 'html.parser' ) if markdown_soup . find ( 'code' , attrs = { 'class' : 'mermaid' } ) : self . _add_mermaid_js ( ) for dot_tag in markdown_soup . find_all ( 'code' , attrs = { 'class' : 'dotgraph' } ) : grap_svg = self . _text_to_graphiz ( dot_tag . string ) graph_soup = BeautifulSoup ( grap_svg , 'html.parser' ) dot_tag . parent . replaceWith ( graph_soup ) self . main_soup . body . append ( markdown_soup ) return self . main_soup . prettify ( )\"], ['return the content of a file', \"def _text_file ( self , url ) : try : with open ( url , 'r' , encoding = 'utf-8' ) as file : return file . read ( ) except FileNotFoundError : print ( 'File `{}` not found' . format ( url ) ) sys . exit ( 0 )\"], ['create a graphviz graph from text', \"def _text_to_graphiz ( self , text ) : dot = Source ( text , format = 'svg' ) return dot . pipe ( ) . decode ( 'utf-8' )\"], ['add js libraries and css files of mermaid js_file', \"def _add_mermaid_js ( self ) : self . add_javascripts ( '{}/js/jquery-1.11.3.min.js' . format ( self . resources_path ) ) self . add_javascripts ( '{}/js/mermaid.min.js' . format ( self . resources_path ) ) self . add_stylesheets ( '{}/css/mermaid.css' . format ( self . resources_path ) ) self . main_soup . script . append ( 'mermaid.initialize({startOnLoad:true  });' )\"], ['Get a character set with individual members or ranges .', 'def getCharacterSet ( self ) : chars = u\\'\\' c = None cnt = 1 start = 0 while True : escaped_slash = False c = self . next ( ) if self . lookahead ( ) == u\\'-\\' and not c == u\\'\\\\\\\\\\' : f = c self . next ( ) c = self . next ( ) if not c or ( c in self . meta_chars ) : raise StringGenerator . SyntaxError ( u\"unexpected end of class range\" ) chars += self . getCharacterRange ( f , c ) elif c == u\\'\\\\\\\\\\' : if self . lookahead ( ) in self . meta_chars : c = self . next ( ) chars += c continue elif self . lookahead ( ) in self . string_code : c = self . next ( ) chars += self . string_code [ c ] elif c and c not in self . meta_chars : chars += c if c == u\\']\\' : if self . lookahead ( ) == u\\'{\\' : [ start , cnt ] = self . getQuantifier ( ) else : start = - 1 cnt = 1 break if c and c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped character in class definition: %s\" % c ) if not c : break return StringGenerator . CharacterSet ( chars , start , cnt )'], ['Get a sequence of non - special characters .', 'def getLiteral ( self ) : chars = u\\'\\' c = self . current ( ) while True : if c and c == u\"\\\\\\\\\" : c = self . next ( ) if c : chars += c continue elif not c or ( c in self . meta_chars ) : break else : chars += c if self . lookahead ( ) and self . lookahead ( ) in self . meta_chars : break c = self . next ( ) return StringGenerator . Literal ( chars )'], ['Get a sequence of nodes .', 'def getSequence ( self , level = 0 ) : seq = [ ] op = \\'\\' left_operand = None right_operand = None sequence_closed = False while True : c = self . next ( ) if not c : break if c and c not in self . meta_chars : seq . append ( self . getLiteral ( ) ) elif c and c == u\\'$\\' and self . lookahead ( ) == u\\'{\\' : seq . append ( self . getSource ( ) ) elif c == u\\'[\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getCharacterSet ( ) ) elif c == u\\'(\\' and not self . last ( ) == u\\'\\\\\\\\\\' : seq . append ( self . getSequence ( level + 1 ) ) elif c == u\\')\\' and not self . last ( ) == u\\'\\\\\\\\\\' : if level == 0 : raise StringGenerator . SyntaxError ( u\"Extra closing parenthesis\" ) sequence_closed = True break elif c == u\\'|\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c elif c == u\\'&\\' and not self . last ( ) == u\\'\\\\\\\\\\' : op = c else : if c in self . meta_chars and not self . last ( ) == u\"\\\\\\\\\" : raise StringGenerator . SyntaxError ( u\"Un-escaped special character: %s\" % c ) if op and not left_operand : if not seq or len ( seq ) < 1 : raise StringGenerator . SyntaxError ( u\"Operator: %s with no left operand\" % op ) left_operand = seq . pop ( ) elif op and len ( seq ) >= 1 and left_operand : right_operand = seq . pop ( ) if op == u\\'|\\' : seq . append ( StringGenerator . SequenceOR ( [ left_operand , right_operand ] ) ) elif op == u\\'&\\' : seq . append ( StringGenerator . SequenceAND ( [ left_operand , right_operand ] ) ) op = u\\'\\' left_operand = None right_operand = None if op : raise StringGenerator . SyntaxError ( u\"Operator: %s with no right operand\" % op ) if level > 0 and not sequence_closed : raise StringGenerator . SyntaxError ( u\"Missing closing parenthesis\" ) return StringGenerator . Sequence ( seq )'], ['Print the parse tree and then call render for an example .', 'def dump ( self , ** kwargs ) : import sys if not self . seq : self . seq = self . getSequence ( ) print ( \"StringGenerator version: %s\" % ( __version__ ) ) print ( \"Python version: %s\" % sys . version ) self . seq . dump ( ) return self . render ( ** kwargs )'], ['Return a list of generated strings .', 'def render_list ( self , cnt , unique = False , progress_callback = None , ** kwargs ) : rendered_list = [ ] i = 0 total_attempts = 0 while True : if i >= cnt : break if total_attempts > cnt * self . unique_attempts_factor : raise StringGenerator . UniquenessError ( u\"couldn\\'t satisfy uniqueness\" ) s = self . render ( ** kwargs ) if unique : if not s in rendered_list : rendered_list . append ( s ) i += 1 else : rendered_list . append ( s ) i += 1 total_attempts += 1 if progress_callback and callable ( progress_callback ) : progress_callback ( i , cnt ) return rendered_list'], ['Establish the connection . This is done automatically for you .', 'def connect ( self ) : self . conn = boto . connect_s3 ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL ) self . bucket = self . conn . get_bucket ( self . AWS_STORAGE_BUCKET_NAME ) self . k = Key ( self . bucket )'], ['Connect to Cloud Front . This is done automatically for you when needed .', 'def connect_cloudfront ( self ) : \"Connect to Cloud Front. This is done automatically for you when needed.\" self . conn_cloudfront = connect_cloudfront ( self . AWS_ACCESS_KEY_ID , self . AWS_SECRET_ACCESS_KEY , debug = self . S3UTILS_DEBUG_LEVEL )'], ['Create a folder on S3 .', 'def mkdir ( self , target_folder ) : self . printv ( \"Making directory: %s\" % target_folder ) self . k . key = re . sub ( r\"^/|/$\" , \"\" , target_folder ) + \"/\" self . k . set_contents_from_string ( \\'\\' ) self . k . close ( )'], ['Delete the path and anything under the path .', 'def rm ( self , path ) : list_of_files = list ( self . ls ( path ) ) if list_of_files : if len ( list_of_files ) == 1 : self . bucket . delete_key ( list_of_files [ 0 ] ) else : self . bucket . delete_keys ( list_of_files ) self . printv ( \"Deleted: %s\" % list_of_files ) else : logger . error ( \"There was nothing to remove under %s\" , path )'], ['Copy a file to s3 .', 'def __put_key ( self , local_file , target_file , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , source = \"filename\" ) : action_word = \"moving\" if del_after_upload else \"copying\" try : self . k . key = target_file if source == \"filename\" : self . k . set_contents_from_filename ( local_file , self . AWS_HEADERS ) elif source == \"fileobj\" : self . k . set_contents_from_file ( local_file , self . AWS_HEADERS ) elif source == \"string\" : self . k . set_contents_from_string ( local_file , self . AWS_HEADERS ) else : raise Exception ( \"%s is not implemented as a source.\" % source ) self . k . set_acl ( acl ) self . k . close ( ) self . printv ( \"%s %s to %s\" % ( action_word , local_file , target_file ) ) if del_after_upload and source == \"filename\" : try : os . remove ( local_file ) except : logger . error ( \"Unable to delete the file: \" , local_file , exc_info = True ) return True except : logger . error ( \"Error in writing to %s\" , target_file , exc_info = True ) return False'], ['Copy a file or folder from local to s3 .', 'def cp ( self , local_path , target_path , acl = \\'public-read\\' , del_after_upload = False , overwrite = True , invalidate = False ) : result = None if overwrite : list_of_files = [ ] else : list_of_files = self . ls ( folder = target_path , begin_from_file = \"\" , num = - 1 , get_grants = False , all_grant_data = False ) if local_path . endswith ( \"/*\" ) : local_path = local_path [ : - 2 ] target_path = re . sub ( r\"^/|/$\" , \"\" , target_path ) else : local_base_name = os . path . basename ( local_path ) local_path = re . sub ( r\"/$\" , \"\" , local_path ) target_path = re . sub ( r\"^/\" , \"\" , target_path ) if not target_path . endswith ( local_base_name ) : target_path = os . path . join ( target_path , local_base_name ) if os . path . exists ( local_path ) : result = self . __find_files_and_copy ( local_path , target_path , acl , del_after_upload , overwrite , invalidate , list_of_files ) else : result = { \\'file_does_not_exist\\' : local_path } logger . error ( \"trying to upload to s3 but file doesn\\'t exist: %s\" % local_path ) return result'], ['Similar to Linux mv command .', \"def mv ( self , local_file , target_file , acl = 'public-read' , overwrite = True , invalidate = False ) : self . cp ( local_file , target_file , acl = acl , del_after_upload = True , overwrite = overwrite , invalidate = invalidate )\"], ['Deal with saving cropduster images to S3 . Cropduster is a Django library for resizing editorial images . S3utils was originally written to put cropduster images on S3 bucket .', 'def cp_cropduster_image ( self , the_image_path , del_after_upload = False , overwrite = False , invalidate = False ) : local_file = os . path . join ( settings . MEDIA_ROOT , the_image_path ) if os . path . exists ( local_file ) : the_image_crops_path = os . path . splitext ( the_image_path ) [ 0 ] the_image_crops_path_full_path = os . path . join ( settings . MEDIA_ROOT , the_image_crops_path ) self . cp ( local_path = local_file , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , ) self . cp ( local_path = the_image_crops_path_full_path + \"/*\" , target_path = os . path . join ( settings . S3_ROOT_BASE , the_image_crops_path ) , del_after_upload = del_after_upload , overwrite = overwrite , invalidate = invalidate , )'], ['sets permissions for a file on S3', \"def chmod ( self , target_file , acl = 'public-read' ) : self . k . key = target_file self . k . set_acl ( acl ) self . k . close ( )\"], ['Get the list of files and permissions from S3 .', 'def ll ( self , folder = \"\" , begin_from_file = \"\" , num = - 1 , all_grant_data = False ) : return self . ls ( folder = folder , begin_from_file = begin_from_file , num = num , get_grants = True , all_grant_data = all_grant_data )'], ['Get the path from a given url including the querystring .', 'def get_path ( url ) : url = urlsplit ( url ) path = url . path if url . query : path += \"?{}\" . format ( url . query ) return path'], ['Reads data from disk and generates CSV files .', \"def run ( self ) : if not os . path . exists ( self . output ) : try : os . mkdir ( self . output ) except : print 'failed to create output directory %s' % self . output if not os . path . isdir ( self . output ) : print 'invalid output directory %s' % self . output sys . exit ( 1 ) visitors = [ _CompaniesCSV ( self . output ) , _ActivitiesCSV ( self . output ) , _ActivitiesSeenCSV ( self . output ) , _QSACSV ( self . output ) , ] for path in glob . glob ( os . path . join ( self . input , '*.json' ) ) : with open ( path , 'r' ) as f : try : data = json . load ( f , encoding = 'utf-8' ) except ValueError : continue for visitor in visitors : visitor . visit ( data )\"], ['Process a list of simple string field definitions and assign their order based on prefix .', \"def process_fields ( self , fields ) : result = [ ] strip = '' . join ( self . PREFIX_MAP ) for field in fields : direction = self . PREFIX_MAP [ '' ] if field [ 0 ] in self . PREFIX_MAP : direction = self . PREFIX_MAP [ field [ 0 ] ] field = field . lstrip ( strip ) result . append ( ( field , direction ) ) return result\"], ['Firms search in rubric', \"def search_in_rubric ( self , ** kwargs ) : point = kwargs . pop ( 'point' , False ) if point : kwargs [ 'point' ] = '%s,%s' % point bound = kwargs . pop ( 'bound' , False ) if bound : kwargs [ 'bound[point1]' ] = bound [ 0 ] kwargs [ 'bound[point2]' ] = bound [ 1 ] filters = kwargs . pop ( 'filters' , False ) if filters : for k , v in filters . items ( ) : kwargs [ 'filters[%s]' % k ] = v return self . _search_in_rubric ( ** kwargs )\"], ['Refresh the list and the screen', 'def refresh ( self ) : self . _screen . force_update ( ) self . _screen . refresh ( ) self . _update ( 1 )'], ['Mark an action as started', 'def start ( self , activity , action ) : try : self . _start_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . refresh ( )'], ['Mark a task as completed', 'def stop ( self , activity , action ) : try : self . _remove_running_action ( activity , action ) except ValueError : retox_log . debug ( \"Could not find action %s in env %s\" % ( activity , self . name ) ) self . _mark_action_completed ( activity , action ) self . refresh ( )'], ['Move laggard tasks over', 'def finish ( self , status ) : retox_log . info ( \"Completing %s with status %s\" % ( self . name , status ) ) result = Screen . COLOUR_GREEN if not status else Screen . COLOUR_RED self . palette [ \\'title\\' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , result ) for item in list ( self . _task_view . options ) : self . _task_view . options . remove ( item ) self . _completed_view . options . append ( item ) self . refresh ( )'], ['Reset the frame between jobs', \"def reset ( self ) : self . palette [ 'title' ] = ( Screen . COLOUR_WHITE , Screen . A_BOLD , Screen . COLOUR_BLUE ) self . _completed_view . options = [ ] self . _task_view . options = [ ] self . refresh ( )\"], ['Returns the available kwargs of the called class', 'def default_arguments ( cls ) : func = cls . __init__ args = func . __code__ . co_varnames defaults = func . __defaults__ index = - len ( defaults ) return { k : v for k , v in zip ( args [ index : ] , defaults ) }'], ['Recreate the class based in your args multiple uses', 'def recreate ( cls , * args , ** kwargs ) : cls . check_arguments ( kwargs ) first_is_callable = True if any ( args ) and callable ( args [ 0 ] ) else False signature = cls . default_arguments ( ) allowed_arguments = { k : v for k , v in kwargs . items ( ) if k in signature } if ( any ( allowed_arguments ) or any ( args ) ) and not first_is_callable : if any ( args ) and not first_is_callable : return cls ( args [ 0 ] , ** allowed_arguments ) elif any ( allowed_arguments ) : return cls ( ** allowed_arguments ) return cls . instances [ - 1 ] if any ( cls . instances ) else cls ( )'], ['Put warnings of arguments whose can t be handle by the class', 'def check_arguments ( cls , passed ) : defaults = list ( cls . default_arguments ( ) . keys ( ) ) template = ( \"Pass arg {argument:!r} in {cname:!r}, can be a typo? \" \"Supported key arguments: {defaults}\" ) fails = [ ] for arg in passed : if arg not in defaults : warn ( template . format ( argument = arg , cname = cls . __name__ , defaults = defaults ) ) fails . append ( arg ) return any ( fails )'], ['process the specified type then process its children', 'def process ( self , data , type , history ) : if type in history : return if type . enum ( ) : return history . append ( type ) resolved = type . resolve ( ) value = None if type . multi_occurrence ( ) : value = [ ] else : if len ( resolved ) > 0 : if resolved . mixed ( ) : value = Factory . property ( resolved . name ) md = value . __metadata__ md . sxtype = resolved else : value = Factory . object ( resolved . name ) md = value . __metadata__ md . sxtype = resolved md . ordering = self . ordering ( resolved ) setattr ( data , type . name , value ) if value is not None : data = value if not isinstance ( data , list ) : self . add_attributes ( data , resolved ) for child , ancestry in resolved . children ( ) : if self . skip_child ( child , ancestry ) : continue self . process ( data , child , history [ : ] )'], ['get whether or not to skip the specified child', 'def skip_child ( self , child , ancestry ) : if child . any ( ) : return True for x in ancestry : if x . choice ( ) : return True return False'], ['Checks whether knocks are enabled for the model given as argument', \"def active_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : return True return _thread_locals . knock_enabled . get ( obj . __class__ , True )\"], ['Context manager to suspend sending knocks for the given model', \"def pause_knocks ( obj ) : if not hasattr ( _thread_locals , 'knock_enabled' ) : _thread_locals . knock_enabled = { } obj . __class__ . _disconnect ( ) _thread_locals . knock_enabled [ obj . __class__ ] = False yield _thread_locals . knock_enabled [ obj . __class__ ] = True obj . __class__ . _connect ( )\"], ['Loop over the report progress', 'def _loopreport ( self ) : while 1 : eventlet . sleep ( 0.2 ) ac2popenlist = { } for action in self . session . _actions : for popen in action . _popenlist : if popen . poll ( ) is None : lst = ac2popenlist . setdefault ( action . activity , [ ] ) lst . append ( popen ) if not action . _popenlist and action in self . _actionmayfinish : super ( RetoxReporter , self ) . logaction_finish ( action ) self . _actionmayfinish . remove ( action ) self . screen . draw_next_frame ( repeat = False )'], ['Send markdown email', \"def send ( email , subject = None , from_email = None , to_email = None , cc = None , bcc = None , reply_to = None , smtp = None ) : if is_string ( email ) : email = EmailContent ( email ) from_email = sanitize_email_address ( from_email or email . headers . get ( 'from' ) ) to_email = sanitize_email_address ( to_email or email . headers . get ( 'to' ) ) cc = sanitize_email_address ( cc or email . headers . get ( 'cc' ) ) bcc = sanitize_email_address ( bcc or email . headers . get ( 'bcc' ) ) reply_to = sanitize_email_address ( reply_to or email . headers . get ( 'reply-to' ) ) message_args = { 'html' : email . html , 'text' : email . text , 'subject' : ( subject or email . headers . get ( 'subject' , '' ) ) , 'mail_from' : from_email , 'mail_to' : to_email } if cc : message_args [ 'cc' ] = cc if bcc : message_args [ 'bcc' ] = bcc if reply_to : message_args [ 'headers' ] = { 'reply-to' : reply_to } message = emails . Message ( ** message_args ) for filename , data in email . inline_images : message . attach ( filename = filename , content_disposition = 'inline' , data = data ) message . send ( smtp = smtp )\"], ['Process timezone casting and conversion .', 'def _process_tz ( self , dt , naive , tz ) : def _tz ( t ) : if t in ( None , \\'naive\\' ) : return t if t == \\'local\\' : if __debug__ and not localtz : raise ValueError ( \"Requested conversion to local timezone, but `localtz` not installed.\" ) t = localtz if not isinstance ( t , tzinfo ) : if __debug__ and not localtz : raise ValueError ( \"The `pytz` package must be installed to look up timezone: \" + repr ( t ) ) t = get_tz ( t ) if not hasattr ( t , \\'normalize\\' ) and get_tz : t = get_tz ( t . tzname ( dt ) ) return t naive = _tz ( naive ) tz = _tz ( tz ) if not dt . tzinfo and naive : if hasattr ( naive , \\'localize\\' ) : dt = naive . localize ( dt ) else : dt = dt . replace ( tzinfo = naive ) if not tz : return dt if hasattr ( tz , \\'normalize\\' ) : dt = tz . normalize ( dt . astimezone ( tz ) ) elif tz == \\'naive\\' : dt = dt . replace ( tzinfo = None ) else : dt = dt . astimezone ( tz ) return dt'], ['Trigger assignment of default values .', 'def _prepare_defaults ( self ) : for name , field in self . __fields__ . items ( ) : if field . assign : getattr ( self , name )'], ['Convert data coming in from the MongoDB wire driver into a Document instance .', \"def from_mongo ( cls , doc ) : if doc is None : return None if isinstance ( doc , Document ) : return doc if cls . __type_store__ and cls . __type_store__ in doc : cls = load ( doc [ cls . __type_store__ ] , 'marrow.mongo.document' ) instance = cls ( _prepare_defaults = False ) instance . __data__ = doc instance . _prepare_defaults ( ) return instance\"], ['Retrieve and remove a value from the backing store optionally with a default .', 'def pop ( self , name , default = SENTINEL ) : if default is SENTINEL : return self . __data__ . pop ( name ) return self . __data__ . pop ( name , default )'], ['A basic operation operating on a single value .', 'def _op ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _op ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) if other is not None : other = f . transformer . foreign ( other , ( f , self . _document ) ) return Filter ( { self . _name : { operation : other } } )'], ['An iterative operation operating on multiple values . Consumes iterators to construct a concrete list at time of execution .', 'def _iop ( self , operation , other , * allowed ) : f = self . _field if self . _combining : return reduce ( self . _combining , ( q . _iop ( operation , other , * allowed ) for q in f ) ) if __debug__ and _complex_safety_check ( f , { operation } | set ( allowed ) ) : raise NotImplementedError ( \"{self!r} does not allow {op} comparison.\" . format ( self = self , op = operation ) ) def _t ( o ) : for value in o : yield None if value is None else f . transformer . foreign ( value , ( f , self . _document ) ) other = other if len ( other ) > 1 else other [ 0 ] values = list ( _t ( other ) ) return Filter ( { self . _name : { operation : values } } )']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lang(lang, top_k=100):\n",
        "  words = list(lang.word2count.keys())\n",
        "  words.sort(key=lambda w: lang.word2count[w], reverse=True)\n",
        "  print(words[:top_k])\n",
        "  count_occurences = sum(lang.word2count.values())\n",
        "\n",
        "  accumulated = 0\n",
        "  counter = 0\n",
        "\n",
        "  while accumulated < count_occurences * 0.8:\n",
        "    accumulated += lang.word2count[words[counter]]\n",
        "    counter += 1\n",
        "\n",
        "  print(f\"The {counter * 100 / len(words)}% most common words \"\n",
        "        f\"account for the {accumulated * 100 / count_occurences}% of the occurrences\")\n",
        "  plt.bar(range(100), [lang.word2count[w] for w in words[:top_k]])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "YzcYpcUePYBP"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_lang(input_lang)"
      ],
      "metadata": {
        "id": "rkWnNVA3PaRB",
        "outputId": "69b1ec75-6995-49d6-a7fd-45f03f594e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.', 'the', 'a', 'to', 'of', 'for', 'and', '-', 'in', 'from', 'file', 'with', 'on', 'is', 'or', 'that', 'if', 'parse', 'given', 'not', 'each', 'as', 'data', 'by', 'an', 'convert', 'get', 'Returns', 'list', 'default', 'make', 'specified', 'have', 'genome', 'remove', 'format', 'value', 'table', 'string', 'it', 'return', 'all', 'based', 'sequences', 'fasta', 'files', 'image', 'If', 'at', 'print', 'are', 'unique', 'be', 'Return', 'path', 'dictionary', 'values', 'set', 'function', 'reads', 'sequence', 'calculate', 'one', 'version', 'type', 'OTU', 'single', 'sam', 'object', 'output', 'Steam', 'custom', 'Default', 'returns', 'will', 'into', 'then', 'read', 'abundance', 'number', 'ID', 'line', 'bit', 'coverage', 'whether', 'insertion', 'validation', 'sure', 'directory', 'does', 'The', 'user', 'mapping', 'group', 'Calculate', 'sample', 'Compute', 'command', 'entri', 'Mengembalikan']\n",
            "The 40.55299539170507% most common words account for the 80.03067484662577% of the occurrences\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_lang(output_lang)"
      ],
      "metadata": {
        "id": "Qzh34WHkPfrR",
        "outputId": "59fd0da5-7a68-43d1-a326-2d2a45cfcc79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(', ')', ',', '.', '=', ':', '[', ']', 'if', '', 'self', 'in', 'for', '1', '0', 'return', 'def', 'line', 'not', 'i', 'append', '==', 'else', 'is', 'False', 'None', 'split', '%', '+', '-', 'and', 'seq', '{', '}', 'True', 'len', 'c', 'name', 'join', 'path', 'print', \"'\", 'genome', 'args', 'gff', 'kwargs', 'os', 'prop', '*', 'value', 'yield', 't', 'f', 'file', 'header', 'info', 'elif', 'strip', 'list', 'b', 'float', 'a', 'fasta', 'id', 'or', '**', \"''\", 'values', '+=', '!=', 'start', 'insertion', '>', '2', 'register', 'out', 'strand', 'continue', 'index', 'table', 'format', 'ID', 'rsplit', 'hits', 'except', 'int', \"'.'\", 'k', 'parser', 'sys', 'try', 'raise', 'length', '%s', 'fastas', 'p', 'items', 'add_argument', 'help', 'read']\n",
            "The 9.073958980733375% most common words account for the 80.02407773669275% of the occurrences\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQG0lEQVR4nO3cf6zddX3H8edrVNnUZZTRNdg2u53rXOoSgTTAolmYbFBwWTUxBrJIY1jqH5DhYrJd3R84DQlL/DFJHEmVTlicjCmOhjay2rEY/wC5OAIUZFwRRptCr0PRzETFvffH+XQ5lnt7f99Lz+f5SE7O9/v+fs73fD753ry+3/M533tSVUiS+vALq90BSdLKMfQlqSOGviR1xNCXpI4Y+pLUkTWr3YGTOeuss2psbGy1uyFJp5QHH3zwu1W1brptr+jQHxsbY2JiYrW7IUmnlCTPzLTN6R1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerISIf+2Pg+xsb3vWxZkno10qEvSfp5hr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjJr6CfZlOTeJI8lOZTkulb/cJIjSR5qj8uHXvPBJJNJnkhy6VB9e6tNJhlfniFJkmayZg5tXgI+UFXfTPLLwINJDrRtn6yqjw03TrIVuAJ4E/B64KtJfqtt/jTwh8Bh4IEke6vqsaUYiCRpdrOGflUdBY625R8meRzYcJKX7ABur6ofA99JMgmc37ZNVtVTAElub20NfUlaIfOa008yBpwL3N9K1yZ5OMmeJGtbbQPw7NDLDrfaTPUT32NXkokkE1NTU/PpniRpFnMO/SSvA74EvL+qfgDcDLwBOIfBJ4GPL0WHqmp3VW2rqm3r1q1bil1Kkpq5zOmT5FUMAv/zVXUnQFU9P7T9M8DdbfUIsGno5RtbjZPUJUkrYC537wS4BXi8qj4xVD97qNk7gUfb8l7giiSnJ9kMbAG+ATwAbEmyOcmrGXzZu3dphiFJmou5XOm/BXgP8EiSh1rtQ8CVSc4BCngaeB9AVR1KcgeDL2hfAq6pqp8BJLkWuAc4DdhTVYeWcCySpFnM5e6drwOZZtP+k7zmBuCGaer7T/Y6SdLy8j9yJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOdBn6Y+P7GBvft9rdkKQV12XoS1KvZg39JJuS3JvksSSHklzX6mcmOZDkyfa8ttWT5KYkk0keTnLe0L52tvZPJtm5fMOSJE1nLlf6LwEfqKqtwIXANUm2AuPAwaraAhxs6wCXAVvaYxdwMwxOEsD1wAXA+cD1x08UkqSVMWvoV9XRqvpmW/4h8DiwAdgB3Nqa3Qq8oy3vAG6rgfuAM5KcDVwKHKiqF6rqe8ABYPuSjkaSdFLzmtNPMgacC9wPrK+qo23Tc8D6trwBeHboZYdbbab6ie+xK8lEkompqan5dE+SNIs5h36S1wFfAt5fVT8Y3lZVBdRSdKiqdlfVtqratm7duqXYpSSpmVPoJ3kVg8D/fFXd2crPt2kb2vOxVj8CbBp6+cZWm6kuSVohc7l7J8AtwONV9YmhTXuB43fg7ATuGqpf1e7iuRB4sU0D3QNckmRt+wL3klZbVd6zL6kna+bQ5i3Ae4BHkjzUah8CbgTuSHI18Azw7rZtP3A5MAn8CHgvQFW9kOSjwAOt3Ueq6oUlGYUkaU5mDf2q+jqQGTZfPE37Aq6ZYV97gD3z6aAkaen4H7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwz9IWPj+xgb37fa3ZCkZWPoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR2YN/SR7khxL8uhQ7cNJjiR5qD0uH9r2wSSTSZ5IculQfXurTSYZX/qhLC3/O1fSKJrLlf7ngO3T1D9ZVee0x36AJFuBK4A3tdf8XZLTkpwGfBq4DNgKXNnaSpJW0JrZGlTV15KMzXF/O4Dbq+rHwHeSTALnt22TVfUUQJLbW9vH5t1jSdKCLWZO/9okD7fpn7WttgF4dqjN4Vabqf4ySXYlmUgyMTU1tYjuSZJOtNDQvxl4A3AOcBT4+FJ1qKp2V9W2qtq2bt26pdqtJIk5TO9Mp6qeP76c5DPA3W31CLBpqOnGVuMkdUnSClnQlX6Ss4dW3wkcv7NnL3BFktOTbAa2AN8AHgC2JNmc5NUMvuzdu/BuS5IWYtYr/SRfAC4CzkpyGLgeuCjJOUABTwPvA6iqQ0nuYPAF7UvANVX1s7afa4F7gNOAPVV1aMlHI0k6qbncvXPlNOVbTtL+BuCGaer7gf3z6p0kaUn5H7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOG/hyMje9jbHzfy5Yl6VRj6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjswa+kn2JDmW5NGh2plJDiR5sj2vbfUkuSnJZJKHk5w39Jqdrf2TSXYuz3AkSSczlyv9zwHbT6iNAweragtwsK0DXAZsaY9dwM0wOEkA1wMXAOcD1x8/UUiSVs6soV9VXwNeOKG8A7i1Ld8KvGOoflsN3AeckeRs4FLgQFW9UFXfAw7w8hOJJGmZLXROf31VHW3LzwHr2/IG4Nmhdodbbab6yyTZlWQiycTU1NQCuydJms6iv8itqgJqCfpyfH+7q2pbVW1bt27dUu1WksTCQ//5Nm1Dez7W6keATUPtNrbaTHVJ0gpaaOjvBY7fgbMTuGuoflW7i+dC4MU2DXQPcEmSte0L3EtaTZK0gtbM1iDJF4CLgLOSHGZwF86NwB1JrgaeAd7dmu8HLgcmgR8B7wWoqheSfBR4oLX7SFWd+OWwJGmZzRr6VXXlDJsunqZtAdfMsJ89wJ559U6StKT8j1xJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sia1e7AqWxsfN//Lz9949tXsSeSNDde6UtSRwx9SeqIoS9JHVlU6Cd5OskjSR5KMtFqZyY5kOTJ9ry21ZPkpiSTSR5Oct5SDECSNHdL8UXu71fVd4fWx4GDVXVjkvG2/pfAZcCW9rgAuLk9jwS/1JV0KliOu3d2ABe15VuBf2cQ+juA26qqgPuSnJHk7Ko6ugx9WFXDJ4BhngwkrbbFzukX8K9JHkyyq9XWDwX5c8D6trwBeHbotYdb7eck2ZVkIsnE1NTUIrsnSRq22Cv9t1bVkSS/BhxI8q3hjVVVSWo+O6yq3cBugG3bts3rtZKkk1vUlX5VHWnPx4AvA+cDzyc5G6A9H2vNjwCbhl6+sdUkSStkwaGf5LVJfvn4MnAJ8CiwF9jZmu0E7mrLe4Gr2l08FwIvjuJ8viS9ki1memc98OUkx/fzj1X1lSQPAHckuRp4Bnh3a78fuByYBH4EvHcR7y1JWoAFh35VPQW8eZr6fwMXT1Mv4JqFvp8kafH8j1xJ6oihL0kdMfQlqSP+nv4K8qcaJK02r/QlqSOGviR1xOmdVeJUj6TV4JW+JHXEK/1XgJl+inmYnwYkLQWv9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDP1TyNj4vjnd3ilJMzH0Jakjhr4kdcTQP0U51SNpIQz9ETB8AvBkIOlkDP0RNtPJwBOD1C9Dv3OeDKS+GPqalicDaTT508qal5nC359+lk4Nhr6WxGI+CXjCkFaO0zuS1BGv9LXqTjZldHzbXJclnZyhr5Ey35PEQpePr0unGkNfWqDluKNpuU9Wiz3RndhXnXpWPPSTbAc+BZwGfLaqblzpPkhavB5OeithpacrV/SL3CSnAZ8GLgO2Alcm2bqSfZCknq303TvnA5NV9VRV/QS4Hdixwn2QpG6lqlbuzZJ3Adur6k/b+nuAC6rq2qE2u4BdbfWNwBOLfNuzgO8uch+nGsfcB8fch4WM+derat10G15xX+RW1W5g91LtL8lEVW1bqv2dChxzHxxzH5Z6zCs9vXME2DS0vrHVJEkrYKVD/wFgS5LNSV4NXAHsXeE+SFK3VnR6p6peSnItcA+DWzb3VNWhZX7bJZsqOoU45j445j4s6ZhX9ItcSdLq8gfXJKkjhr4kdWRkQz/J9iRPJJlMMr7a/VkOSTYluTfJY0kOJbmu1c9MciDJk+157Wr3daklOS3JfyS5u61vTnJ/O97/1G4UGBlJzkjyxSTfSvJ4kt8d9eOc5M/b3/WjSb6Q5BdH8Tgn2ZPkWJJHh2rTHtsM3NTG/3CS8+b7fiMZ+h393MNLwAeqaitwIXBNG+c4cLCqtgAH2/qouQ54fGj9b4BPVtVvAt8Drl6VXi2fTwFfqarfBt7MYOwje5yTbAD+DNhWVb/D4MaPKxjN4/w5YPsJtZmO7WXAlvbYBdw83zcbydCnk597qKqjVfXNtvxDBkGwgcFYb23NbgXesTo9XB5JNgJvBz7b1gO8DfhiazJSY07yK8DvAbcAVNVPqur7jPhxZnB34S8lWQO8BjjKCB7nqvoa8MIJ5ZmO7Q7gthq4Dzgjydnzeb9RDf0NwLND64dbbWQlGQPOBe4H1lfV0bbpOWD9KnVrufwt8BfA/7b1XwW+X1UvtfVRO96bgSng79uU1meTvJYRPs5VdQT4GPBfDML+ReBBRvs4D5vp2C4620Y19LuS5HXAl4D3V9UPhrfV4J7ckbkvN8kfAceq6sHV7ssKWgOcB9xcVecC/8MJUzkjeJzXMriq3Qy8HngtL58C6cJSH9tRDf1ufu4hyasYBP7nq+rOVn7++Ee+9nxstfq3DN4C/HGSpxlM272NwXz3GW0aAEbveB8GDlfV/W39iwxOAqN8nP8A+E5VTVXVT4E7GRz7UT7Ow2Y6tovOtlEN/S5+7qHNZd8CPF5VnxjatBfY2ZZ3AnetdN+WS1V9sKo2VtUYg+P6b1X1J8C9wLtas1Eb83PAs0ne2EoXA48xwseZwbTOhUle0/7Oj495ZI/zCWY6tnuBq9pdPBcCLw5NA81NVY3kA7gc+E/g28BfrXZ/lmmMb2Xwse9h4KH2uJzBHPdB4Engq8CZq93XZRr/RcDdbfk3gG8Ak8A/A6evdv+WeKznABPtWP8LsHbUjzPw18C3gEeBfwBOH8XjDHyBwfcWP2Xwqe7qmY4tEAZ3Jn4beITB3U3zej9/hkGSOjKq0zuSpGkY+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj/wd00sBsYu6AGgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedded = self.embedding(input)#.view(1, 1, -1)\n",
        "    output = embedded\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self, batch_size):\n",
        "    return torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    output = self.embedding(input)\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    output = self.softmax(self.out(output))\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "def to_train(input_lang, output_lang, pairs, max_len=MAX_LENGTH+2):\n",
        "  x_input = []\n",
        "  x_output = []\n",
        "  target = []\n",
        "  for i, o in pairs:\n",
        "    s_i = [2] * max_len + [0] + [input_lang.word2index[w] for w in i.split(\" \")] + [1]\n",
        "    s_o = [0] + [output_lang.word2index[w] for w in o.split(\" \")] + [1] + [2] * max_len\n",
        "    s_to = s_o[1:] + [2]\n",
        "    x_input.append(s_i[-max_len:])\n",
        "    x_output.append(s_o[:max_len])\n",
        "    target.append(s_to[:max_len])\n",
        "  return x_input, x_output, target"
      ],
      "metadata": {
        "id": "pVVjFWKIPjiu"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_input, x_partial, y = to_train(input_lang, output_lang, pairs)"
      ],
      "metadata": {
        "id": "UpE0ELbUPrAe"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Representation of an input sentece:')\n",
        "print(x_input[0])\n",
        "print(' '.join([input_lang.index2word[w] for w in x_input[0]]))\n",
        "print('\\nRepresentation of an partial sentece:')\n",
        "print(x_partial[0])\n",
        "print(' '.join([output_lang.index2word[w] for w in x_partial[0]]))\n",
        "print('\\nRepresentation of an target sentece:')\n",
        "print(y[0])\n",
        "print(' '.join([output_lang.index2word[w] for w in y[0]]))"
      ],
      "metadata": {
        "id": "lkGRBdQDPtlG",
        "outputId": "ce265cc9-75db-40c9-dc60-bf7008493e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Representation of an input sentece:\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1]\n",
            "PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD SOS Return either the full or truncated version of a QIIME - formatted taxonomy string . EOS\n",
            "\n",
            "Representation of an partial sentece:\n",
            "[0, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 8, 9, 8, 13, 14, 15, 9, 6, 16, 17, 5, 8, 11, 18, 15, 19, 20, 21, 13, 8, 13, 15, 19, 22, 21, 16, 17, 5, 23, 11, 19, 20, 21, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "SOS def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ] EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n",
            "\n",
            "Representation of an target sentece:\n",
            "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 8, 9, 8, 13, 14, 15, 9, 6, 16, 17, 5, 8, 11, 18, 15, 19, 20, 21, 13, 8, 13, 15, 19, 22, 21, 16, 17, 5, 23, 11, 19, 20, 21, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "def split_phylogeny ( p , level = \"s\" ) : level = level + \"__\" result = p . split ( level ) return result [ 0 ] + level + result [ 1 ] . split ( \";\" ) [ 0 ] EOS PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD PAD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(encoder, decoder, input, output):\n",
        "  _, hidden = encoder(input, encoder.initHidden(input.shape[0]))\n",
        "  out, _ = decoder(output, hidden)\n",
        "  return out\n",
        "\n",
        "def train(encoder, decoder, loss, input, output, target, learning_rate=0.001, epochs=10, batch_size=100):\n",
        "\n",
        "  plot_losses = []\n",
        "  plot_full_losses = []\n",
        "\n",
        "  encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "  for _ in tqdm(range(epochs)):\n",
        "    c_input, c_output, c_target = shuffle(input, output, target)\n",
        "    c_input = torch.tensor(c_input, dtype=torch.long, device=device)\n",
        "    c_output = torch.tensor(c_output, dtype=torch.long, device=device)\n",
        "    c_target = torch.tensor(c_target, dtype=torch.long, device=device)\n",
        "    acc_loss = 0\n",
        "    for i in range(0, c_target.shape[0], batch_size):\n",
        "      c_batch_size = c_target[i:i+batch_size, ...].shape[0]\n",
        "      encoder_optimizer.zero_grad()\n",
        "      decoder_optimizer.zero_grad()\n",
        "\n",
        "      out = predict(encoder, decoder, c_input[i:i+batch_size, ...], c_output[i:i+batch_size, ...])\n",
        "      #Reshapes the output and target to use the expected loss format.\n",
        "      # N x Classes for the output\n",
        "      # N for the targets\n",
        "      # Where N is the batch size\n",
        "      out = out.reshape(c_batch_size * c_input.shape[1], -1)\n",
        "      r_target = c_target[i:i+batch_size, ...].reshape(c_batch_size * c_input.shape[1])\n",
        "\n",
        "      c_loss = loss(out, r_target)\n",
        "      # Mask the errors for padding as they are not usefull!\n",
        "      valid = torch.where(r_target == 2, 0, 1)\n",
        "      c_loss = c_loss * valid\n",
        "      c_loss = torch.sum(c_loss) #/ torch.sum(valid)\n",
        "\n",
        "      c_loss.backward()\n",
        "\n",
        "      encoder_optimizer.step()\n",
        "      decoder_optimizer.step()\n",
        "      plot_full_losses.append(c_loss.detach().cpu().numpy())\n",
        "      acc_loss += c_loss.detach().cpu().numpy()\n",
        "    plot_losses.append(acc_loss /math.ceil(c_target.shape[0] / batch_size))\n",
        "  return plot_losses, plot_full_losses"
      ],
      "metadata": {
        "id": "5IHzk4nkPwIe"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 300\n",
        "num_epochs = 10  # Change this to 50 (original value!)\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "epoch_error, batch_error = train(encoder, decoder,\n",
        "                                 nn.NLLLoss(reduction='none'),\n",
        "                                 x_input, x_partial, y,\n",
        "                                 epochs=num_epochs)"
      ],
      "metadata": {
        "id": "6MkfziMlPwZK",
        "outputId": "64a690d3-8407-4a5c-944d-b2b35ff3daa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "843b6c4e8a754ff78c3d7fe5b9e8415e",
            "8098ec46c5bb4e17a2ed494f7f01d31d",
            "50037afb8751477688a4b4597c985351",
            "b5cd00a828144b2e91cb609adb3f6e78",
            "cdf4bcb1803a4f48a43c1628ca6716ee",
            "22eb8b66c2b64d5dafef3e474379ad5b",
            "16690e46bb53464eb2edc7b01505e8ce",
            "9646255fbf084e6a8f0e5323b53330df",
            "aace67b0ffd745d4a595321f0558b871",
            "3454a6f046da470c8c9d036184a80af5",
            "45ee471439b44bbe92adaf6590bdd387"
          ]
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "843b6c4e8a754ff78c3d7fe5b9e8415e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(epoch_error)\n",
        "#print(batch_error)\n",
        "\n",
        "plt.plot(batch_error)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('minibatch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_error)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "phruRlJ9P4jD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "cbad4ab8-1d3c-447b-983a-78bb2dfb6829"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = predict(encoder, decoder, torch.tensor([x_input[100]],\n",
        "                                           dtype=torch.long,\n",
        "                                           device=device),\n",
        "            torch.tensor([x_partial[100]], dtype=torch.long, device=device))\n",
        "\n",
        "p = p.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "FE20nh29QetB"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argmax(p, axis=-1))\n",
        "print(x_partial[40])"
      ],
      "metadata": {
        "id": "lhB6hHOpQz56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127fd720-ea5e-495d-de0c-181a81ee190d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 50 665 665 ... 665 665 665]]\n",
            "[0, 3, 727, 5, 220, 7, 728, 11, 12, 729, 9, 728, 16, 678, 5, 155, 9, 730, 11, 731, 9, 728, 16, 678, 5, 155, 9, 732, 11, 733, 9, 728, 16, 678, 5, 155, 9, 734, 11, 26, 729, 12, 729, 9, 729, 16, 735, 5, 736, 11, 26, 731, 12, 220, 16, 729, 9, 66, 731, 16, 491, 16, 116, 5, 11, 12, 731, 19, 737, 21, 16, 116, 5, 11, 71, 220, 16, 738, 9, 731, 16, 739, 16, 116, 5, 11, 220, 16, 738, 214, 693, 693, 13, 728, 16, 678, 5, 155, 9, 740, 11, 16, 491, 16, 116, 5, 11, 43, 12, 220, 16, 729, 9, 66, 95, 16, 491, 16, 116, 5, 11, 12, 95, 19, 737, 21, 16, 116, 5, 11, 75, 95, 76, 729, 71, 26, 729, 43, 66, 71, 220, 16, 733, 9, 733, 16, 491, 16, 116, 5, 11, 26, 733, 43, 192, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_translation(encoder, decoder, text, input_lang, output_lang,\n",
        "                    max_len=MAX_LENGTH+2):\n",
        "\n",
        "  text =  [2] * max_len + [0] + [input_lang.word2index[w] for w in text.split(\" \")] + [1]\n",
        "  text = torch.tensor([text[-max_len:]], dtype=torch.long, device=device)\n",
        "  out = [0] + [2] * max_len\n",
        "  out = [out[:max_len]]\n",
        "  for i in range(1, max_len):\n",
        "    pt_out =torch.tensor(out, dtype=torch.long, device=device)\n",
        "    p = predict(encoder, decoder, text, pt_out).detach().cpu().numpy()\n",
        "    out[0][i] = np.argmax(p, axis=-1)[0, i-1]\n",
        "    if np.argmax(p, axis=-1)[0, i-1] == 1:\n",
        "      break\n",
        "\n",
        "  return ' '.join([output_lang.index2word[idx] for idx in out[0]])"
      ],
      "metadata": {
        "id": "tqkBnZaBlZDR"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_translation(encoder, decoder, pairs[40][0], input_lang, output_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "tvRU7L-wpERB",
        "outputId": "9d5b2dd1-b090-4e80-ae6b-dc1da96001cb"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SOS None kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs kwargs'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(40):\n",
        "  print('> {}'.format(pairs[i][0]))\n",
        "  print('= {}'.format(pairs[i][1]))\n",
        "  print('< {}'.format(gen_translation(encoder, decoder,\n",
        "                                      pairs[i][0],\n",
        "                                      input_lang,\n",
        "                                      output_lang)))\n",
        "  print('*' * 40)"
      ],
      "metadata": {
        "id": "g2-aVAwKpLlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-Ga42NcRp45Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}